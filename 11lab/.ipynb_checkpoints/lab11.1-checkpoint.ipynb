{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 11\n",
    "## Esercizio 11.1\n",
    "Svolgo un'analisi delle reti neurali, studiando il fit della retta $y=2x+1$. I dati utilizzati sono distribuiti normalmente attorno a questa retta, con una deviazione standard $\\sigma\\in[0.1,1.1]$; ho variato il numero di epoche nell'intervallo $n_{epochs}\\in[10,60]$ ed il numero di dati nel dataset di training $N_{train}\\in[500,1000]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# target parameters of f(x) = m*x + b\n",
    "m = 2 # slope\n",
    "b = 1 # intersect\n",
    "\n",
    "# compose the NN model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(1, input_shape=(1,)))\n",
    "\n",
    "# compile the model choosing optimizer, loss and metrics objects\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])\n",
    "# get a summary of our composed model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruisco una rete neurale con $N_{epochs}=10$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.0436 - mse: 2.0436 - val_loss: 1.9030 - val_mse: 1.9030\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4627 - mse: 1.4627 - val_loss: 1.3658 - val_mse: 1.3658\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0817 - mse: 1.0817 - val_loss: 1.0075 - val_mse: 1.0075\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8206 - mse: 0.8206 - val_loss: 0.7622 - val_mse: 0.7622\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6356 - mse: 0.6356 - val_loss: 0.5871 - val_mse: 0.5871\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4986 - mse: 0.4986 - val_loss: 0.4566 - val_mse: 0.4566\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3946 - mse: 0.3946 - val_loss: 0.3601 - val_mse: 0.3601\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3151 - mse: 0.3151 - val_loss: 0.2873 - val_mse: 0.2873\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.2301 - val_mse: 0.2301\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2044 - mse: 0.2044 - val_loss: 0.1862 - val_mse: 0.1862\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.1862 - mse: 0.1862\n",
      "\n",
      "Weight: [array([[1.2881609]], dtype=float32), array([0.9308365], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2645 - mse: 0.2645 - val_loss: 0.2076 - val_mse: 0.2076\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2261 - mse: 0.2261 - val_loss: 0.1776 - val_mse: 0.1776\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1965 - mse: 0.1965 - val_loss: 0.1544 - val_mse: 0.1544\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1736 - mse: 0.1736 - val_loss: 0.1362 - val_mse: 0.1362\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1221 - val_mse: 0.1221\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1415 - mse: 0.1415 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1305 - mse: 0.1305 - val_loss: 0.1019 - val_mse: 0.1019\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1215 - mse: 0.1215 - val_loss: 0.0947 - val_mse: 0.0947\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1145 - mse: 0.1145 - val_loss: 0.0888 - val_mse: 0.0888\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1088 - mse: 0.1088 - val_loss: 0.0841 - val_mse: 0.0841\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0841 - mse: 0.0841\n",
      "\n",
      "Weight: [array([[1.7878343]], dtype=float32), array([1.00327], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2413 - mse: 0.2413 - val_loss: 0.1883 - val_mse: 0.1883\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2383 - mse: 0.2383 - val_loss: 0.1858 - val_mse: 0.1858\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2357 - mse: 0.2357 - val_loss: 0.1838 - val_mse: 0.1838\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2337 - mse: 0.2337 - val_loss: 0.1822 - val_mse: 0.1822\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2321 - mse: 0.2321 - val_loss: 0.1808 - val_mse: 0.1808\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2307 - mse: 0.2307 - val_loss: 0.1795 - val_mse: 0.1795\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2297 - mse: 0.2297 - val_loss: 0.1786 - val_mse: 0.1786\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2288 - mse: 0.2288 - val_loss: 0.1778 - val_mse: 0.1778\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2281 - mse: 0.2281 - val_loss: 0.1772 - val_mse: 0.1772\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2276 - mse: 0.2276 - val_loss: 0.1766 - val_mse: 0.1766\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1766 - mse: 0.1766\n",
      "\n",
      "Weight: [array([[1.9374702]], dtype=float32), array([0.9806217], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5198 - mse: 0.5198 - val_loss: 0.4081 - val_mse: 0.4081\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5198 - mse: 0.5198 - val_loss: 0.4083 - val_mse: 0.4083\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5198 - mse: 0.5198 - val_loss: 0.4083 - val_mse: 0.4083\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5198 - mse: 0.5198 - val_loss: 0.4084 - val_mse: 0.4084\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5199 - mse: 0.5199 - val_loss: 0.4086 - val_mse: 0.4086\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5198 - mse: 0.5198 - val_loss: 0.4089 - val_mse: 0.4089\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5196 - mse: 0.5196 - val_loss: 0.4088 - val_mse: 0.4088\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5195 - mse: 0.5195 - val_loss: 0.4087 - val_mse: 0.4087\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5199 - mse: 0.5199 - val_loss: 0.4087 - val_mse: 0.4087\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5197 - mse: 0.5197 - val_loss: 0.4086 - val_mse: 0.4086\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4086 - mse: 0.4086\n",
      "\n",
      "Weight: [array([[1.9337122]], dtype=float32), array([0.99485147], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7860 - mse: 0.7860 - val_loss: 1.0657 - val_mse: 1.0657\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7859 - mse: 0.7859 - val_loss: 1.0643 - val_mse: 1.0643\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7860 - mse: 0.7860 - val_loss: 1.0630 - val_mse: 1.0630\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7855 - mse: 0.7855 - val_loss: 1.0625 - val_mse: 1.0625\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7853 - mse: 0.7853 - val_loss: 1.0616 - val_mse: 1.0616\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7855 - mse: 0.7855 - val_loss: 1.0605 - val_mse: 1.0605\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7853 - mse: 0.7853 - val_loss: 1.0602 - val_mse: 1.0602\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7852 - mse: 0.7852 - val_loss: 1.0598 - val_mse: 1.0598\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7852 - mse: 0.7852 - val_loss: 1.0594 - val_mse: 1.0594\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7853 - mse: 0.7853 - val_loss: 1.0592 - val_mse: 1.0592\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.0592 - mse: 1.0592\n",
      "\n",
      "Weight: [array([[1.9524325]], dtype=float32), array([1.0241684], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1387 - mse: 1.1387 - val_loss: 1.3832 - val_mse: 1.3832\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.1384 - mse: 1.1384 - val_loss: 1.3833 - val_mse: 1.3833\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1383 - mse: 1.1383 - val_loss: 1.3834 - val_mse: 1.3834\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1378 - mse: 1.1378 - val_loss: 1.3837 - val_mse: 1.3837\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1380 - mse: 1.1380 - val_loss: 1.3838 - val_mse: 1.3838\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1379 - mse: 1.1379 - val_loss: 1.3838 - val_mse: 1.3838\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1382 - mse: 1.1382 - val_loss: 1.3836 - val_mse: 1.3836\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1381 - mse: 1.1381 - val_loss: 1.3837 - val_mse: 1.3837\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1383 - mse: 1.1383 - val_loss: 1.3837 - val_mse: 1.3837\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1381 - mse: 1.1381 - val_loss: 1.3838 - val_mse: 1.3838\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.3838 - mse: 1.3838\n",
      "\n",
      "Weight: [array([[1.9419092]], dtype=float32), array([0.9950571], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/10\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0088 - mse: 0.0088\n",
      "\n",
      "Weight: [array([[1.9893533]], dtype=float32), array([0.9982804], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/10\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0882 - mse: 0.0882 - val_loss: 0.0867 - val_mse: 0.0867\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0881 - mse: 0.0881 - val_loss: 0.0869 - val_mse: 0.0869\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0880 - mse: 0.0880 - val_loss: 0.0869 - val_mse: 0.0869\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0879 - mse: 0.0879 - val_loss: 0.0871 - val_mse: 0.0871\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0879 - mse: 0.0879 - val_loss: 0.0873 - val_mse: 0.0873\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0879 - mse: 0.0879 - val_loss: 0.0873 - val_mse: 0.0873\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0879 - mse: 0.0879 - val_loss: 0.0872 - val_mse: 0.0872\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0879 - mse: 0.0879 - val_loss: 0.0873 - val_mse: 0.0873\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0879 - mse: 0.0879 - val_loss: 0.0872 - val_mse: 0.0872\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0879 - mse: 0.0879 - val_loss: 0.0873 - val_mse: 0.0873\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0873 - mse: 0.0873\n",
      "\n",
      "Weight: [array([[1.9682764]], dtype=float32), array([1.005269], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2456 - mse: 0.2456 - val_loss: 0.3159 - val_mse: 0.3159\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2446 - mse: 0.2446 - val_loss: 0.3150 - val_mse: 0.3150\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2443 - mse: 0.2443 - val_loss: 0.3146 - val_mse: 0.3146\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2441 - mse: 0.2441 - val_loss: 0.3143 - val_mse: 0.3143\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2441 - mse: 0.2441 - val_loss: 0.3143 - val_mse: 0.3143\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2439 - mse: 0.2439 - val_loss: 0.3142 - val_mse: 0.3142\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2439 - mse: 0.2439 - val_loss: 0.3143 - val_mse: 0.3143\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2438 - mse: 0.2438 - val_loss: 0.3143 - val_mse: 0.3143\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2438 - mse: 0.2438 - val_loss: 0.3143 - val_mse: 0.3143\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2437 - mse: 0.2437 - val_loss: 0.3143 - val_mse: 0.3143\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.3143 - mse: 0.3143\n",
      "\n",
      "Weight: [array([[1.9982086]], dtype=float32), array([0.9628678], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4773 - mse: 0.4773 - val_loss: 0.5700 - val_mse: 0.5700\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4750 - mse: 0.4750 - val_loss: 0.5738 - val_mse: 0.5738\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4740 - mse: 0.4740 - val_loss: 0.5762 - val_mse: 0.5762\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4735 - mse: 0.4735 - val_loss: 0.5783 - val_mse: 0.5783\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4733 - mse: 0.4733 - val_loss: 0.5800 - val_mse: 0.5800\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4733 - mse: 0.4733 - val_loss: 0.5815 - val_mse: 0.5815\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4730 - mse: 0.4730 - val_loss: 0.5823 - val_mse: 0.5823\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4729 - mse: 0.4729 - val_loss: 0.5834 - val_mse: 0.5834\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4731 - mse: 0.4731 - val_loss: 0.5838 - val_mse: 0.5838\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4731 - mse: 0.4731 - val_loss: 0.5846 - val_mse: 0.5846\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5846 - mse: 0.5846\n",
      "\n",
      "Weight: [array([[2.0261884]], dtype=float32), array([1.0377488], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.8490 - mse: 0.8490 - val_loss: 0.7838 - val_mse: 0.7838\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8485 - mse: 0.8485 - val_loss: 0.7819 - val_mse: 0.7819\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8483 - mse: 0.8483 - val_loss: 0.7802 - val_mse: 0.7802\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8483 - mse: 0.8483 - val_loss: 0.7796 - val_mse: 0.7796\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8484 - mse: 0.8484 - val_loss: 0.7786 - val_mse: 0.7786\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8481 - mse: 0.8481 - val_loss: 0.7778 - val_mse: 0.7778\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8482 - mse: 0.8482 - val_loss: 0.7778 - val_mse: 0.7778\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8479 - mse: 0.8479 - val_loss: 0.7764 - val_mse: 0.7764\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8481 - mse: 0.8481 - val_loss: 0.7750 - val_mse: 0.7750\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8481 - mse: 0.8481 - val_loss: 0.7748 - val_mse: 0.7748\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.7748 - mse: 0.7748\n",
      "\n",
      "Weight: [array([[2.0052214]], dtype=float32), array([1.0041281], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 1.3179 - mse: 1.3179 - val_loss: 0.9938 - val_mse: 0.9938\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.3162 - mse: 1.3162 - val_loss: 0.9915 - val_mse: 0.9915\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.3152 - mse: 1.3152 - val_loss: 0.9903 - val_mse: 0.9903\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.3150 - mse: 1.3150 - val_loss: 0.9905 - val_mse: 0.9905\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.3148 - mse: 1.3148 - val_loss: 0.9890 - val_mse: 0.9890\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.3146 - mse: 1.3146 - val_loss: 0.9889 - val_mse: 0.9889\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.3150 - mse: 1.3150 - val_loss: 0.9895 - val_mse: 0.9895\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.3142 - mse: 1.3142 - val_loss: 0.9889 - val_mse: 0.9889\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.3142 - mse: 1.3142 - val_loss: 0.9892 - val_mse: 0.9892\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.3148 - mse: 1.3148 - val_loss: 0.9890 - val_mse: 0.9890\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.9890 - mse: 0.9890\n",
      "\n",
      "Weight: [array([[1.9623699]], dtype=float32), array([0.94895333], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0086 - mse: 0.0086\n",
      "\n",
      "Weight: [array([[1.9908036]], dtype=float32), array([0.9971563], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0771 - mse: 0.0771 - val_loss: 0.1265 - val_mse: 0.1265\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0770 - mse: 0.0770 - val_loss: 0.1265 - val_mse: 0.1265\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0770 - mse: 0.0770 - val_loss: 0.1266 - val_mse: 0.1266\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0770 - mse: 0.0770 - val_loss: 0.1267 - val_mse: 0.1267\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0770 - mse: 0.0770 - val_loss: 0.1268 - val_mse: 0.1268\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0770 - mse: 0.0770 - val_loss: 0.1269 - val_mse: 0.1269\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0770 - mse: 0.0770 - val_loss: 0.1269 - val_mse: 0.1269\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0770 - mse: 0.0770 - val_loss: 0.1270 - val_mse: 0.1270\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0770 - mse: 0.0770 - val_loss: 0.1270 - val_mse: 0.1270\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0770 - mse: 0.0770 - val_loss: 0.1271 - val_mse: 0.1271\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.1271 - mse: 0.1271\n",
      "\n",
      "Weight: [array([[2.0037782]], dtype=float32), array([1.0018861], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2327 - mse: 0.2327 - val_loss: 0.2300 - val_mse: 0.2300\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2322 - mse: 0.2322 - val_loss: 0.2292 - val_mse: 0.2292\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2321 - mse: 0.2321 - val_loss: 0.2284 - val_mse: 0.2284\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2318 - mse: 0.2318 - val_loss: 0.2278 - val_mse: 0.2278\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2319 - mse: 0.2319 - val_loss: 0.2273 - val_mse: 0.2273\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2317 - mse: 0.2317 - val_loss: 0.2272 - val_mse: 0.2272\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2318 - mse: 0.2318 - val_loss: 0.2270 - val_mse: 0.2270\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2317 - mse: 0.2317 - val_loss: 0.2268 - val_mse: 0.2268\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2316 - mse: 0.2316 - val_loss: 0.2268 - val_mse: 0.2268\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2317 - mse: 0.2317 - val_loss: 0.2268 - val_mse: 0.2268\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2268 - mse: 0.2268\n",
      "\n",
      "Weight: [array([[1.9802116]], dtype=float32), array([1.0374187], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.5211 - mse: 0.5211 - val_loss: 0.4479 - val_mse: 0.4479\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5198 - mse: 0.5198 - val_loss: 0.4496 - val_mse: 0.4496\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5190 - mse: 0.5190 - val_loss: 0.4512 - val_mse: 0.4512\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5188 - mse: 0.5188 - val_loss: 0.4527 - val_mse: 0.4527\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5186 - mse: 0.5186 - val_loss: 0.4539 - val_mse: 0.4539\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5184 - mse: 0.5184 - val_loss: 0.4550 - val_mse: 0.4550\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5182 - mse: 0.5182 - val_loss: 0.4560 - val_mse: 0.4560\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5182 - mse: 0.5182 - val_loss: 0.4569 - val_mse: 0.4569\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5182 - mse: 0.5182 - val_loss: 0.4576 - val_mse: 0.4576\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5180 - mse: 0.5180 - val_loss: 0.4584 - val_mse: 0.4584\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4584 - mse: 0.4584\n",
      "\n",
      "Weight: [array([[2.0339215]], dtype=float32), array([0.98286265], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.7602 - mse: 0.7602 - val_loss: 0.7080 - val_mse: 0.7080\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7601 - mse: 0.7601 - val_loss: 0.7067 - val_mse: 0.7067\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7603 - mse: 0.7603 - val_loss: 0.7058 - val_mse: 0.7058\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7601 - mse: 0.7601 - val_loss: 0.7048 - val_mse: 0.7048\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7601 - mse: 0.7601 - val_loss: 0.7041 - val_mse: 0.7041\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7603 - mse: 0.7603 - val_loss: 0.7036 - val_mse: 0.7036\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7597 - mse: 0.7597 - val_loss: 0.7030 - val_mse: 0.7030\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7599 - mse: 0.7599 - val_loss: 0.7029 - val_mse: 0.7029\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7599 - mse: 0.7599 - val_loss: 0.7023 - val_mse: 0.7023\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7599 - mse: 0.7599 - val_loss: 0.7017 - val_mse: 0.7017\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.7017 - mse: 0.7017\n",
      "\n",
      "Weight: [array([[2.009064]], dtype=float32), array([0.9789069], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.2481 - mse: 1.2481 - val_loss: 0.9898 - val_mse: 0.9898\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2461 - mse: 1.2461 - val_loss: 0.9911 - val_mse: 0.9911\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2450 - mse: 1.2450 - val_loss: 0.9921 - val_mse: 0.9921\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2449 - mse: 1.2449 - val_loss: 0.9932 - val_mse: 0.9932\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2446 - mse: 1.2446 - val_loss: 0.9931 - val_mse: 0.9931\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2446 - mse: 1.2446 - val_loss: 0.9936 - val_mse: 0.9936\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2450 - mse: 1.2450 - val_loss: 0.9942 - val_mse: 0.9942\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2443 - mse: 1.2443 - val_loss: 0.9943 - val_mse: 0.9943\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2446 - mse: 1.2446 - val_loss: 0.9943 - val_mse: 0.9943\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2444 - mse: 1.2444 - val_loss: 0.9948 - val_mse: 0.9948\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.9948 - mse: 0.9948\n",
      "\n",
      "Weight: [array([[2.0099156]], dtype=float32), array([1.0503142], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0079 - mse: 0.0079\n",
      "\n",
      "Weight: [array([[1.9952193]], dtype=float32), array([1.0020437], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0957 - mse: 0.0957 - val_loss: 0.0657 - val_mse: 0.0657\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0957 - mse: 0.0957 - val_loss: 0.0659 - val_mse: 0.0659\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0957 - mse: 0.0957 - val_loss: 0.0661 - val_mse: 0.0661\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0956 - mse: 0.0956 - val_loss: 0.0663 - val_mse: 0.0663\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0956 - mse: 0.0956 - val_loss: 0.0664 - val_mse: 0.0664\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0956 - mse: 0.0956 - val_loss: 0.0665 - val_mse: 0.0665\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0956 - mse: 0.0956 - val_loss: 0.0665 - val_mse: 0.0665\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0956 - mse: 0.0956 - val_loss: 0.0666 - val_mse: 0.0666\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0956 - mse: 0.0956 - val_loss: 0.0667 - val_mse: 0.0667\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0956 - mse: 0.0956 - val_loss: 0.0668 - val_mse: 0.0668\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0668 - mse: 0.0668\n",
      "\n",
      "Weight: [array([[1.977266]], dtype=float32), array([0.99890643], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2375 - mse: 0.2375 - val_loss: 0.2086 - val_mse: 0.2086\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2372 - mse: 0.2372 - val_loss: 0.2077 - val_mse: 0.2077\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2370 - mse: 0.2370 - val_loss: 0.2073 - val_mse: 0.2073\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2370 - mse: 0.2370 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2368 - mse: 0.2368 - val_loss: 0.2062 - val_mse: 0.2062\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2367 - mse: 0.2367 - val_loss: 0.2058 - val_mse: 0.2058\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2366 - mse: 0.2366 - val_loss: 0.2056 - val_mse: 0.2056\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2367 - mse: 0.2367 - val_loss: 0.2056 - val_mse: 0.2056\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2367 - mse: 0.2367 - val_loss: 0.2055 - val_mse: 0.2055\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2367 - mse: 0.2367 - val_loss: 0.2054 - val_mse: 0.2054\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2054 - mse: 0.2054\n",
      "\n",
      "Weight: [array([[2.0173125]], dtype=float32), array([0.9806495], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4930 - mse: 0.4930 - val_loss: 0.4192 - val_mse: 0.4192\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4928 - mse: 0.4928 - val_loss: 0.4183 - val_mse: 0.4183\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4925 - mse: 0.4925 - val_loss: 0.4176 - val_mse: 0.4176\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4924 - mse: 0.4924 - val_loss: 0.4170 - val_mse: 0.4170\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4925 - mse: 0.4925 - val_loss: 0.4169 - val_mse: 0.4169\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4923 - mse: 0.4923 - val_loss: 0.4171 - val_mse: 0.4171\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4925 - mse: 0.4925 - val_loss: 0.4171 - val_mse: 0.4171\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4924 - mse: 0.4924 - val_loss: 0.4171 - val_mse: 0.4171\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4925 - mse: 0.4925 - val_loss: 0.4170 - val_mse: 0.4170\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4924 - mse: 0.4924 - val_loss: 0.4172 - val_mse: 0.4172\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4172 - mse: 0.4172\n",
      "\n",
      "Weight: [array([[2.0165179]], dtype=float32), array([1.004622], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8674 - mse: 0.8674 - val_loss: 0.6843 - val_mse: 0.6843\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8671 - mse: 0.8671 - val_loss: 0.6849 - val_mse: 0.6849\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8670 - mse: 0.8670 - val_loss: 0.6855 - val_mse: 0.6855\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8668 - mse: 0.8668 - val_loss: 0.6859 - val_mse: 0.6859\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8669 - mse: 0.8669 - val_loss: 0.6864 - val_mse: 0.6864\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8667 - mse: 0.8667 - val_loss: 0.6868 - val_mse: 0.6868\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8666 - mse: 0.8666 - val_loss: 0.6870 - val_mse: 0.6870\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8667 - mse: 0.8667 - val_loss: 0.6872 - val_mse: 0.6872\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8667 - mse: 0.8667 - val_loss: 0.6875 - val_mse: 0.6875\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8666 - mse: 0.8666 - val_loss: 0.6877 - val_mse: 0.6877\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.6877 - mse: 0.6877\n",
      "\n",
      "Weight: [array([[1.9841669]], dtype=float32), array([1.0283048], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2646 - mse: 1.2646 - val_loss: 1.3823 - val_mse: 1.3823\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2636 - mse: 1.2636 - val_loss: 1.3793 - val_mse: 1.3793\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2629 - mse: 1.2629 - val_loss: 1.3766 - val_mse: 1.3766\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2628 - mse: 1.2628 - val_loss: 1.3761 - val_mse: 1.3761\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2625 - mse: 1.2625 - val_loss: 1.3753 - val_mse: 1.3753\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2625 - mse: 1.2625 - val_loss: 1.3743 - val_mse: 1.3743\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2624 - mse: 1.2624 - val_loss: 1.3732 - val_mse: 1.3732\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2624 - mse: 1.2624 - val_loss: 1.3732 - val_mse: 1.3732\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2619 - mse: 1.2619 - val_loss: 1.3731 - val_mse: 1.3731\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2623 - mse: 1.2623 - val_loss: 1.3726 - val_mse: 1.3726\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.3726 - mse: 1.3726\n",
      "\n",
      "Weight: [array([[1.9267598]], dtype=float32), array([0.99062663], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0130 - val_mse: 0.0130\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0125 - val_mse: 0.0125\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0113 - mse: 0.0113\n",
      "\n",
      "Weight: [array([[1.9950836]], dtype=float32), array([0.997123], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0609 - val_mse: 0.0609\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0609 - val_mse: 0.0609\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0608 - val_mse: 0.0608\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0609 - val_mse: 0.0609\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0607 - val_mse: 0.0607\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0606 - val_mse: 0.0606\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0606 - val_mse: 0.0606\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0608 - val_mse: 0.0608\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0603 - val_mse: 0.0603\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0601 - val_mse: 0.0601\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0601 - mse: 0.0601\n",
      "\n",
      "Weight: [array([[2.004345]], dtype=float32), array([1.0089891], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2540 - mse: 0.2540 - val_loss: 0.1676 - val_mse: 0.1676\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2539 - mse: 0.2539 - val_loss: 0.1684 - val_mse: 0.1684\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2537 - mse: 0.2537 - val_loss: 0.1692 - val_mse: 0.1692\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2538 - mse: 0.2538 - val_loss: 0.1699 - val_mse: 0.1699\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2537 - mse: 0.2537 - val_loss: 0.1705 - val_mse: 0.1705\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2537 - mse: 0.2537 - val_loss: 0.1712 - val_mse: 0.1712\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.1714 - val_mse: 0.1714\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.1717 - val_mse: 0.1717\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.1720 - val_mse: 0.1720\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.1716 - val_mse: 0.1716\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.1716 - mse: 0.1716\n",
      "\n",
      "Weight: [array([[2.0171807]], dtype=float32), array([0.986409], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4849 - mse: 0.4849 - val_loss: 0.6353 - val_mse: 0.6353\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4849 - mse: 0.4849 - val_loss: 0.6357 - val_mse: 0.6357\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4847 - mse: 0.4847 - val_loss: 0.6422 - val_mse: 0.6422\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4843 - mse: 0.4843 - val_loss: 0.6439 - val_mse: 0.6439\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4845 - mse: 0.4845 - val_loss: 0.6443 - val_mse: 0.6443\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4844 - mse: 0.4844 - val_loss: 0.6455 - val_mse: 0.6455\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4845 - mse: 0.4845 - val_loss: 0.6426 - val_mse: 0.6426\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4842 - mse: 0.4842 - val_loss: 0.6449 - val_mse: 0.6449\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4842 - mse: 0.4842 - val_loss: 0.6407 - val_mse: 0.6407\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4843 - mse: 0.4843 - val_loss: 0.6433 - val_mse: 0.6433\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.6433 - mse: 0.6433\n",
      "\n",
      "Weight: [array([[2.052697]], dtype=float32), array([0.99447256], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.7757 - mse: 0.7757 - val_loss: 0.8622 - val_mse: 0.8622\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7752 - mse: 0.7752 - val_loss: 0.8620 - val_mse: 0.8620\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7752 - mse: 0.7752 - val_loss: 0.8644 - val_mse: 0.8644\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7753 - mse: 0.7753 - val_loss: 0.8599 - val_mse: 0.8599\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7751 - mse: 0.7751 - val_loss: 0.8558 - val_mse: 0.8558\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7754 - mse: 0.7754 - val_loss: 0.8563 - val_mse: 0.8563\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7752 - mse: 0.7752 - val_loss: 0.8611 - val_mse: 0.8611\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7749 - mse: 0.7749 - val_loss: 0.8625 - val_mse: 0.8625\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7751 - mse: 0.7751 - val_loss: 0.8559 - val_mse: 0.8559\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7747 - mse: 0.7747 - val_loss: 0.8570 - val_mse: 0.8570\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.8570 - mse: 0.8570\n",
      "\n",
      "Weight: [array([[2.085607]], dtype=float32), array([0.9701897], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2430 - mse: 1.2430 - val_loss: 0.9311 - val_mse: 0.9311\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2399 - mse: 1.2399 - val_loss: 0.9438 - val_mse: 0.9438\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2392 - mse: 1.2392 - val_loss: 0.9462 - val_mse: 0.9462\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2394 - mse: 1.2394 - val_loss: 0.9488 - val_mse: 0.9488\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2394 - mse: 1.2394 - val_loss: 0.9443 - val_mse: 0.9443\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2390 - mse: 1.2390 - val_loss: 0.9424 - val_mse: 0.9424\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2393 - mse: 1.2393 - val_loss: 0.9418 - val_mse: 0.9418\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2395 - mse: 1.2395 - val_loss: 0.9469 - val_mse: 0.9469\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2389 - mse: 1.2389 - val_loss: 0.9419 - val_mse: 0.9419\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2391 - mse: 1.2391 - val_loss: 0.9425 - val_mse: 0.9425\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.9425 - mse: 0.9425\n",
      "\n",
      "Weight: [array([[2.0923033]], dtype=float32), array([1.0452201], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101\n",
      "\n",
      "Weight: [array([[2.0096688]], dtype=float32), array([0.99748504], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0825 - mse: 0.0825 - val_loss: 0.0950 - val_mse: 0.0950\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0825 - mse: 0.0825 - val_loss: 0.0952 - val_mse: 0.0952\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0825 - mse: 0.0825 - val_loss: 0.0952 - val_mse: 0.0952\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0825 - mse: 0.0825 - val_loss: 0.0953 - val_mse: 0.0953\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0825 - mse: 0.0825 - val_loss: 0.0954 - val_mse: 0.0954\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0825 - mse: 0.0825 - val_loss: 0.0954 - val_mse: 0.0954\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0825 - mse: 0.0825 - val_loss: 0.0954 - val_mse: 0.0954\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0825 - mse: 0.0825 - val_loss: 0.0954 - val_mse: 0.0954\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0825 - mse: 0.0825 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0825 - mse: 0.0825 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0955 - mse: 0.0955\n",
      "\n",
      "Weight: [array([[1.9944916]], dtype=float32), array([1.0022353], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2018 - val_mse: 0.2018\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2511 - mse: 0.2511 - val_loss: 0.2022 - val_mse: 0.2022\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2511 - mse: 0.2511 - val_loss: 0.2025 - val_mse: 0.2025\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2511 - mse: 0.2511 - val_loss: 0.2023 - val_mse: 0.2023\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2511 - mse: 0.2511 - val_loss: 0.2027 - val_mse: 0.2027\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2511 - mse: 0.2511 - val_loss: 0.2028 - val_mse: 0.2028\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2511 - mse: 0.2511 - val_loss: 0.2023 - val_mse: 0.2023\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2511 - mse: 0.2511 - val_loss: 0.2024 - val_mse: 0.2024\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2512 - mse: 0.2512 - val_loss: 0.2027 - val_mse: 0.2027\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2511 - mse: 0.2511 - val_loss: 0.2027 - val_mse: 0.2027\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2027 - mse: 0.2027\n",
      "\n",
      "Weight: [array([[2.0007002]], dtype=float32), array([1.0253628], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5082 - mse: 0.5082 - val_loss: 0.4196 - val_mse: 0.4196\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4188 - val_mse: 0.4188\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5078 - mse: 0.5078 - val_loss: 0.4187 - val_mse: 0.4187\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5078 - mse: 0.5078 - val_loss: 0.4184 - val_mse: 0.4184\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4189 - val_mse: 0.4189\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4195 - val_mse: 0.4195\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5078 - mse: 0.5078 - val_loss: 0.4197 - val_mse: 0.4197\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4190 - val_mse: 0.4190\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4189 - val_mse: 0.4189\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4186 - val_mse: 0.4186\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4186 - mse: 0.4186\n",
      "\n",
      "Weight: [array([[2.0094717]], dtype=float32), array([1.0036477], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8541 - mse: 0.8541 - val_loss: 0.7998 - val_mse: 0.7998\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8534 - mse: 0.8534 - val_loss: 0.7992 - val_mse: 0.7992\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8532 - mse: 0.8532 - val_loss: 0.7983 - val_mse: 0.7983\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8531 - mse: 0.8531 - val_loss: 0.7965 - val_mse: 0.7965\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8527 - mse: 0.8527 - val_loss: 0.7973 - val_mse: 0.7973\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8529 - mse: 0.8529 - val_loss: 0.7985 - val_mse: 0.7985\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8529 - mse: 0.8529 - val_loss: 0.7970 - val_mse: 0.7970\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8526 - mse: 0.8526 - val_loss: 0.7961 - val_mse: 0.7961\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8528 - mse: 0.8528 - val_loss: 0.7963 - val_mse: 0.7963\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8531 - mse: 0.8531 - val_loss: 0.7972 - val_mse: 0.7972\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.7972 - mse: 0.7972\n",
      "\n",
      "Weight: [array([[1.969065]], dtype=float32), array([0.96770394], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2154 - mse: 1.2154 - val_loss: 1.5014 - val_mse: 1.5014\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2151 - mse: 1.2151 - val_loss: 1.4996 - val_mse: 1.4996\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2150 - mse: 1.2150 - val_loss: 1.5037 - val_mse: 1.5037\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2149 - mse: 1.2149 - val_loss: 1.5073 - val_mse: 1.5073\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2148 - mse: 1.2148 - val_loss: 1.5077 - val_mse: 1.5077\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2149 - mse: 1.2149 - val_loss: 1.5023 - val_mse: 1.5023\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2149 - mse: 1.2149 - val_loss: 1.5024 - val_mse: 1.5024\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2148 - mse: 1.2148 - val_loss: 1.5006 - val_mse: 1.5006\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2145 - mse: 1.2145 - val_loss: 1.5049 - val_mse: 1.5049\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2144 - mse: 1.2144 - val_loss: 1.5051 - val_mse: 1.5051\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.5051 - mse: 1.5051\n",
      "\n",
      "Weight: [array([[2.013701]], dtype=float32), array([0.9795867], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "x_valid = np.random.uniform(-1, 1, 50)\n",
    "x_valid.sort()\n",
    "y_target = m * x_valid + b # ideal (target) linear function\n",
    "\n",
    "N_train = np.zeros(6)\n",
    "sigma = np.zeros(6)\n",
    "score10 = np.zeros((6,6,2))\n",
    "weight10 = np.zeros((6,6,2))\n",
    "\n",
    "history10 = []\n",
    "\n",
    "\n",
    "#GRID SEARCH\n",
    "for i in range(6):\n",
    "    N_train[i] = 500 + i*100\n",
    "    row = []\n",
    "    for j in range(6):\n",
    "        sigma[j] = 0.1 + j*0.2 # noise standard deviation\n",
    "        x_train = np.random.uniform(-1, 1, int(N_train[i]))\n",
    "    \n",
    "        y_train = np.random.normal(m * x_train + b, sigma[j]) # actual measures from which we want to guess regression parameters\n",
    "        y_valid = np.random.normal(m * x_valid + b, sigma[j])\n",
    "        # fit the model using training dataset\n",
    "        # over 10 epochs of 32 batch size each\n",
    "        # report training progress against validation data\n",
    "        print(\"\\n\\nN_train = \",N_train[i])\n",
    "        print(\"Sigma = \",sigma[j])\n",
    "        row.append(model.fit(x=x_train, y=y_train, \n",
    "              batch_size=32, epochs=10,\n",
    "              shuffle=True, # a good idea is to shuffle input before at each epoch\n",
    "              validation_data=(x_valid, y_valid)))\n",
    "        score10[i][j] = model.evaluate(x_valid, y_valid, batch_size=32, verbose=1)\n",
    "        pesi = model.get_weights()\n",
    "        weight10[i][j] = [pesi[0][0][0],pesi[1][0]]\n",
    "        print(\"\\nWeight:\",pesi)\n",
    "    history10.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruisco una rete neurale con $N_{epochs}=20$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0109 - mse: 0.0109\n",
      "\n",
      "Weight: [array([[2.0039518]], dtype=float32), array([0.9949785], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0863 - mse: 0.0863 - val_loss: 0.0943 - val_mse: 0.0943\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0862 - mse: 0.0862 - val_loss: 0.0942 - val_mse: 0.0942\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0862 - mse: 0.0862 - val_loss: 0.0941 - val_mse: 0.0941\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0862 - mse: 0.0862 - val_loss: 0.0940 - val_mse: 0.0940\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0940 - val_mse: 0.0940\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0940 - val_mse: 0.0940\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0940 - val_mse: 0.0940\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0940 - val_mse: 0.0940\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0940 - val_mse: 0.0940\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0939 - val_mse: 0.0939\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0939 - val_mse: 0.0939\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0939 - val_mse: 0.0939\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0939 - val_mse: 0.0939\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0939 - val_mse: 0.0939\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0939 - val_mse: 0.0939\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0939 - val_mse: 0.0939\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0939 - val_mse: 0.0939\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0939 - val_mse: 0.0939\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0939 - val_mse: 0.0939\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0940 - val_mse: 0.0940\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0940 - mse: 0.0940\n",
      "\n",
      "Weight: [array([[1.9836476]], dtype=float32), array([1.001433], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2387 - mse: 0.2387 - val_loss: 0.1705 - val_mse: 0.1705\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2386 - mse: 0.2386 - val_loss: 0.1703 - val_mse: 0.1703\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2384 - mse: 0.2384 - val_loss: 0.1701 - val_mse: 0.1701\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2383 - mse: 0.2383 - val_loss: 0.1699 - val_mse: 0.1699\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2383 - mse: 0.2383 - val_loss: 0.1696 - val_mse: 0.1696\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2382 - mse: 0.2382 - val_loss: 0.1695 - val_mse: 0.1695\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2381 - mse: 0.2381 - val_loss: 0.1693 - val_mse: 0.1693\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2380 - mse: 0.2380 - val_loss: 0.1693 - val_mse: 0.1693\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2381 - mse: 0.2381 - val_loss: 0.1694 - val_mse: 0.1694\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2380 - mse: 0.2380 - val_loss: 0.1692 - val_mse: 0.1692\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2378 - mse: 0.2378 - val_loss: 0.1691 - val_mse: 0.1691\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2378 - mse: 0.2378 - val_loss: 0.1692 - val_mse: 0.1692\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2378 - mse: 0.2378 - val_loss: 0.1693 - val_mse: 0.1693\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2378 - mse: 0.2378 - val_loss: 0.1690 - val_mse: 0.1690\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2378 - mse: 0.2378 - val_loss: 0.1691 - val_mse: 0.1691\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2378 - mse: 0.2378 - val_loss: 0.1690 - val_mse: 0.1690\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2378 - mse: 0.2378 - val_loss: 0.1688 - val_mse: 0.1688\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2378 - mse: 0.2378 - val_loss: 0.1688 - val_mse: 0.1688\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2378 - mse: 0.2378 - val_loss: 0.1688 - val_mse: 0.1688\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2378 - mse: 0.2378 - val_loss: 0.1688 - val_mse: 0.1688\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.1688 - mse: 0.1688\n",
      "\n",
      "Weight: [array([[2.0350623]], dtype=float32), array([0.9986359], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5176 - mse: 0.5176 - val_loss: 0.4958 - val_mse: 0.4958\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5174 - mse: 0.5174 - val_loss: 0.4956 - val_mse: 0.4956\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5175 - mse: 0.5175 - val_loss: 0.4953 - val_mse: 0.4953\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5175 - mse: 0.5175 - val_loss: 0.4949 - val_mse: 0.4949\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5174 - mse: 0.5174 - val_loss: 0.4950 - val_mse: 0.4950\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5174 - mse: 0.5174 - val_loss: 0.4947 - val_mse: 0.4947\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5174 - mse: 0.5174 - val_loss: 0.4948 - val_mse: 0.4948\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5176 - mse: 0.5176 - val_loss: 0.4949 - val_mse: 0.4949\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5176 - mse: 0.5176 - val_loss: 0.4947 - val_mse: 0.4947\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5176 - mse: 0.5176 - val_loss: 0.4946 - val_mse: 0.4946\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5174 - mse: 0.5174 - val_loss: 0.4948 - val_mse: 0.4948\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5175 - mse: 0.5175 - val_loss: 0.4949 - val_mse: 0.4949\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5174 - mse: 0.5174 - val_loss: 0.4949 - val_mse: 0.4949\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5175 - mse: 0.5175 - val_loss: 0.4949 - val_mse: 0.4949\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5177 - mse: 0.5177 - val_loss: 0.4947 - val_mse: 0.4947\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5175 - mse: 0.5175 - val_loss: 0.4946 - val_mse: 0.4946\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5175 - mse: 0.5175 - val_loss: 0.4946 - val_mse: 0.4946\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5174 - mse: 0.5174 - val_loss: 0.4943 - val_mse: 0.4943\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5173 - mse: 0.5173 - val_loss: 0.4944 - val_mse: 0.4944\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5174 - mse: 0.5174 - val_loss: 0.4946 - val_mse: 0.4946\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4946 - mse: 0.4946\n",
      "\n",
      "Weight: [array([[2.0368702]], dtype=float32), array([1.0106893], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6849 - mse: 0.6849 - val_loss: 0.8920 - val_mse: 0.8920\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6830 - mse: 0.6830 - val_loss: 0.8832 - val_mse: 0.8832\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6817 - mse: 0.6817 - val_loss: 0.8763 - val_mse: 0.8763\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6810 - mse: 0.6810 - val_loss: 0.8708 - val_mse: 0.8708\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6803 - mse: 0.6803 - val_loss: 0.8685 - val_mse: 0.8685\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6805 - mse: 0.6805 - val_loss: 0.8658 - val_mse: 0.8658\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6804 - mse: 0.6804 - val_loss: 0.8641 - val_mse: 0.8641\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6803 - mse: 0.6803 - val_loss: 0.8613 - val_mse: 0.8613\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6799 - mse: 0.6799 - val_loss: 0.8596 - val_mse: 0.8596\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6802 - mse: 0.6802 - val_loss: 0.8604 - val_mse: 0.8604\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6801 - mse: 0.6801 - val_loss: 0.8604 - val_mse: 0.8604\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6801 - mse: 0.6801 - val_loss: 0.8588 - val_mse: 0.8588\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6800 - mse: 0.6800 - val_loss: 0.8592 - val_mse: 0.8592\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6801 - mse: 0.6801 - val_loss: 0.8601 - val_mse: 0.8601\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6799 - mse: 0.6799 - val_loss: 0.8595 - val_mse: 0.8595\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6801 - mse: 0.6801 - val_loss: 0.8580 - val_mse: 0.8580\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6802 - mse: 0.6802 - val_loss: 0.8593 - val_mse: 0.8593\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6799 - mse: 0.6799 - val_loss: 0.8593 - val_mse: 0.8593\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6799 - mse: 0.6799 - val_loss: 0.8589 - val_mse: 0.8589\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6799 - mse: 0.6799 - val_loss: 0.8575 - val_mse: 0.8575\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.8575 - mse: 0.8575\n",
      "\n",
      "Weight: [array([[1.9849898]], dtype=float32), array([0.93739194], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1949 - mse: 1.1949 - val_loss: 1.7527 - val_mse: 1.7527\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1944 - mse: 1.1944 - val_loss: 1.7523 - val_mse: 1.7523\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1930 - mse: 1.1930 - val_loss: 1.7504 - val_mse: 1.7504\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1929 - mse: 1.1929 - val_loss: 1.7483 - val_mse: 1.7483\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1919 - mse: 1.1919 - val_loss: 1.7483 - val_mse: 1.7483\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1923 - mse: 1.1923 - val_loss: 1.7484 - val_mse: 1.7484\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1916 - mse: 1.1916 - val_loss: 1.7461 - val_mse: 1.7461\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1916 - mse: 1.1916 - val_loss: 1.7458 - val_mse: 1.7458\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1915 - mse: 1.1915 - val_loss: 1.7418 - val_mse: 1.7418\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1917 - mse: 1.1917 - val_loss: 1.7404 - val_mse: 1.7404\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1914 - mse: 1.1914 - val_loss: 1.7400 - val_mse: 1.7400\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1908 - mse: 1.1908 - val_loss: 1.7442 - val_mse: 1.7442\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1911 - mse: 1.1911 - val_loss: 1.7433 - val_mse: 1.7433\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1908 - mse: 1.1908 - val_loss: 1.7431 - val_mse: 1.7431\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1907 - mse: 1.1907 - val_loss: 1.7432 - val_mse: 1.7432\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1905 - mse: 1.1905 - val_loss: 1.7427 - val_mse: 1.7427\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1909 - mse: 1.1909 - val_loss: 1.7452 - val_mse: 1.7452\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1909 - mse: 1.1909 - val_loss: 1.7459 - val_mse: 1.7459\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1906 - mse: 1.1906 - val_loss: 1.7448 - val_mse: 1.7448\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1903 - mse: 1.1903 - val_loss: 1.7452 - val_mse: 1.7452\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.7452 - mse: 1.7452\n",
      "\n",
      "Weight: [array([[2.0869648]], dtype=float32), array([0.94171256], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/20\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0126 - val_mse: 0.0126\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0108 - mse: 0.0108\n",
      "\n",
      "Weight: [array([[1.9992203]], dtype=float32), array([1.00267], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/20\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0786 - mse: 0.0786 - val_loss: 0.0842 - val_mse: 0.0842\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0786 - mse: 0.0786 - val_loss: 0.0846 - val_mse: 0.0846\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0785 - mse: 0.0785 - val_loss: 0.0847 - val_mse: 0.0847\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0785 - mse: 0.0785 - val_loss: 0.0849 - val_mse: 0.0849\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0785 - mse: 0.0785 - val_loss: 0.0849 - val_mse: 0.0849\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0785 - mse: 0.0785 - val_loss: 0.0850 - val_mse: 0.0850\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0786 - mse: 0.0786 - val_loss: 0.0849 - val_mse: 0.0849\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0785 - mse: 0.0785 - val_loss: 0.0848 - val_mse: 0.0848\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0785 - mse: 0.0785 - val_loss: 0.0849 - val_mse: 0.0849\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0785 - mse: 0.0785 - val_loss: 0.0850 - val_mse: 0.0850\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0785 - mse: 0.0785 - val_loss: 0.0851 - val_mse: 0.0851\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0785 - mse: 0.0785 - val_loss: 0.0851 - val_mse: 0.0851\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0785 - mse: 0.0785 - val_loss: 0.0851 - val_mse: 0.0851\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0786 - mse: 0.0786 - val_loss: 0.0850 - val_mse: 0.0850\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0785 - mse: 0.0785 - val_loss: 0.0849 - val_mse: 0.0849\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0785 - mse: 0.0785 - val_loss: 0.0849 - val_mse: 0.0849\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0785 - mse: 0.0785 - val_loss: 0.0850 - val_mse: 0.0850\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0785 - mse: 0.0785 - val_loss: 0.0850 - val_mse: 0.0850\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0786 - mse: 0.0786 - val_loss: 0.0851 - val_mse: 0.0851\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0786 - mse: 0.0786 - val_loss: 0.0852 - val_mse: 0.0852\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0852 - mse: 0.0852\n",
      "\n",
      "Weight: [array([[1.994643]], dtype=float32), array([0.9949376], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/20\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2435 - mse: 0.2435 - val_loss: 0.2878 - val_mse: 0.2878\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2430 - mse: 0.2430 - val_loss: 0.2913 - val_mse: 0.2913\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2428 - mse: 0.2428 - val_loss: 0.2935 - val_mse: 0.2935\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2426 - mse: 0.2426 - val_loss: 0.2948 - val_mse: 0.2948\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2426 - mse: 0.2426 - val_loss: 0.2962 - val_mse: 0.2962\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2426 - mse: 0.2426 - val_loss: 0.2966 - val_mse: 0.2966\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2425 - mse: 0.2425 - val_loss: 0.2968 - val_mse: 0.2968\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2426 - mse: 0.2426 - val_loss: 0.2960 - val_mse: 0.2960\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2426 - mse: 0.2426 - val_loss: 0.2963 - val_mse: 0.2963\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2426 - mse: 0.2426 - val_loss: 0.2969 - val_mse: 0.2969\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2425 - mse: 0.2425 - val_loss: 0.2977 - val_mse: 0.2977\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2425 - mse: 0.2425 - val_loss: 0.2969 - val_mse: 0.2969\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2426 - mse: 0.2426 - val_loss: 0.2967 - val_mse: 0.2967\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2426 - mse: 0.2426 - val_loss: 0.2965 - val_mse: 0.2965\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2425 - mse: 0.2425 - val_loss: 0.2973 - val_mse: 0.2973\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2425 - mse: 0.2425 - val_loss: 0.2967 - val_mse: 0.2967\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2425 - mse: 0.2425 - val_loss: 0.2980 - val_mse: 0.2980\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2425 - mse: 0.2425 - val_loss: 0.2979 - val_mse: 0.2979\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2425 - mse: 0.2425 - val_loss: 0.2983 - val_mse: 0.2983\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2425 - mse: 0.2425 - val_loss: 0.2973 - val_mse: 0.2973\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2973 - mse: 0.2973\n",
      "\n",
      "Weight: [array([[1.9708583]], dtype=float32), array([0.9624974], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/20\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4649 - mse: 0.4649 - val_loss: 0.6808 - val_mse: 0.6808\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4645 - mse: 0.4645 - val_loss: 0.6828 - val_mse: 0.6828\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4643 - mse: 0.4643 - val_loss: 0.6845 - val_mse: 0.6845\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4644 - mse: 0.4644 - val_loss: 0.6858 - val_mse: 0.6858\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4641 - mse: 0.4641 - val_loss: 0.6863 - val_mse: 0.6863\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4641 - mse: 0.4641 - val_loss: 0.6864 - val_mse: 0.6864\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4642 - mse: 0.4642 - val_loss: 0.6872 - val_mse: 0.6872\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4641 - mse: 0.4641 - val_loss: 0.6870 - val_mse: 0.6870\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4641 - mse: 0.4641 - val_loss: 0.6872 - val_mse: 0.6872\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4641 - mse: 0.4641 - val_loss: 0.6879 - val_mse: 0.6879\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4641 - mse: 0.4641 - val_loss: 0.6886 - val_mse: 0.6886\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4640 - mse: 0.4640 - val_loss: 0.6881 - val_mse: 0.6881\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4641 - mse: 0.4641 - val_loss: 0.6887 - val_mse: 0.6887\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4641 - mse: 0.4641 - val_loss: 0.6889 - val_mse: 0.6889\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4641 - mse: 0.4641 - val_loss: 0.6896 - val_mse: 0.6896\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4641 - mse: 0.4641 - val_loss: 0.6893 - val_mse: 0.6893\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4641 - mse: 0.4641 - val_loss: 0.6895 - val_mse: 0.6895\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4639 - mse: 0.4639 - val_loss: 0.6894 - val_mse: 0.6894\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4641 - mse: 0.4641 - val_loss: 0.6897 - val_mse: 0.6897\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4640 - mse: 0.4640 - val_loss: 0.6893 - val_mse: 0.6893\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.6893 - mse: 0.6893\n",
      "\n",
      "Weight: [array([[1.9460938]], dtype=float32), array([0.99644583], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/20\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.7899 - mse: 0.7899 - val_loss: 0.7534 - val_mse: 0.7534\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.7887 - mse: 0.7887 - val_loss: 0.7544 - val_mse: 0.7544\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7880 - mse: 0.7880 - val_loss: 0.7554 - val_mse: 0.7554\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7876 - mse: 0.7876 - val_loss: 0.7564 - val_mse: 0.7564\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7873 - mse: 0.7873 - val_loss: 0.7572 - val_mse: 0.7572\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7868 - mse: 0.7868 - val_loss: 0.7577 - val_mse: 0.7577\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7862 - mse: 0.7862 - val_loss: 0.7584 - val_mse: 0.7584\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7859 - mse: 0.7859 - val_loss: 0.7593 - val_mse: 0.7593\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7858 - mse: 0.7858 - val_loss: 0.7599 - val_mse: 0.7599\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7859 - mse: 0.7859 - val_loss: 0.7602 - val_mse: 0.7602\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7857 - mse: 0.7857 - val_loss: 0.7609 - val_mse: 0.7609\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7855 - mse: 0.7855 - val_loss: 0.7611 - val_mse: 0.7611\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7852 - mse: 0.7852 - val_loss: 0.7615 - val_mse: 0.7615\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7852 - mse: 0.7852 - val_loss: 0.7620 - val_mse: 0.7620\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7856 - mse: 0.7856 - val_loss: 0.7625 - val_mse: 0.7625\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7852 - mse: 0.7852 - val_loss: 0.7624 - val_mse: 0.7624\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7851 - mse: 0.7851 - val_loss: 0.7626 - val_mse: 0.7626\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7853 - mse: 0.7853 - val_loss: 0.7630 - val_mse: 0.7630\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7851 - mse: 0.7851 - val_loss: 0.7633 - val_mse: 0.7633\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.7853 - mse: 0.7853 - val_loss: 0.7636 - val_mse: 0.7636\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.7636 - mse: 0.7636\n",
      "\n",
      "Weight: [array([[2.0597837]], dtype=float32), array([1.0081226], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/20\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1.0075 - mse: 1.0075 - val_loss: 1.0430 - val_mse: 1.0430\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0065 - mse: 1.0065 - val_loss: 1.0380 - val_mse: 1.0380\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0052 - mse: 1.0052 - val_loss: 1.0347 - val_mse: 1.0347\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0053 - mse: 1.0053 - val_loss: 1.0317 - val_mse: 1.0317\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0049 - mse: 1.0049 - val_loss: 1.0311 - val_mse: 1.0311\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0049 - mse: 1.0049 - val_loss: 1.0317 - val_mse: 1.0317\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0048 - mse: 1.0048 - val_loss: 1.0306 - val_mse: 1.0306\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0047 - mse: 1.0047 - val_loss: 1.0304 - val_mse: 1.0304\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0048 - mse: 1.0048 - val_loss: 1.0299 - val_mse: 1.0299\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0050 - mse: 1.0050 - val_loss: 1.0307 - val_mse: 1.0307\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0056 - mse: 1.0056 - val_loss: 1.0298 - val_mse: 1.0298\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0050 - mse: 1.0050 - val_loss: 1.0308 - val_mse: 1.0308\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0049 - mse: 1.0049 - val_loss: 1.0307 - val_mse: 1.0307\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0050 - mse: 1.0050 - val_loss: 1.0320 - val_mse: 1.0320\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0048 - mse: 1.0048 - val_loss: 1.0306 - val_mse: 1.0306\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0048 - mse: 1.0048 - val_loss: 1.0301 - val_mse: 1.0301\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0047 - mse: 1.0047 - val_loss: 1.0301 - val_mse: 1.0301\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0046 - mse: 1.0046 - val_loss: 1.0297 - val_mse: 1.0297\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0049 - mse: 1.0049 - val_loss: 1.0309 - val_mse: 1.0309\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.0050 - mse: 1.0050 - val_loss: 1.0294 - val_mse: 1.0294\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.0294 - mse: 1.0294\n",
      "\n",
      "Weight: [array([[2.0227613]], dtype=float32), array([0.9435291], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079\n",
      "\n",
      "Weight: [array([[2.0028315]], dtype=float32), array([0.9989526], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.0548 - val_mse: 0.0548\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0548 - val_mse: 0.0548\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0548 - val_mse: 0.0548\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0549 - val_mse: 0.0549\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0549 - val_mse: 0.0549\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0549 - val_mse: 0.0549\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0549 - val_mse: 0.0549\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0549 - val_mse: 0.0549\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0549 - val_mse: 0.0549\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.0549 - val_mse: 0.0549\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.0549 - val_mse: 0.0549\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0549 - val_mse: 0.0549\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0549 - val_mse: 0.0549\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0549 - val_mse: 0.0549\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.0549 - val_mse: 0.0549\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0549 - val_mse: 0.0549\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0549 - val_mse: 0.0549\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0549 - val_mse: 0.0549\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0549 - val_mse: 0.0549\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.0548 - val_mse: 0.0548\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0548 - mse: 0.0548\n",
      "\n",
      "Weight: [array([[2.0084016]], dtype=float32), array([1.0069239], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2614 - mse: 0.2614 - val_loss: 0.2135 - val_mse: 0.2135\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2612 - mse: 0.2612 - val_loss: 0.2121 - val_mse: 0.2121\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.2113 - val_mse: 0.2113\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2609 - mse: 0.2609 - val_loss: 0.2110 - val_mse: 0.2110\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.2100 - val_mse: 0.2100\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2609 - mse: 0.2609 - val_loss: 0.2100 - val_mse: 0.2100\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.2101 - val_mse: 0.2101\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.2097 - val_mse: 0.2097\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.2097 - val_mse: 0.2097\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.2096 - val_mse: 0.2096\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2609 - mse: 0.2609 - val_loss: 0.2094 - val_mse: 0.2094\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2609 - mse: 0.2609 - val_loss: 0.2097 - val_mse: 0.2097\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.2097 - val_mse: 0.2097\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2609 - mse: 0.2609 - val_loss: 0.2096 - val_mse: 0.2096\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.2094 - val_mse: 0.2094\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.2096 - val_mse: 0.2096\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.2093 - val_mse: 0.2093\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2609 - mse: 0.2609 - val_loss: 0.2095 - val_mse: 0.2095\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.2096 - val_mse: 0.2096\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.2100 - val_mse: 0.2100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2100 - mse: 0.2100\n",
      "\n",
      "Weight: [array([[2.0030324]], dtype=float32), array([0.9853597], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5307 - mse: 0.5307 - val_loss: 0.6783 - val_mse: 0.6783\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5304 - mse: 0.5304 - val_loss: 0.6779 - val_mse: 0.6779\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5305 - mse: 0.5305 - val_loss: 0.6779 - val_mse: 0.6779\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5305 - mse: 0.5305 - val_loss: 0.6778 - val_mse: 0.6778\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5305 - mse: 0.5305 - val_loss: 0.6776 - val_mse: 0.6776\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5304 - mse: 0.5304 - val_loss: 0.6776 - val_mse: 0.6776\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5305 - mse: 0.5305 - val_loss: 0.6775 - val_mse: 0.6775\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5305 - mse: 0.5305 - val_loss: 0.6774 - val_mse: 0.6774\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5305 - mse: 0.5305 - val_loss: 0.6775 - val_mse: 0.6775\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5303 - mse: 0.5303 - val_loss: 0.6776 - val_mse: 0.6776\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5303 - mse: 0.5303 - val_loss: 0.6775 - val_mse: 0.6775\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5305 - mse: 0.5305 - val_loss: 0.6774 - val_mse: 0.6774\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5303 - mse: 0.5303 - val_loss: 0.6773 - val_mse: 0.6773\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5304 - mse: 0.5304 - val_loss: 0.6773 - val_mse: 0.6773\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5303 - mse: 0.5303 - val_loss: 0.6773 - val_mse: 0.6773\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5305 - mse: 0.5305 - val_loss: 0.6773 - val_mse: 0.6773\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5306 - mse: 0.5306 - val_loss: 0.6773 - val_mse: 0.6773\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5304 - mse: 0.5304 - val_loss: 0.6772 - val_mse: 0.6772\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5304 - mse: 0.5304 - val_loss: 0.6772 - val_mse: 0.6772\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5304 - mse: 0.5304 - val_loss: 0.6772 - val_mse: 0.6772\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.6772 - mse: 0.6772\n",
      "\n",
      "Weight: [array([[2.0146396]], dtype=float32), array([1.0057168], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.7729 - mse: 0.7729 - val_loss: 1.0525 - val_mse: 1.0525\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7728 - mse: 0.7728 - val_loss: 1.0528 - val_mse: 1.0528\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7726 - mse: 0.7726 - val_loss: 1.0529 - val_mse: 1.0529\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7729 - mse: 0.7729 - val_loss: 1.0530 - val_mse: 1.0530\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7727 - mse: 0.7727 - val_loss: 1.0528 - val_mse: 1.0528\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7725 - mse: 0.7725 - val_loss: 1.0534 - val_mse: 1.0534\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7726 - mse: 0.7726 - val_loss: 1.0538 - val_mse: 1.0538\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7725 - mse: 0.7725 - val_loss: 1.0536 - val_mse: 1.0536\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7726 - mse: 0.7726 - val_loss: 1.0531 - val_mse: 1.0531\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7726 - mse: 0.7726 - val_loss: 1.0532 - val_mse: 1.0532\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7729 - mse: 0.7729 - val_loss: 1.0534 - val_mse: 1.0534\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7726 - mse: 0.7726 - val_loss: 1.0534 - val_mse: 1.0534\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7725 - mse: 0.7725 - val_loss: 1.0535 - val_mse: 1.0535\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7726 - mse: 0.7726 - val_loss: 1.0543 - val_mse: 1.0543\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7726 - mse: 0.7726 - val_loss: 1.0545 - val_mse: 1.0545\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7726 - mse: 0.7726 - val_loss: 1.0542 - val_mse: 1.0542\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7726 - mse: 0.7726 - val_loss: 1.0537 - val_mse: 1.0537\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7728 - mse: 0.7728 - val_loss: 1.0536 - val_mse: 1.0536\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7728 - mse: 0.7728 - val_loss: 1.0541 - val_mse: 1.0541\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7726 - mse: 0.7726 - val_loss: 1.0541 - val_mse: 1.0541\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.0541 - mse: 1.0541\n",
      "\n",
      "Weight: [array([[2.0095491]], dtype=float32), array([1.0176593], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.2389 - mse: 1.2389 - val_loss: 1.2559 - val_mse: 1.2559\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2381 - mse: 1.2381 - val_loss: 1.2513 - val_mse: 1.2513\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2379 - mse: 1.2379 - val_loss: 1.2487 - val_mse: 1.2487\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2371 - mse: 1.2371 - val_loss: 1.2469 - val_mse: 1.2469\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2370 - mse: 1.2370 - val_loss: 1.2447 - val_mse: 1.2447\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2368 - mse: 1.2368 - val_loss: 1.2462 - val_mse: 1.2462\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2367 - mse: 1.2367 - val_loss: 1.2470 - val_mse: 1.2470\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2366 - mse: 1.2366 - val_loss: 1.2453 - val_mse: 1.2453\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2367 - mse: 1.2367 - val_loss: 1.2459 - val_mse: 1.2459\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2368 - mse: 1.2368 - val_loss: 1.2462 - val_mse: 1.2462\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2366 - mse: 1.2366 - val_loss: 1.2480 - val_mse: 1.2480\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2366 - mse: 1.2366 - val_loss: 1.2472 - val_mse: 1.2472\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2371 - mse: 1.2371 - val_loss: 1.2470 - val_mse: 1.2470\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2362 - mse: 1.2362 - val_loss: 1.2478 - val_mse: 1.2478\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2369 - mse: 1.2369 - val_loss: 1.2458 - val_mse: 1.2458\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2365 - mse: 1.2365 - val_loss: 1.2464 - val_mse: 1.2464\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2369 - mse: 1.2369 - val_loss: 1.2460 - val_mse: 1.2460\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2363 - mse: 1.2363 - val_loss: 1.2461 - val_mse: 1.2461\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2364 - mse: 1.2364 - val_loss: 1.2457 - val_mse: 1.2457\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2368 - mse: 1.2368 - val_loss: 1.2454 - val_mse: 1.2454\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.2454 - mse: 1.2454\n",
      "\n",
      "Weight: [array([[1.9364603]], dtype=float32), array([0.98744094], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/20\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094\n",
      "\n",
      "Weight: [array([[2.0022192]], dtype=float32), array([0.9969124], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/20\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0874 - mse: 0.0874 - val_loss: 0.0716 - val_mse: 0.0716\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0716 - val_mse: 0.0716\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0716 - val_mse: 0.0716\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0874 - mse: 0.0874 - val_loss: 0.0716 - val_mse: 0.0716\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0717 - val_mse: 0.0717\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0718 - val_mse: 0.0718\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0718 - val_mse: 0.0718\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0719 - val_mse: 0.0719\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0719 - val_mse: 0.0719\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0719 - val_mse: 0.0719\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0719 - val_mse: 0.0719\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0872 - mse: 0.0872 - val_loss: 0.0720 - val_mse: 0.0720\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0720 - val_mse: 0.0720\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0720 - val_mse: 0.0720\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0720 - val_mse: 0.0720\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0872 - mse: 0.0872 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0720 - val_mse: 0.0720\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0721 - val_mse: 0.0721\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0721 - mse: 0.0721\n",
      "\n",
      "Weight: [array([[1.9830574]], dtype=float32), array([1.0044575], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/20\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2214 - mse: 0.2214 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2208 - mse: 0.2208 - val_loss: 0.2046 - val_mse: 0.2046\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2206 - mse: 0.2206 - val_loss: 0.2044 - val_mse: 0.2044\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2203 - mse: 0.2203 - val_loss: 0.2042 - val_mse: 0.2042\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2201 - mse: 0.2201 - val_loss: 0.2044 - val_mse: 0.2044\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2201 - mse: 0.2201 - val_loss: 0.2043 - val_mse: 0.2043\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2200 - mse: 0.2200 - val_loss: 0.2042 - val_mse: 0.2042\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2199 - mse: 0.2199 - val_loss: 0.2042 - val_mse: 0.2042\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2199 - mse: 0.2199 - val_loss: 0.2041 - val_mse: 0.2041\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2198 - mse: 0.2198 - val_loss: 0.2039 - val_mse: 0.2039\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2198 - mse: 0.2198 - val_loss: 0.2040 - val_mse: 0.2040\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2198 - mse: 0.2198 - val_loss: 0.2040 - val_mse: 0.2040\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2198 - mse: 0.2198 - val_loss: 0.2038 - val_mse: 0.2038\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2198 - mse: 0.2198 - val_loss: 0.2042 - val_mse: 0.2042\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2198 - mse: 0.2198 - val_loss: 0.2043 - val_mse: 0.2043\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2198 - mse: 0.2198 - val_loss: 0.2042 - val_mse: 0.2042\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2197 - mse: 0.2197 - val_loss: 0.2042 - val_mse: 0.2042\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2198 - mse: 0.2198 - val_loss: 0.2041 - val_mse: 0.2041\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2198 - mse: 0.2198 - val_loss: 0.2039 - val_mse: 0.2039\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2197 - mse: 0.2197 - val_loss: 0.2040 - val_mse: 0.2040\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2040 - mse: 0.2040\n",
      "\n",
      "Weight: [array([[2.050879]], dtype=float32), array([0.9847549], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/20\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4971 - mse: 0.4971 - val_loss: 0.3892 - val_mse: 0.3892\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4958 - mse: 0.4958 - val_loss: 0.3913 - val_mse: 0.3913\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4951 - mse: 0.4951 - val_loss: 0.3931 - val_mse: 0.3931\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4946 - mse: 0.4946 - val_loss: 0.3948 - val_mse: 0.3948\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4945 - mse: 0.4945 - val_loss: 0.3962 - val_mse: 0.3962\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4942 - mse: 0.4942 - val_loss: 0.3975 - val_mse: 0.3975\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4941 - mse: 0.4941 - val_loss: 0.3987 - val_mse: 0.3987\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4937 - mse: 0.4937 - val_loss: 0.3997 - val_mse: 0.3997\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4938 - mse: 0.4938 - val_loss: 0.4006 - val_mse: 0.4006\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4936 - mse: 0.4936 - val_loss: 0.4014 - val_mse: 0.4014\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4937 - mse: 0.4937 - val_loss: 0.4020 - val_mse: 0.4020\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4936 - mse: 0.4936 - val_loss: 0.4026 - val_mse: 0.4026\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4936 - mse: 0.4936 - val_loss: 0.4029 - val_mse: 0.4029\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4935 - mse: 0.4935 - val_loss: 0.4033 - val_mse: 0.4033\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4937 - mse: 0.4937 - val_loss: 0.4037 - val_mse: 0.4037\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4935 - mse: 0.4935 - val_loss: 0.4041 - val_mse: 0.4041\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4935 - mse: 0.4935 - val_loss: 0.4043 - val_mse: 0.4043\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4936 - mse: 0.4936 - val_loss: 0.4045 - val_mse: 0.4045\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4935 - mse: 0.4935 - val_loss: 0.4046 - val_mse: 0.4046\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4935 - mse: 0.4935 - val_loss: 0.4046 - val_mse: 0.4046\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4046 - mse: 0.4046\n",
      "\n",
      "Weight: [array([[1.9542643]], dtype=float32), array([1.0208335], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8791 - mse: 0.8791 - val_loss: 0.8910 - val_mse: 0.8910\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8782 - mse: 0.8782 - val_loss: 0.8908 - val_mse: 0.8908\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8776 - mse: 0.8776 - val_loss: 0.8923 - val_mse: 0.8923\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8767 - mse: 0.8767 - val_loss: 0.8935 - val_mse: 0.8935\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8762 - mse: 0.8762 - val_loss: 0.8942 - val_mse: 0.8942\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8763 - mse: 0.8763 - val_loss: 0.8942 - val_mse: 0.8942\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8758 - mse: 0.8758 - val_loss: 0.8932 - val_mse: 0.8932\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8755 - mse: 0.8755 - val_loss: 0.8944 - val_mse: 0.8944\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8754 - mse: 0.8754 - val_loss: 0.8957 - val_mse: 0.8957\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8757 - mse: 0.8757 - val_loss: 0.8968 - val_mse: 0.8968\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8756 - mse: 0.8756 - val_loss: 0.8963 - val_mse: 0.8963\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8755 - mse: 0.8755 - val_loss: 0.8970 - val_mse: 0.8970\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8756 - mse: 0.8756 - val_loss: 0.8960 - val_mse: 0.8960\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8756 - mse: 0.8756 - val_loss: 0.8954 - val_mse: 0.8954\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8752 - mse: 0.8752 - val_loss: 0.8954 - val_mse: 0.8954\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8754 - mse: 0.8754 - val_loss: 0.8955 - val_mse: 0.8955\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8754 - mse: 0.8754 - val_loss: 0.8961 - val_mse: 0.8961\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8756 - mse: 0.8756 - val_loss: 0.8962 - val_mse: 0.8962\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8753 - mse: 0.8753 - val_loss: 0.8965 - val_mse: 0.8965\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8755 - mse: 0.8755 - val_loss: 0.8975 - val_mse: 0.8975\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.8975 - mse: 0.8975\n",
      "\n",
      "Weight: [array([[2.0641277]], dtype=float32), array([1.0272304], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1376 - mse: 1.1376 - val_loss: 1.2908 - val_mse: 1.2908\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1364 - mse: 1.1364 - val_loss: 1.3010 - val_mse: 1.3010\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1353 - mse: 1.1353 - val_loss: 1.3039 - val_mse: 1.3039\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1348 - mse: 1.1348 - val_loss: 1.3056 - val_mse: 1.3056\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1348 - mse: 1.1348 - val_loss: 1.3090 - val_mse: 1.3090\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1347 - mse: 1.1347 - val_loss: 1.3114 - val_mse: 1.3114\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1345 - mse: 1.1345 - val_loss: 1.3142 - val_mse: 1.3142\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1349 - mse: 1.1349 - val_loss: 1.3155 - val_mse: 1.3155\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1350 - mse: 1.1350 - val_loss: 1.3159 - val_mse: 1.3159\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1343 - mse: 1.1343 - val_loss: 1.3177 - val_mse: 1.3177\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1346 - mse: 1.1346 - val_loss: 1.3181 - val_mse: 1.3181\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1346 - mse: 1.1346 - val_loss: 1.3166 - val_mse: 1.3166\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1346 - mse: 1.1346 - val_loss: 1.3149 - val_mse: 1.3149\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1346 - mse: 1.1346 - val_loss: 1.3134 - val_mse: 1.3134\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1346 - mse: 1.1346 - val_loss: 1.3159 - val_mse: 1.3159\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1343 - mse: 1.1343 - val_loss: 1.3161 - val_mse: 1.3161\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1345 - mse: 1.1345 - val_loss: 1.3164 - val_mse: 1.3164\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1348 - mse: 1.1348 - val_loss: 1.3143 - val_mse: 1.3143\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1344 - mse: 1.1344 - val_loss: 1.3152 - val_mse: 1.3152\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1346 - mse: 1.1346 - val_loss: 1.3140 - val_mse: 1.3140\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.3140 - mse: 1.3140\n",
      "\n",
      "Weight: [array([[2.003309]], dtype=float32), array([0.978422], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/20\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 2/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 3/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 4/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 5/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 6/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 7/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 8/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 9/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 10/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 11/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 12/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 13/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 14/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 15/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 16/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 17/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 18/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 19/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 20/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0111 - mse: 0.0111\n",
      "\n",
      "Weight: [array([[2.0036938]], dtype=float32), array([1.002714], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/20\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0916 - mse: 0.0916 - val_loss: 0.1037 - val_mse: 0.1037\n",
      "Epoch 2/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0916 - mse: 0.0916 - val_loss: 0.1036 - val_mse: 0.1036\n",
      "Epoch 3/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0916 - mse: 0.0916 - val_loss: 0.1035 - val_mse: 0.1035\n",
      "Epoch 4/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0916 - mse: 0.0916 - val_loss: 0.1034 - val_mse: 0.1034\n",
      "Epoch 5/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0916 - mse: 0.0916 - val_loss: 0.1034 - val_mse: 0.1034\n",
      "Epoch 6/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0916 - mse: 0.0916 - val_loss: 0.1034 - val_mse: 0.1034\n",
      "Epoch 7/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.1037 - val_mse: 0.1037\n",
      "Epoch 8/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0916 - mse: 0.0916 - val_loss: 0.1032 - val_mse: 0.1032\n",
      "Epoch 9/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.1029 - val_mse: 0.1029\n",
      "Epoch 10/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.1028 - val_mse: 0.1028\n",
      "Epoch 11/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0916 - mse: 0.0916 - val_loss: 0.1029 - val_mse: 0.1029\n",
      "Epoch 12/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.1029 - val_mse: 0.1029\n",
      "Epoch 13/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.1026 - val_mse: 0.1026\n",
      "Epoch 14/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.1027 - val_mse: 0.1027\n",
      "Epoch 15/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.1026 - val_mse: 0.1026\n",
      "Epoch 16/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.1027 - val_mse: 0.1027\n",
      "Epoch 17/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.1026 - val_mse: 0.1026\n",
      "Epoch 18/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.1026 - val_mse: 0.1026\n",
      "Epoch 19/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.1026 - val_mse: 0.1026\n",
      "Epoch 20/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.1025 - val_mse: 0.1025\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.1025 - mse: 0.1025\n",
      "\n",
      "Weight: [array([[2.0237174]], dtype=float32), array([1.003654], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/20\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2326 - mse: 0.2326 - val_loss: 0.2647 - val_mse: 0.2647\n",
      "Epoch 2/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2326 - mse: 0.2326 - val_loss: 0.2646 - val_mse: 0.2646\n",
      "Epoch 3/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2326 - mse: 0.2326 - val_loss: 0.2644 - val_mse: 0.2644\n",
      "Epoch 4/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2327 - mse: 0.2327 - val_loss: 0.2646 - val_mse: 0.2646\n",
      "Epoch 5/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2326 - mse: 0.2326 - val_loss: 0.2647 - val_mse: 0.2647\n",
      "Epoch 6/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2326 - mse: 0.2326 - val_loss: 0.2648 - val_mse: 0.2648\n",
      "Epoch 7/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2325 - mse: 0.2325 - val_loss: 0.2653 - val_mse: 0.2653\n",
      "Epoch 8/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2326 - mse: 0.2326 - val_loss: 0.2653 - val_mse: 0.2653\n",
      "Epoch 9/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2325 - mse: 0.2325 - val_loss: 0.2652 - val_mse: 0.2652\n",
      "Epoch 10/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2325 - mse: 0.2325 - val_loss: 0.2655 - val_mse: 0.2655\n",
      "Epoch 11/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2326 - mse: 0.2326 - val_loss: 0.2651 - val_mse: 0.2651\n",
      "Epoch 12/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2325 - mse: 0.2325 - val_loss: 0.2647 - val_mse: 0.2647\n",
      "Epoch 13/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2325 - mse: 0.2325 - val_loss: 0.2649 - val_mse: 0.2649\n",
      "Epoch 14/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2325 - mse: 0.2325 - val_loss: 0.2653 - val_mse: 0.2653\n",
      "Epoch 15/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2326 - mse: 0.2326 - val_loss: 0.2651 - val_mse: 0.2651\n",
      "Epoch 16/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2326 - mse: 0.2326 - val_loss: 0.2652 - val_mse: 0.2652\n",
      "Epoch 17/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2326 - mse: 0.2326 - val_loss: 0.2647 - val_mse: 0.2647\n",
      "Epoch 18/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2325 - mse: 0.2325 - val_loss: 0.2656 - val_mse: 0.2656\n",
      "Epoch 19/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2326 - mse: 0.2326 - val_loss: 0.2652 - val_mse: 0.2652\n",
      "Epoch 20/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2325 - mse: 0.2325 - val_loss: 0.2653 - val_mse: 0.2653\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2653 - mse: 0.2653\n",
      "\n",
      "Weight: [array([[2.0140426]], dtype=float32), array([0.99388075], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5057 - mse: 0.5057 - val_loss: 0.5292 - val_mse: 0.5292\n",
      "Epoch 2/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5052 - mse: 0.5052 - val_loss: 0.5286 - val_mse: 0.5286\n",
      "Epoch 3/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5054 - mse: 0.5054 - val_loss: 0.5289 - val_mse: 0.5289\n",
      "Epoch 4/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5052 - mse: 0.5052 - val_loss: 0.5293 - val_mse: 0.5293\n",
      "Epoch 5/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5053 - mse: 0.5053 - val_loss: 0.5294 - val_mse: 0.5294\n",
      "Epoch 6/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5053 - mse: 0.5053 - val_loss: 0.5295 - val_mse: 0.5295\n",
      "Epoch 7/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5053 - mse: 0.5053 - val_loss: 0.5299 - val_mse: 0.5299\n",
      "Epoch 8/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5053 - mse: 0.5053 - val_loss: 0.5302 - val_mse: 0.5302\n",
      "Epoch 9/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5054 - mse: 0.5054 - val_loss: 0.5297 - val_mse: 0.5297\n",
      "Epoch 10/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5052 - mse: 0.5052 - val_loss: 0.5291 - val_mse: 0.5291\n",
      "Epoch 11/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5050 - mse: 0.5050 - val_loss: 0.5286 - val_mse: 0.5286\n",
      "Epoch 12/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5051 - mse: 0.5051 - val_loss: 0.5292 - val_mse: 0.5292\n",
      "Epoch 13/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5051 - mse: 0.5051 - val_loss: 0.5290 - val_mse: 0.5290\n",
      "Epoch 14/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5051 - mse: 0.5051 - val_loss: 0.5283 - val_mse: 0.5283\n",
      "Epoch 15/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5052 - mse: 0.5052 - val_loss: 0.5284 - val_mse: 0.5284\n",
      "Epoch 16/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5051 - mse: 0.5051 - val_loss: 0.5290 - val_mse: 0.5290\n",
      "Epoch 17/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5052 - mse: 0.5052 - val_loss: 0.5292 - val_mse: 0.5292\n",
      "Epoch 18/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5053 - mse: 0.5053 - val_loss: 0.5291 - val_mse: 0.5291\n",
      "Epoch 19/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5052 - mse: 0.5052 - val_loss: 0.5291 - val_mse: 0.5291\n",
      "Epoch 20/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5054 - mse: 0.5054 - val_loss: 0.5292 - val_mse: 0.5292\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5292 - mse: 0.5292\n",
      "\n",
      "Weight: [array([[2.035087]], dtype=float32), array([1.0141989], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8344 - mse: 0.8344 - val_loss: 0.7875 - val_mse: 0.7875\n",
      "Epoch 2/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8318 - mse: 0.8318 - val_loss: 0.7894 - val_mse: 0.7894\n",
      "Epoch 3/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8283 - mse: 0.8283 - val_loss: 0.7893 - val_mse: 0.7893\n",
      "Epoch 4/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8278 - mse: 0.8278 - val_loss: 0.7907 - val_mse: 0.7907\n",
      "Epoch 5/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8272 - mse: 0.8272 - val_loss: 0.7909 - val_mse: 0.7909\n",
      "Epoch 6/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8270 - mse: 0.8270 - val_loss: 0.7929 - val_mse: 0.7929\n",
      "Epoch 7/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.7938 - val_mse: 0.7938\n",
      "Epoch 8/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.7964 - val_mse: 0.7964\n",
      "Epoch 9/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.7921 - val_mse: 0.7921\n",
      "Epoch 10/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8268 - mse: 0.8268 - val_loss: 0.7934 - val_mse: 0.7934\n",
      "Epoch 11/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8267 - mse: 0.8267 - val_loss: 0.7947 - val_mse: 0.7947\n",
      "Epoch 12/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.7952 - val_mse: 0.7952\n",
      "Epoch 13/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8264 - mse: 0.8264 - val_loss: 0.7935 - val_mse: 0.7935\n",
      "Epoch 14/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.7948 - val_mse: 0.7948\n",
      "Epoch 15/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.7958 - val_mse: 0.7958\n",
      "Epoch 16/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.7963 - val_mse: 0.7963\n",
      "Epoch 17/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.7952 - val_mse: 0.7952\n",
      "Epoch 18/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.7958 - val_mse: 0.7958\n",
      "Epoch 19/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.7932 - val_mse: 0.7932\n",
      "Epoch 20/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.7923 - val_mse: 0.7923\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.7923 - mse: 0.7923\n",
      "\n",
      "Weight: [array([[1.9343456]], dtype=float32), array([1.0948789], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2410 - mse: 1.2410 - val_loss: 0.9487 - val_mse: 0.9487\n",
      "Epoch 2/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2355 - mse: 1.2355 - val_loss: 0.9473 - val_mse: 0.9473\n",
      "Epoch 3/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2346 - mse: 1.2346 - val_loss: 0.9497 - val_mse: 0.9497\n",
      "Epoch 4/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2341 - mse: 1.2341 - val_loss: 0.9503 - val_mse: 0.9503\n",
      "Epoch 5/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2336 - mse: 1.2336 - val_loss: 0.9496 - val_mse: 0.9496\n",
      "Epoch 6/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2339 - mse: 1.2339 - val_loss: 0.9495 - val_mse: 0.9495\n",
      "Epoch 7/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2336 - mse: 1.2336 - val_loss: 0.9492 - val_mse: 0.9492\n",
      "Epoch 8/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2335 - mse: 1.2335 - val_loss: 0.9467 - val_mse: 0.9467\n",
      "Epoch 9/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2335 - mse: 1.2335 - val_loss: 0.9491 - val_mse: 0.9491\n",
      "Epoch 10/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2334 - mse: 1.2334 - val_loss: 0.9463 - val_mse: 0.9463\n",
      "Epoch 11/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2333 - mse: 1.2333 - val_loss: 0.9468 - val_mse: 0.9468\n",
      "Epoch 12/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2335 - mse: 1.2335 - val_loss: 0.9468 - val_mse: 0.9468\n",
      "Epoch 13/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2336 - mse: 1.2336 - val_loss: 0.9464 - val_mse: 0.9464\n",
      "Epoch 14/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2333 - mse: 1.2333 - val_loss: 0.9492 - val_mse: 0.9492\n",
      "Epoch 15/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2333 - mse: 1.2333 - val_loss: 0.9487 - val_mse: 0.9487\n",
      "Epoch 16/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2336 - mse: 1.2336 - val_loss: 0.9440 - val_mse: 0.9440\n",
      "Epoch 17/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2335 - mse: 1.2335 - val_loss: 0.9413 - val_mse: 0.9413\n",
      "Epoch 18/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2337 - mse: 1.2337 - val_loss: 0.9409 - val_mse: 0.9409\n",
      "Epoch 19/20\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2338 - mse: 1.2338 - val_loss: 0.9394 - val_mse: 0.9394\n",
      "Epoch 20/20\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2341 - mse: 1.2341 - val_loss: 0.9385 - val_mse: 0.9385\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.9385 - mse: 0.9385\n",
      "\n",
      "Weight: [array([[1.843311]], dtype=float32), array([1.0377344], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0204 - val_mse: 0.0204\n",
      "Epoch 2/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0184 - val_mse: 0.0184\n",
      "Epoch 3/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0171 - val_mse: 0.0171\n",
      "Epoch 4/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0160 - val_mse: 0.0160\n",
      "Epoch 5/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0153 - val_mse: 0.0153\n",
      "Epoch 6/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0148 - val_mse: 0.0148\n",
      "Epoch 7/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 8/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 9/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 10/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 11/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 12/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 13/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 14/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 15/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 16/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 17/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 18/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 19/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 20/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132\n",
      "\n",
      "Weight: [array([[1.9931828]], dtype=float32), array([1.0030525], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1194 - val_mse: 0.1194\n",
      "Epoch 2/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1191 - val_mse: 0.1191\n",
      "Epoch 3/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1192 - val_mse: 0.1192\n",
      "Epoch 4/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1191 - val_mse: 0.1191\n",
      "Epoch 5/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1192 - val_mse: 0.1192\n",
      "Epoch 6/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0902 - mse: 0.0902 - val_loss: 0.1192 - val_mse: 0.1192\n",
      "Epoch 7/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1193 - val_mse: 0.1193\n",
      "Epoch 8/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1191 - val_mse: 0.1191\n",
      "Epoch 9/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1192 - val_mse: 0.1192\n",
      "Epoch 10/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0902 - mse: 0.0902 - val_loss: 0.1191 - val_mse: 0.1191\n",
      "Epoch 11/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1192 - val_mse: 0.1192\n",
      "Epoch 12/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1192 - val_mse: 0.1192\n",
      "Epoch 13/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1191 - val_mse: 0.1191\n",
      "Epoch 14/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0902 - mse: 0.0902 - val_loss: 0.1191 - val_mse: 0.1191\n",
      "Epoch 15/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1191 - val_mse: 0.1191\n",
      "Epoch 16/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0902 - mse: 0.0902 - val_loss: 0.1191 - val_mse: 0.1191\n",
      "Epoch 17/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1191 - val_mse: 0.1191\n",
      "Epoch 18/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1191 - val_mse: 0.1191\n",
      "Epoch 19/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1190 - val_mse: 0.1190\n",
      "Epoch 20/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0902 - mse: 0.0902 - val_loss: 0.1189 - val_mse: 0.1189\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1189 - mse: 0.1189\n",
      "\n",
      "Weight: [array([[1.9799637]], dtype=float32), array([1.0035347], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2664 - mse: 0.2664 - val_loss: 0.2148 - val_mse: 0.2148\n",
      "Epoch 2/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2148 - val_mse: 0.2148\n",
      "Epoch 3/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2149 - val_mse: 0.2149\n",
      "Epoch 4/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2150 - val_mse: 0.2150\n",
      "Epoch 5/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2155 - val_mse: 0.2155\n",
      "Epoch 6/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2147 - val_mse: 0.2147\n",
      "Epoch 7/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2664 - mse: 0.2664 - val_loss: 0.2146 - val_mse: 0.2146\n",
      "Epoch 8/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2141 - val_mse: 0.2141\n",
      "Epoch 9/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2142 - val_mse: 0.2142\n",
      "Epoch 10/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2148 - val_mse: 0.2148\n",
      "Epoch 11/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2143 - val_mse: 0.2143\n",
      "Epoch 12/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2142 - val_mse: 0.2142\n",
      "Epoch 13/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2142 - val_mse: 0.2142\n",
      "Epoch 14/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2664 - mse: 0.2664 - val_loss: 0.2144 - val_mse: 0.2144\n",
      "Epoch 15/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2144 - val_mse: 0.2144\n",
      "Epoch 16/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2664 - mse: 0.2664 - val_loss: 0.2142 - val_mse: 0.2142\n",
      "Epoch 17/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2149 - val_mse: 0.2149\n",
      "Epoch 18/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2664 - mse: 0.2664 - val_loss: 0.2145 - val_mse: 0.2145\n",
      "Epoch 19/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2140 - val_mse: 0.2140\n",
      "Epoch 20/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2138 - val_mse: 0.2138\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2138 - mse: 0.2138\n",
      "\n",
      "Weight: [array([[1.9906555]], dtype=float32), array([1.0103306], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5153 - mse: 0.5153 - val_loss: 0.5327 - val_mse: 0.5327\n",
      "Epoch 2/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5147 - mse: 0.5147 - val_loss: 0.5316 - val_mse: 0.5316\n",
      "Epoch 3/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5145 - mse: 0.5145 - val_loss: 0.5293 - val_mse: 0.5293\n",
      "Epoch 4/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5140 - mse: 0.5140 - val_loss: 0.5298 - val_mse: 0.5298\n",
      "Epoch 5/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5139 - mse: 0.5139 - val_loss: 0.5289 - val_mse: 0.5289\n",
      "Epoch 6/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5139 - mse: 0.5139 - val_loss: 0.5267 - val_mse: 0.5267\n",
      "Epoch 7/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5139 - mse: 0.5139 - val_loss: 0.5261 - val_mse: 0.5261\n",
      "Epoch 8/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5140 - mse: 0.5140 - val_loss: 0.5253 - val_mse: 0.5253\n",
      "Epoch 9/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5138 - mse: 0.5138 - val_loss: 0.5261 - val_mse: 0.5261\n",
      "Epoch 10/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5138 - mse: 0.5138 - val_loss: 0.5274 - val_mse: 0.5274\n",
      "Epoch 11/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5137 - mse: 0.5137 - val_loss: 0.5279 - val_mse: 0.5279\n",
      "Epoch 12/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5138 - mse: 0.5138 - val_loss: 0.5257 - val_mse: 0.5257\n",
      "Epoch 13/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5137 - mse: 0.5137 - val_loss: 0.5248 - val_mse: 0.5248\n",
      "Epoch 14/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5136 - mse: 0.5136 - val_loss: 0.5266 - val_mse: 0.5266\n",
      "Epoch 15/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5136 - mse: 0.5136 - val_loss: 0.5277 - val_mse: 0.5277\n",
      "Epoch 16/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5136 - mse: 0.5136 - val_loss: 0.5269 - val_mse: 0.5269\n",
      "Epoch 17/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5138 - mse: 0.5138 - val_loss: 0.5255 - val_mse: 0.5255\n",
      "Epoch 18/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5138 - mse: 0.5138 - val_loss: 0.5262 - val_mse: 0.5262\n",
      "Epoch 19/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5138 - mse: 0.5138 - val_loss: 0.5245 - val_mse: 0.5245\n",
      "Epoch 20/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5137 - mse: 0.5137 - val_loss: 0.5253 - val_mse: 0.5253\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5253 - mse: 0.5253\n",
      "\n",
      "Weight: [array([[2.0680437]], dtype=float32), array([1.0009594], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7757 - mse: 0.7757 - val_loss: 0.9436 - val_mse: 0.9436\n",
      "Epoch 2/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7747 - mse: 0.7747 - val_loss: 0.9428 - val_mse: 0.9428\n",
      "Epoch 3/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.9427 - val_mse: 0.9427\n",
      "Epoch 4/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.9429 - val_mse: 0.9429\n",
      "Epoch 5/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7736 - mse: 0.7736 - val_loss: 0.9432 - val_mse: 0.9432\n",
      "Epoch 6/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7735 - mse: 0.7735 - val_loss: 0.9437 - val_mse: 0.9437\n",
      "Epoch 7/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7734 - mse: 0.7734 - val_loss: 0.9439 - val_mse: 0.9439\n",
      "Epoch 8/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7733 - mse: 0.7733 - val_loss: 0.9446 - val_mse: 0.9446\n",
      "Epoch 9/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7730 - mse: 0.7730 - val_loss: 0.9433 - val_mse: 0.9433\n",
      "Epoch 10/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7732 - mse: 0.7732 - val_loss: 0.9435 - val_mse: 0.9435\n",
      "Epoch 11/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7731 - mse: 0.7731 - val_loss: 0.9442 - val_mse: 0.9442\n",
      "Epoch 12/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7734 - mse: 0.7734 - val_loss: 0.9439 - val_mse: 0.9439\n",
      "Epoch 13/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7732 - mse: 0.7732 - val_loss: 0.9446 - val_mse: 0.9446\n",
      "Epoch 14/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7731 - mse: 0.7731 - val_loss: 0.9452 - val_mse: 0.9452\n",
      "Epoch 15/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7731 - mse: 0.7731 - val_loss: 0.9445 - val_mse: 0.9445\n",
      "Epoch 16/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7733 - mse: 0.7733 - val_loss: 0.9461 - val_mse: 0.9461\n",
      "Epoch 17/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7735 - mse: 0.7735 - val_loss: 0.9469 - val_mse: 0.9469\n",
      "Epoch 18/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7735 - mse: 0.7735 - val_loss: 0.9453 - val_mse: 0.9453\n",
      "Epoch 19/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7733 - mse: 0.7733 - val_loss: 0.9451 - val_mse: 0.9451\n",
      "Epoch 20/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7731 - mse: 0.7731 - val_loss: 0.9451 - val_mse: 0.9451\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.9451 - mse: 0.9451\n",
      "\n",
      "Weight: [array([[1.9797951]], dtype=float32), array([1.0301312], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1489 - mse: 1.1489 - val_loss: 1.2055 - val_mse: 1.2055\n",
      "Epoch 2/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1485 - mse: 1.1485 - val_loss: 1.2043 - val_mse: 1.2043\n",
      "Epoch 3/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1474 - mse: 1.1474 - val_loss: 1.2012 - val_mse: 1.2012\n",
      "Epoch 4/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1467 - mse: 1.1467 - val_loss: 1.1989 - val_mse: 1.1989\n",
      "Epoch 5/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1463 - mse: 1.1463 - val_loss: 1.1957 - val_mse: 1.1957\n",
      "Epoch 6/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1462 - mse: 1.1462 - val_loss: 1.1956 - val_mse: 1.1956\n",
      "Epoch 7/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1463 - mse: 1.1463 - val_loss: 1.1957 - val_mse: 1.1957\n",
      "Epoch 8/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1462 - mse: 1.1462 - val_loss: 1.1970 - val_mse: 1.1970\n",
      "Epoch 9/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1458 - mse: 1.1458 - val_loss: 1.1956 - val_mse: 1.1956\n",
      "Epoch 10/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1458 - mse: 1.1458 - val_loss: 1.1949 - val_mse: 1.1949\n",
      "Epoch 11/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1459 - mse: 1.1459 - val_loss: 1.1945 - val_mse: 1.1945\n",
      "Epoch 12/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1459 - mse: 1.1459 - val_loss: 1.1953 - val_mse: 1.1953\n",
      "Epoch 13/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1459 - mse: 1.1459 - val_loss: 1.1963 - val_mse: 1.1963\n",
      "Epoch 14/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1457 - mse: 1.1457 - val_loss: 1.1951 - val_mse: 1.1951\n",
      "Epoch 15/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1461 - mse: 1.1461 - val_loss: 1.1940 - val_mse: 1.1940\n",
      "Epoch 16/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1458 - mse: 1.1458 - val_loss: 1.1944 - val_mse: 1.1944\n",
      "Epoch 17/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1459 - mse: 1.1459 - val_loss: 1.1957 - val_mse: 1.1957\n",
      "Epoch 18/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1456 - mse: 1.1456 - val_loss: 1.1972 - val_mse: 1.1972\n",
      "Epoch 19/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1454 - mse: 1.1454 - val_loss: 1.1972 - val_mse: 1.1972\n",
      "Epoch 20/20\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1457 - mse: 1.1457 - val_loss: 1.1971 - val_mse: 1.1971\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.1971 - mse: 1.1971\n",
      "\n",
      "Weight: [array([[2.094249]], dtype=float32), array([1.0266118], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "N_train = np.zeros(6)\n",
    "sigma = np.zeros(6)\n",
    "score20 = np.zeros((6,6,2))\n",
    "weight20 = np.zeros((6,6,2))\n",
    "\n",
    "history20 = []\n",
    "\n",
    "\n",
    "#GRID SEARCH\n",
    "for i in range(6):\n",
    "    N_train[i] = 500 + i*100\n",
    "    row = []\n",
    "    for j in range(6):\n",
    "        sigma[j] = 0.1 + j*0.2 # noise standard deviation\n",
    "        x_train = np.random.uniform(-1, 1, int(N_train[i]))\n",
    "    \n",
    "        y_train = np.random.normal(m * x_train + b, sigma[j]) # actual measures from which we want to guess regression parameters\n",
    "        y_valid = np.random.normal(m * x_valid + b, sigma[j])\n",
    "        # fit the model using training dataset\n",
    "        # over 20 epochs of 32 batch size each\n",
    "        # report training progress against validation data\n",
    "        print(\"\\n\\nN_train = \",N_train[i])\n",
    "        print(\"Sigma = \",sigma[j])\n",
    "        row.append(model.fit(x=x_train, y=y_train, \n",
    "              batch_size=32, epochs=20,\n",
    "              shuffle=True, # a good idea is to shuffle input before at each epoch\n",
    "              validation_data=(x_valid, y_valid)))\n",
    "        score20[i][j] = model.evaluate(x_valid, y_valid, batch_size=32, verbose=1)\n",
    "        pesi = model.get_weights()\n",
    "        weight20[i][j] = [pesi[0][0][0],pesi[1][0]]\n",
    "        print(\"\\nWeight:\",pesi)    \n",
    "        history20.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruisco una rete neurale con $N_{epochs}=30$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0073 - mse: 0.0073\n",
      "\n",
      "Weight: [array([[2.0003114]], dtype=float32), array([1.0053222], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0845 - mse: 0.0845 - val_loss: 0.1056 - val_mse: 0.1056\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0845 - mse: 0.0845 - val_loss: 0.1056 - val_mse: 0.1056\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0845 - mse: 0.0845 - val_loss: 0.1057 - val_mse: 0.1057\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0845 - mse: 0.0845 - val_loss: 0.1057 - val_mse: 0.1057\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1057 - val_mse: 0.1057\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1057 - val_mse: 0.1057\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1057 - val_mse: 0.1057\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1057 - val_mse: 0.1057\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1057 - val_mse: 0.1057\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1058 - val_mse: 0.1058\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1059 - val_mse: 0.1059\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1059 - val_mse: 0.1059\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1059 - val_mse: 0.1059\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1059 - val_mse: 0.1059\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1060 - val_mse: 0.1060\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1060 - val_mse: 0.1060\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0843 - mse: 0.0843 - val_loss: 0.1060 - val_mse: 0.1060\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1059 - val_mse: 0.1059\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0843 - mse: 0.0843 - val_loss: 0.1060 - val_mse: 0.1060\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1060 - val_mse: 0.1060\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1061 - val_mse: 0.1061\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1061 - val_mse: 0.1061\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0843 - mse: 0.0843 - val_loss: 0.1060 - val_mse: 0.1060\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1061 - val_mse: 0.1061\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1061 - val_mse: 0.1061\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1062 - val_mse: 0.1062\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0843 - mse: 0.0843 - val_loss: 0.1062 - val_mse: 0.1062\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1061 - val_mse: 0.1061\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0843 - mse: 0.0843 - val_loss: 0.1061 - val_mse: 0.1061\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0844 - mse: 0.0844 - val_loss: 0.1061 - val_mse: 0.1061\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.1061 - mse: 0.1061\n",
      "\n",
      "Weight: [array([[1.9784647]], dtype=float32), array([1.0065663], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2626 - mse: 0.2626 - val_loss: 0.2498 - val_mse: 0.2498\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2625 - mse: 0.2625 - val_loss: 0.2502 - val_mse: 0.2502\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2623 - mse: 0.2623 - val_loss: 0.2504 - val_mse: 0.2504\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2621 - mse: 0.2621 - val_loss: 0.2505 - val_mse: 0.2505\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2507 - val_mse: 0.2507\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.2508 - val_mse: 0.2508\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2508 - val_mse: 0.2508\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2511 - val_mse: 0.2511\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2618 - mse: 0.2618 - val_loss: 0.2512 - val_mse: 0.2512\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2618 - mse: 0.2618 - val_loss: 0.2515 - val_mse: 0.2515\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2618 - mse: 0.2618 - val_loss: 0.2518 - val_mse: 0.2518\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2618 - mse: 0.2618 - val_loss: 0.2519 - val_mse: 0.2519\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2617 - mse: 0.2617 - val_loss: 0.2523 - val_mse: 0.2523\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2616 - mse: 0.2616 - val_loss: 0.2524 - val_mse: 0.2524\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2618 - mse: 0.2618 - val_loss: 0.2524 - val_mse: 0.2524\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2616 - mse: 0.2616 - val_loss: 0.2524 - val_mse: 0.2524\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2617 - mse: 0.2617 - val_loss: 0.2526 - val_mse: 0.2526\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2617 - mse: 0.2617 - val_loss: 0.2526 - val_mse: 0.2526\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2616 - mse: 0.2616 - val_loss: 0.2528 - val_mse: 0.2528\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2616 - mse: 0.2616 - val_loss: 0.2528 - val_mse: 0.2528\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2617 - mse: 0.2617 - val_loss: 0.2528 - val_mse: 0.2528\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2616 - mse: 0.2616 - val_loss: 0.2529 - val_mse: 0.2529\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2618 - mse: 0.2618 - val_loss: 0.2531 - val_mse: 0.2531\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2617 - mse: 0.2617 - val_loss: 0.2530 - val_mse: 0.2530\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2617 - mse: 0.2617 - val_loss: 0.2532 - val_mse: 0.2532\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2616 - mse: 0.2616 - val_loss: 0.2534 - val_mse: 0.2534\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2616 - mse: 0.2616 - val_loss: 0.2536 - val_mse: 0.2536\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2616 - mse: 0.2616 - val_loss: 0.2537 - val_mse: 0.2537\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2616 - mse: 0.2616 - val_loss: 0.2537 - val_mse: 0.2537\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2617 - mse: 0.2617 - val_loss: 0.2535 - val_mse: 0.2535\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2535 - mse: 0.2535\n",
      "\n",
      "Weight: [array([[2.031633]], dtype=float32), array([1.018219], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4801 - mse: 0.4801 - val_loss: 0.4928 - val_mse: 0.4928\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4797 - mse: 0.4797 - val_loss: 0.4939 - val_mse: 0.4939\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4797 - mse: 0.4797 - val_loss: 0.4945 - val_mse: 0.4945\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4796 - mse: 0.4796 - val_loss: 0.4950 - val_mse: 0.4950\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4798 - mse: 0.4798 - val_loss: 0.4956 - val_mse: 0.4956\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4797 - mse: 0.4797 - val_loss: 0.4964 - val_mse: 0.4964\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4795 - mse: 0.4795 - val_loss: 0.4966 - val_mse: 0.4966\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4795 - mse: 0.4795 - val_loss: 0.4973 - val_mse: 0.4973\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4795 - mse: 0.4795 - val_loss: 0.4976 - val_mse: 0.4976\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4796 - mse: 0.4796 - val_loss: 0.4977 - val_mse: 0.4977\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4794 - mse: 0.4794 - val_loss: 0.4988 - val_mse: 0.4988\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4795 - mse: 0.4795 - val_loss: 0.4988 - val_mse: 0.4988\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4795 - mse: 0.4795 - val_loss: 0.4987 - val_mse: 0.4987\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4794 - mse: 0.4794 - val_loss: 0.4988 - val_mse: 0.4988\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4796 - mse: 0.4796 - val_loss: 0.4988 - val_mse: 0.4988\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4796 - mse: 0.4796 - val_loss: 0.4994 - val_mse: 0.4994\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4795 - mse: 0.4795 - val_loss: 0.5002 - val_mse: 0.5002\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4794 - mse: 0.4794 - val_loss: 0.5003 - val_mse: 0.5003\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4795 - mse: 0.4795 - val_loss: 0.5004 - val_mse: 0.5004\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4795 - mse: 0.4795 - val_loss: 0.5003 - val_mse: 0.5003\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4793 - mse: 0.4793 - val_loss: 0.5007 - val_mse: 0.5007\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4794 - mse: 0.4794 - val_loss: 0.5008 - val_mse: 0.5008\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4795 - mse: 0.4795 - val_loss: 0.5012 - val_mse: 0.5012\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4795 - mse: 0.4795 - val_loss: 0.5011 - val_mse: 0.5011\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4795 - mse: 0.4795 - val_loss: 0.5014 - val_mse: 0.5014\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4793 - mse: 0.4793 - val_loss: 0.5014 - val_mse: 0.5014\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4793 - mse: 0.4793 - val_loss: 0.5015 - val_mse: 0.5015\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4796 - mse: 0.4796 - val_loss: 0.5015 - val_mse: 0.5015\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4794 - mse: 0.4794 - val_loss: 0.5021 - val_mse: 0.5021\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4795 - mse: 0.4795 - val_loss: 0.5021 - val_mse: 0.5021\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5021 - mse: 0.5021\n",
      "\n",
      "Weight: [array([[1.9973022]], dtype=float32), array([1.0263941], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8075 - mse: 0.8075 - val_loss: 0.8503 - val_mse: 0.8503\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8076 - mse: 0.8076 - val_loss: 0.8502 - val_mse: 0.8502\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8072 - mse: 0.8072 - val_loss: 0.8506 - val_mse: 0.8506\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8074 - mse: 0.8074 - val_loss: 0.8505 - val_mse: 0.8505\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8074 - mse: 0.8074 - val_loss: 0.8503 - val_mse: 0.8503\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8072 - mse: 0.8072 - val_loss: 0.8508 - val_mse: 0.8508\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8076 - mse: 0.8076 - val_loss: 0.8519 - val_mse: 0.8519\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8076 - mse: 0.8076 - val_loss: 0.8507 - val_mse: 0.8507\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8072 - mse: 0.8072 - val_loss: 0.8510 - val_mse: 0.8510\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8076 - mse: 0.8076 - val_loss: 0.8515 - val_mse: 0.8515\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8072 - mse: 0.8072 - val_loss: 0.8505 - val_mse: 0.8505\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8077 - mse: 0.8077 - val_loss: 0.8481 - val_mse: 0.8481\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8074 - mse: 0.8074 - val_loss: 0.8494 - val_mse: 0.8494\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8074 - mse: 0.8074 - val_loss: 0.8507 - val_mse: 0.8507\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8072 - mse: 0.8072 - val_loss: 0.8501 - val_mse: 0.8501\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8074 - mse: 0.8074 - val_loss: 0.8503 - val_mse: 0.8503\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8074 - mse: 0.8074 - val_loss: 0.8503 - val_mse: 0.8503\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8072 - mse: 0.8072 - val_loss: 0.8519 - val_mse: 0.8519\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8073 - mse: 0.8073 - val_loss: 0.8516 - val_mse: 0.8516\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8074 - mse: 0.8074 - val_loss: 0.8525 - val_mse: 0.8525\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8075 - mse: 0.8075 - val_loss: 0.8538 - val_mse: 0.8538\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8074 - mse: 0.8074 - val_loss: 0.8527 - val_mse: 0.8527\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8072 - mse: 0.8072 - val_loss: 0.8529 - val_mse: 0.8529\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8076 - mse: 0.8076 - val_loss: 0.8516 - val_mse: 0.8516\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8074 - mse: 0.8074 - val_loss: 0.8525 - val_mse: 0.8525\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8075 - mse: 0.8075 - val_loss: 0.8510 - val_mse: 0.8510\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8074 - mse: 0.8074 - val_loss: 0.8517 - val_mse: 0.8517\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8072 - mse: 0.8072 - val_loss: 0.8516 - val_mse: 0.8516\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8071 - mse: 0.8071 - val_loss: 0.8501 - val_mse: 0.8501\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8073 - mse: 0.8073 - val_loss: 0.8506 - val_mse: 0.8506\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.8506 - mse: 0.8506\n",
      "\n",
      "Weight: [array([[2.0097792]], dtype=float32), array([1.0280838], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.4065 - mse: 1.4065 - val_loss: 1.1193 - val_mse: 1.1193\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4051 - mse: 1.4051 - val_loss: 1.1237 - val_mse: 1.1237\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4054 - mse: 1.4054 - val_loss: 1.1288 - val_mse: 1.1288\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4048 - mse: 1.4048 - val_loss: 1.1324 - val_mse: 1.1324\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4048 - mse: 1.4048 - val_loss: 1.1311 - val_mse: 1.1311\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4047 - mse: 1.4047 - val_loss: 1.1316 - val_mse: 1.1316\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4044 - mse: 1.4044 - val_loss: 1.1339 - val_mse: 1.1339\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4041 - mse: 1.4041 - val_loss: 1.1329 - val_mse: 1.1329\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4044 - mse: 1.4044 - val_loss: 1.1325 - val_mse: 1.1325\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4043 - mse: 1.4043 - val_loss: 1.1338 - val_mse: 1.1338\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4047 - mse: 1.4047 - val_loss: 1.1315 - val_mse: 1.1315\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4042 - mse: 1.4042 - val_loss: 1.1290 - val_mse: 1.1290\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4043 - mse: 1.4043 - val_loss: 1.1310 - val_mse: 1.1310\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4040 - mse: 1.4040 - val_loss: 1.1321 - val_mse: 1.1321\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4046 - mse: 1.4046 - val_loss: 1.1336 - val_mse: 1.1336\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4043 - mse: 1.4043 - val_loss: 1.1356 - val_mse: 1.1356\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4042 - mse: 1.4042 - val_loss: 1.1349 - val_mse: 1.1349\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4043 - mse: 1.4043 - val_loss: 1.1360 - val_mse: 1.1360\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4040 - mse: 1.4040 - val_loss: 1.1344 - val_mse: 1.1344\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4044 - mse: 1.4044 - val_loss: 1.1325 - val_mse: 1.1325\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4041 - mse: 1.4041 - val_loss: 1.1345 - val_mse: 1.1345\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4043 - mse: 1.4043 - val_loss: 1.1350 - val_mse: 1.1350\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4044 - mse: 1.4044 - val_loss: 1.1348 - val_mse: 1.1348\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4040 - mse: 1.4040 - val_loss: 1.1328 - val_mse: 1.1328\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4042 - mse: 1.4042 - val_loss: 1.1340 - val_mse: 1.1340\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4042 - mse: 1.4042 - val_loss: 1.1345 - val_mse: 1.1345\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4038 - mse: 1.4038 - val_loss: 1.1345 - val_mse: 1.1345\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4040 - mse: 1.4040 - val_loss: 1.1353 - val_mse: 1.1353\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4043 - mse: 1.4043 - val_loss: 1.1353 - val_mse: 1.1353\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4042 - mse: 1.4042 - val_loss: 1.1334 - val_mse: 1.1334\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.1334 - mse: 1.1334\n",
      "\n",
      "Weight: [array([[2.006283]], dtype=float32), array([1.0807127], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0130 - val_mse: 0.0130\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0094 - mse: 0.0094\n",
      "\n",
      "Weight: [array([[1.9823668]], dtype=float32), array([0.9985425], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0947 - mse: 0.0947 - val_loss: 0.1221 - val_mse: 0.1221\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0945 - mse: 0.0945 - val_loss: 0.1223 - val_mse: 0.1223\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0944 - mse: 0.0944 - val_loss: 0.1224 - val_mse: 0.1224\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0943 - mse: 0.0943 - val_loss: 0.1226 - val_mse: 0.1226\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0942 - mse: 0.0942 - val_loss: 0.1227 - val_mse: 0.1227\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0941 - mse: 0.0941 - val_loss: 0.1228 - val_mse: 0.1228\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0941 - mse: 0.0941 - val_loss: 0.1230 - val_mse: 0.1230\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0941 - mse: 0.0941 - val_loss: 0.1231 - val_mse: 0.1231\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1232 - val_mse: 0.1232\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0941 - mse: 0.0941 - val_loss: 0.1234 - val_mse: 0.1234\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1234 - val_mse: 0.1234\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1235 - val_mse: 0.1235\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1236 - val_mse: 0.1236\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1236 - val_mse: 0.1236\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1237 - val_mse: 0.1237\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1238 - val_mse: 0.1238\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1239 - val_mse: 0.1239\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1238 - val_mse: 0.1238\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1238 - val_mse: 0.1238\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1239 - val_mse: 0.1239\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1239 - val_mse: 0.1239\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1240 - val_mse: 0.1240\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0939 - mse: 0.0939 - val_loss: 0.1240 - val_mse: 0.1240\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0939 - mse: 0.0939 - val_loss: 0.1239 - val_mse: 0.1239\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1239 - val_mse: 0.1239\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1239 - val_mse: 0.1239\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1240 - val_mse: 0.1240\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0939 - mse: 0.0939 - val_loss: 0.1240 - val_mse: 0.1240\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1240 - val_mse: 0.1240\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0940 - mse: 0.0940 - val_loss: 0.1241 - val_mse: 0.1241\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.1241 - mse: 0.1241\n",
      "\n",
      "Weight: [array([[2.0307086]], dtype=float32), array([0.9986245], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2457 - mse: 0.2457 - val_loss: 0.2290 - val_mse: 0.2290\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2456 - mse: 0.2456 - val_loss: 0.2293 - val_mse: 0.2293\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2455 - mse: 0.2455 - val_loss: 0.2294 - val_mse: 0.2294\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2454 - mse: 0.2454 - val_loss: 0.2296 - val_mse: 0.2296\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2453 - mse: 0.2453 - val_loss: 0.2299 - val_mse: 0.2299\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2453 - mse: 0.2453 - val_loss: 0.2301 - val_mse: 0.2301\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2452 - mse: 0.2452 - val_loss: 0.2304 - val_mse: 0.2304\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2452 - mse: 0.2452 - val_loss: 0.2305 - val_mse: 0.2305\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2452 - mse: 0.2452 - val_loss: 0.2307 - val_mse: 0.2307\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2451 - mse: 0.2451 - val_loss: 0.2308 - val_mse: 0.2308\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2452 - mse: 0.2452 - val_loss: 0.2310 - val_mse: 0.2310\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2451 - mse: 0.2451 - val_loss: 0.2310 - val_mse: 0.2310\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2452 - mse: 0.2452 - val_loss: 0.2310 - val_mse: 0.2310\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2452 - mse: 0.2452 - val_loss: 0.2311 - val_mse: 0.2311\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2451 - mse: 0.2451 - val_loss: 0.2312 - val_mse: 0.2312\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2452 - mse: 0.2452 - val_loss: 0.2314 - val_mse: 0.2314\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2450 - mse: 0.2450 - val_loss: 0.2313 - val_mse: 0.2313\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2452 - mse: 0.2452 - val_loss: 0.2314 - val_mse: 0.2314\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2452 - mse: 0.2452 - val_loss: 0.2314 - val_mse: 0.2314\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2453 - mse: 0.2453 - val_loss: 0.2315 - val_mse: 0.2315\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2451 - mse: 0.2451 - val_loss: 0.2316 - val_mse: 0.2316\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2451 - mse: 0.2451 - val_loss: 0.2317 - val_mse: 0.2317\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2451 - mse: 0.2451 - val_loss: 0.2317 - val_mse: 0.2317\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2451 - mse: 0.2451 - val_loss: 0.2315 - val_mse: 0.2315\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2451 - mse: 0.2451 - val_loss: 0.2316 - val_mse: 0.2316\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2451 - mse: 0.2451 - val_loss: 0.2315 - val_mse: 0.2315\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2451 - mse: 0.2451 - val_loss: 0.2317 - val_mse: 0.2317\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2451 - mse: 0.2451 - val_loss: 0.2318 - val_mse: 0.2318\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2451 - mse: 0.2451 - val_loss: 0.2317 - val_mse: 0.2317\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2451 - mse: 0.2451 - val_loss: 0.2318 - val_mse: 0.2318\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2318 - mse: 0.2318\n",
      "\n",
      "Weight: [array([[1.9871744]], dtype=float32), array([0.9986313], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4811 - mse: 0.4811 - val_loss: 0.4650 - val_mse: 0.4650\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4802 - mse: 0.4802 - val_loss: 0.4659 - val_mse: 0.4659\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4800 - mse: 0.4800 - val_loss: 0.4658 - val_mse: 0.4658\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4793 - mse: 0.4793 - val_loss: 0.4658 - val_mse: 0.4658\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4792 - mse: 0.4792 - val_loss: 0.4657 - val_mse: 0.4657\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4791 - mse: 0.4791 - val_loss: 0.4663 - val_mse: 0.4663\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4790 - mse: 0.4790 - val_loss: 0.4663 - val_mse: 0.4663\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.4668 - val_mse: 0.4668\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4786 - mse: 0.4786 - val_loss: 0.4664 - val_mse: 0.4664\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4787 - mse: 0.4787 - val_loss: 0.4660 - val_mse: 0.4660\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4786 - mse: 0.4786 - val_loss: 0.4658 - val_mse: 0.4658\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4787 - mse: 0.4787 - val_loss: 0.4652 - val_mse: 0.4652\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4785 - mse: 0.4785 - val_loss: 0.4654 - val_mse: 0.4654\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4786 - mse: 0.4786 - val_loss: 0.4653 - val_mse: 0.4653\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4786 - mse: 0.4786 - val_loss: 0.4651 - val_mse: 0.4651\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4786 - mse: 0.4786 - val_loss: 0.4652 - val_mse: 0.4652\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4785 - mse: 0.4785 - val_loss: 0.4648 - val_mse: 0.4648\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4786 - mse: 0.4786 - val_loss: 0.4642 - val_mse: 0.4642\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4785 - mse: 0.4785 - val_loss: 0.4642 - val_mse: 0.4642\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4785 - mse: 0.4785 - val_loss: 0.4643 - val_mse: 0.4643\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4785 - mse: 0.4785 - val_loss: 0.4635 - val_mse: 0.4635\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4785 - mse: 0.4785 - val_loss: 0.4642 - val_mse: 0.4642\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4786 - mse: 0.4786 - val_loss: 0.4643 - val_mse: 0.4643\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4786 - mse: 0.4786 - val_loss: 0.4644 - val_mse: 0.4644\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4785 - mse: 0.4785 - val_loss: 0.4641 - val_mse: 0.4641\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4785 - mse: 0.4785 - val_loss: 0.4643 - val_mse: 0.4643\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4785 - mse: 0.4785 - val_loss: 0.4647 - val_mse: 0.4647\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4786 - mse: 0.4786 - val_loss: 0.4642 - val_mse: 0.4642\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4785 - mse: 0.4785 - val_loss: 0.4643 - val_mse: 0.4643\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4786 - mse: 0.4786 - val_loss: 0.4655 - val_mse: 0.4655\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4655 - mse: 0.4655\n",
      "\n",
      "Weight: [array([[1.9056075]], dtype=float32), array([0.97239685], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.7830 - mse: 0.7830 - val_loss: 0.7308 - val_mse: 0.7308\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7821 - mse: 0.7821 - val_loss: 0.7299 - val_mse: 0.7299\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7824 - mse: 0.7824 - val_loss: 0.7301 - val_mse: 0.7301\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7819 - mse: 0.7819 - val_loss: 0.7299 - val_mse: 0.7299\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7817 - mse: 0.7817 - val_loss: 0.7295 - val_mse: 0.7295\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7818 - mse: 0.7818 - val_loss: 0.7297 - val_mse: 0.7297\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7819 - mse: 0.7819 - val_loss: 0.7298 - val_mse: 0.7298\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7818 - mse: 0.7818 - val_loss: 0.7295 - val_mse: 0.7295\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7818 - mse: 0.7818 - val_loss: 0.7292 - val_mse: 0.7292\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7814 - mse: 0.7814 - val_loss: 0.7285 - val_mse: 0.7285\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7815 - mse: 0.7815 - val_loss: 0.7286 - val_mse: 0.7286\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7818 - mse: 0.7818 - val_loss: 0.7284 - val_mse: 0.7284\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7816 - mse: 0.7816 - val_loss: 0.7281 - val_mse: 0.7281\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7815 - mse: 0.7815 - val_loss: 0.7281 - val_mse: 0.7281\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7814 - mse: 0.7814 - val_loss: 0.7284 - val_mse: 0.7284\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7815 - mse: 0.7815 - val_loss: 0.7284 - val_mse: 0.7284\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7815 - mse: 0.7815 - val_loss: 0.7284 - val_mse: 0.7284\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7814 - mse: 0.7814 - val_loss: 0.7282 - val_mse: 0.7282\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7813 - mse: 0.7813 - val_loss: 0.7282 - val_mse: 0.7282\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7816 - mse: 0.7816 - val_loss: 0.7279 - val_mse: 0.7279\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.7812 - mse: 0.7812 - val_loss: 0.7282 - val_mse: 0.7282\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7817 - mse: 0.7817 - val_loss: 0.7280 - val_mse: 0.7280\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7813 - mse: 0.7813 - val_loss: 0.7285 - val_mse: 0.7285\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7814 - mse: 0.7814 - val_loss: 0.7285 - val_mse: 0.7285\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7816 - mse: 0.7816 - val_loss: 0.7288 - val_mse: 0.7288\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7815 - mse: 0.7815 - val_loss: 0.7286 - val_mse: 0.7286\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7817 - mse: 0.7817 - val_loss: 0.7285 - val_mse: 0.7285\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7815 - mse: 0.7815 - val_loss: 0.7288 - val_mse: 0.7288\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7814 - mse: 0.7814 - val_loss: 0.7284 - val_mse: 0.7284\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7815 - mse: 0.7815 - val_loss: 0.7284 - val_mse: 0.7284\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.7284 - mse: 0.7284\n",
      "\n",
      "Weight: [array([[1.9621851]], dtype=float32), array([0.9585649], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1.1637 - mse: 1.1637 - val_loss: 1.2220 - val_mse: 1.2220\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1629 - mse: 1.1629 - val_loss: 1.2226 - val_mse: 1.2226\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1632 - mse: 1.1632 - val_loss: 1.2229 - val_mse: 1.2229\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1633 - mse: 1.1633 - val_loss: 1.2224 - val_mse: 1.2224\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1631 - mse: 1.1631 - val_loss: 1.2228 - val_mse: 1.2228\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1633 - mse: 1.1633 - val_loss: 1.2234 - val_mse: 1.2234\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1628 - mse: 1.1628 - val_loss: 1.2234 - val_mse: 1.2234\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1633 - mse: 1.1633 - val_loss: 1.2236 - val_mse: 1.2236\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1630 - mse: 1.1630 - val_loss: 1.2231 - val_mse: 1.2231\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1631 - mse: 1.1631 - val_loss: 1.2237 - val_mse: 1.2237\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1632 - mse: 1.1632 - val_loss: 1.2241 - val_mse: 1.2241\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1630 - mse: 1.1630 - val_loss: 1.2236 - val_mse: 1.2236\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1631 - mse: 1.1631 - val_loss: 1.2239 - val_mse: 1.2239\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1628 - mse: 1.1628 - val_loss: 1.2240 - val_mse: 1.2240\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1634 - mse: 1.1634 - val_loss: 1.2237 - val_mse: 1.2237\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1631 - mse: 1.1631 - val_loss: 1.2236 - val_mse: 1.2236\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1634 - mse: 1.1634 - val_loss: 1.2233 - val_mse: 1.2233\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1631 - mse: 1.1631 - val_loss: 1.2233 - val_mse: 1.2233\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1631 - mse: 1.1631 - val_loss: 1.2235 - val_mse: 1.2235\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1631 - mse: 1.1631 - val_loss: 1.2245 - val_mse: 1.2245\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1634 - mse: 1.1634 - val_loss: 1.2244 - val_mse: 1.2244\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1629 - mse: 1.1629 - val_loss: 1.2243 - val_mse: 1.2243\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1627 - mse: 1.1627 - val_loss: 1.2240 - val_mse: 1.2240\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1627 - mse: 1.1627 - val_loss: 1.2234 - val_mse: 1.2234\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1636 - mse: 1.1636 - val_loss: 1.2232 - val_mse: 1.2232\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1630 - mse: 1.1630 - val_loss: 1.2237 - val_mse: 1.2237\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1631 - mse: 1.1631 - val_loss: 1.2233 - val_mse: 1.2233\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1634 - mse: 1.1634 - val_loss: 1.2233 - val_mse: 1.2233\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1634 - mse: 1.1634 - val_loss: 1.2234 - val_mse: 1.2234\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1633 - mse: 1.1633 - val_loss: 1.2228 - val_mse: 1.2228\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.2228 - mse: 1.2228\n",
      "\n",
      "Weight: [array([[1.9904648]], dtype=float32), array([0.95514804], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0082 - mse: 0.0082\n",
      "\n",
      "Weight: [array([[1.994397]], dtype=float32), array([1.0021476], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0877 - mse: 0.0877 - val_loss: 0.0745 - val_mse: 0.0745\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0877 - mse: 0.0877 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0741 - val_mse: 0.0741\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0740 - val_mse: 0.0740\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0739 - val_mse: 0.0739\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0738 - val_mse: 0.0738\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0738 - val_mse: 0.0738\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0737 - val_mse: 0.0737\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0737 - val_mse: 0.0737\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0736 - val_mse: 0.0736\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0736 - val_mse: 0.0736\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0736 - val_mse: 0.0736\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0736 - val_mse: 0.0736\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0736 - val_mse: 0.0736\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0736 - val_mse: 0.0736\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0736 - val_mse: 0.0736\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0736 - val_mse: 0.0736\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0736 - val_mse: 0.0736\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0736 - val_mse: 0.0736\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0736 - val_mse: 0.0736\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0736 - val_mse: 0.0736\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0736 - val_mse: 0.0736\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0735 - val_mse: 0.0735\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0735 - val_mse: 0.0735\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0735 - val_mse: 0.0735\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0735 - val_mse: 0.0735\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0734 - val_mse: 0.0734\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0735 - val_mse: 0.0735\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0735 - val_mse: 0.0735\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.0735 - val_mse: 0.0735\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0735 - mse: 0.0735\n",
      "\n",
      "Weight: [array([[1.9750184]], dtype=float32), array([1.009495], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2457 - mse: 0.2457 - val_loss: 0.2078 - val_mse: 0.2078\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2454 - mse: 0.2454 - val_loss: 0.2072 - val_mse: 0.2072\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2452 - mse: 0.2452 - val_loss: 0.2070 - val_mse: 0.2070\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2451 - mse: 0.2451 - val_loss: 0.2069 - val_mse: 0.2069\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2450 - mse: 0.2450 - val_loss: 0.2069 - val_mse: 0.2069\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2449 - mse: 0.2449 - val_loss: 0.2067 - val_mse: 0.2067\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2449 - mse: 0.2449 - val_loss: 0.2066 - val_mse: 0.2066\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2449 - mse: 0.2449 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2448 - mse: 0.2448 - val_loss: 0.2064 - val_mse: 0.2064\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2448 - mse: 0.2448 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2448 - mse: 0.2448 - val_loss: 0.2066 - val_mse: 0.2066\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2449 - mse: 0.2449 - val_loss: 0.2066 - val_mse: 0.2066\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2448 - mse: 0.2448 - val_loss: 0.2063 - val_mse: 0.2063\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2064 - val_mse: 0.2064\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2448 - mse: 0.2448 - val_loss: 0.2061 - val_mse: 0.2061\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2448 - mse: 0.2448 - val_loss: 0.2063 - val_mse: 0.2063\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2063 - val_mse: 0.2063\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2064 - val_mse: 0.2064\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2064 - val_mse: 0.2064\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2446 - mse: 0.2446 - val_loss: 0.2066 - val_mse: 0.2066\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2448 - mse: 0.2448 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2066 - val_mse: 0.2066\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2448 - mse: 0.2448 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2064 - val_mse: 0.2064\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2064 - val_mse: 0.2064\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2063 - val_mse: 0.2063\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2063 - val_mse: 0.2063\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2063 - mse: 0.2063\n",
      "\n",
      "Weight: [array([[2.0288165]], dtype=float32), array([1.0004582], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4697 - mse: 0.4697 - val_loss: 0.3586 - val_mse: 0.3586\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4696 - mse: 0.4696 - val_loss: 0.3583 - val_mse: 0.3583\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4696 - mse: 0.4696 - val_loss: 0.3579 - val_mse: 0.3579\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4695 - mse: 0.4695 - val_loss: 0.3579 - val_mse: 0.3579\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4695 - mse: 0.4695 - val_loss: 0.3578 - val_mse: 0.3578\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4694 - mse: 0.4694 - val_loss: 0.3578 - val_mse: 0.3578\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4695 - mse: 0.4695 - val_loss: 0.3577 - val_mse: 0.3577\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4695 - mse: 0.4695 - val_loss: 0.3575 - val_mse: 0.3575\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4696 - mse: 0.4696 - val_loss: 0.3576 - val_mse: 0.3576\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4696 - mse: 0.4696 - val_loss: 0.3576 - val_mse: 0.3576\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4696 - mse: 0.4696 - val_loss: 0.3574 - val_mse: 0.3574\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4694 - mse: 0.4694 - val_loss: 0.3574 - val_mse: 0.3574\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4695 - mse: 0.4695 - val_loss: 0.3571 - val_mse: 0.3571\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4697 - mse: 0.4697 - val_loss: 0.3572 - val_mse: 0.3572\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4695 - mse: 0.4695 - val_loss: 0.3573 - val_mse: 0.3573\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4695 - mse: 0.4695 - val_loss: 0.3573 - val_mse: 0.3573\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4696 - mse: 0.4696 - val_loss: 0.3574 - val_mse: 0.3574\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4694 - mse: 0.4694 - val_loss: 0.3574 - val_mse: 0.3574\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4696 - mse: 0.4696 - val_loss: 0.3572 - val_mse: 0.3572\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4695 - mse: 0.4695 - val_loss: 0.3572 - val_mse: 0.3572\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4696 - mse: 0.4696 - val_loss: 0.3571 - val_mse: 0.3571\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4694 - mse: 0.4694 - val_loss: 0.3571 - val_mse: 0.3571\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4695 - mse: 0.4695 - val_loss: 0.3570 - val_mse: 0.3570\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4695 - mse: 0.4695 - val_loss: 0.3571 - val_mse: 0.3571\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4695 - mse: 0.4695 - val_loss: 0.3572 - val_mse: 0.3572\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4695 - mse: 0.4695 - val_loss: 0.3572 - val_mse: 0.3572\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4695 - mse: 0.4695 - val_loss: 0.3572 - val_mse: 0.3572\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4695 - mse: 0.4695 - val_loss: 0.3573 - val_mse: 0.3573\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4695 - mse: 0.4695 - val_loss: 0.3575 - val_mse: 0.3575\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4695 - mse: 0.4695 - val_loss: 0.3572 - val_mse: 0.3572\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.3572 - mse: 0.3572\n",
      "\n",
      "Weight: [array([[2.0151265]], dtype=float32), array([1.0033982], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.8409 - mse: 0.8409 - val_loss: 0.9048 - val_mse: 0.9048\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8385 - mse: 0.8385 - val_loss: 0.9145 - val_mse: 0.9145\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8377 - mse: 0.8377 - val_loss: 0.9222 - val_mse: 0.9222\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8371 - mse: 0.8371 - val_loss: 0.9290 - val_mse: 0.9290\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8369 - mse: 0.8369 - val_loss: 0.9303 - val_mse: 0.9303\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8367 - mse: 0.8367 - val_loss: 0.9343 - val_mse: 0.9343\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8371 - mse: 0.8371 - val_loss: 0.9340 - val_mse: 0.9340\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8369 - mse: 0.8369 - val_loss: 0.9344 - val_mse: 0.9344\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8369 - mse: 0.8369 - val_loss: 0.9334 - val_mse: 0.9334\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8368 - mse: 0.8368 - val_loss: 0.9354 - val_mse: 0.9354\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8368 - mse: 0.8368 - val_loss: 0.9373 - val_mse: 0.9373\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8369 - mse: 0.8369 - val_loss: 0.9369 - val_mse: 0.9369\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8370 - mse: 0.8370 - val_loss: 0.9363 - val_mse: 0.9363\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8368 - mse: 0.8368 - val_loss: 0.9343 - val_mse: 0.9343\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8367 - mse: 0.8367 - val_loss: 0.9333 - val_mse: 0.9333\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8367 - mse: 0.8367 - val_loss: 0.9344 - val_mse: 0.9344\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8367 - mse: 0.8367 - val_loss: 0.9345 - val_mse: 0.9345\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8367 - mse: 0.8367 - val_loss: 0.9341 - val_mse: 0.9341\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8368 - mse: 0.8368 - val_loss: 0.9350 - val_mse: 0.9350\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8368 - mse: 0.8368 - val_loss: 0.9368 - val_mse: 0.9368\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8368 - mse: 0.8368 - val_loss: 0.9359 - val_mse: 0.9359\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8369 - mse: 0.8369 - val_loss: 0.9360 - val_mse: 0.9360\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8366 - mse: 0.8366 - val_loss: 0.9395 - val_mse: 0.9395\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8367 - mse: 0.8367 - val_loss: 0.9374 - val_mse: 0.9374\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8368 - mse: 0.8368 - val_loss: 0.9381 - val_mse: 0.9381\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8367 - mse: 0.8367 - val_loss: 0.9371 - val_mse: 0.9371\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8367 - mse: 0.8367 - val_loss: 0.9364 - val_mse: 0.9364\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8367 - mse: 0.8367 - val_loss: 0.9368 - val_mse: 0.9368\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8372 - mse: 0.8372 - val_loss: 0.9372 - val_mse: 0.9372\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8366 - mse: 0.8366 - val_loss: 0.9352 - val_mse: 0.9352\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.9352 - mse: 0.9352\n",
      "\n",
      "Weight: [array([[1.9827532]], dtype=float32), array([1.0790759], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.2026 - mse: 1.2026 - val_loss: 1.4386 - val_mse: 1.4386\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2008 - mse: 1.2008 - val_loss: 1.4340 - val_mse: 1.4340\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2002 - mse: 1.2002 - val_loss: 1.4323 - val_mse: 1.4323\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1995 - mse: 1.1995 - val_loss: 1.4303 - val_mse: 1.4303\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1995 - mse: 1.1995 - val_loss: 1.4295 - val_mse: 1.4295\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1990 - mse: 1.1990 - val_loss: 1.4276 - val_mse: 1.4276\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1992 - mse: 1.1992 - val_loss: 1.4271 - val_mse: 1.4271\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1990 - mse: 1.1990 - val_loss: 1.4276 - val_mse: 1.4276\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1992 - mse: 1.1992 - val_loss: 1.4273 - val_mse: 1.4273\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1993 - mse: 1.1993 - val_loss: 1.4270 - val_mse: 1.4270\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1996 - mse: 1.1996 - val_loss: 1.4266 - val_mse: 1.4266\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1995 - mse: 1.1995 - val_loss: 1.4267 - val_mse: 1.4267\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1994 - mse: 1.1994 - val_loss: 1.4262 - val_mse: 1.4262\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1992 - mse: 1.1992 - val_loss: 1.4269 - val_mse: 1.4269\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1991 - mse: 1.1991 - val_loss: 1.4272 - val_mse: 1.4272\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1995 - mse: 1.1995 - val_loss: 1.4269 - val_mse: 1.4269\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1991 - mse: 1.1991 - val_loss: 1.4269 - val_mse: 1.4269\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1991 - mse: 1.1991 - val_loss: 1.4269 - val_mse: 1.4269\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1988 - mse: 1.1988 - val_loss: 1.4264 - val_mse: 1.4264\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1995 - mse: 1.1995 - val_loss: 1.4257 - val_mse: 1.4257\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1995 - mse: 1.1995 - val_loss: 1.4260 - val_mse: 1.4260\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1994 - mse: 1.1994 - val_loss: 1.4259 - val_mse: 1.4259\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1993 - mse: 1.1993 - val_loss: 1.4256 - val_mse: 1.4256\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1992 - mse: 1.1992 - val_loss: 1.4263 - val_mse: 1.4263\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1993 - mse: 1.1993 - val_loss: 1.4263 - val_mse: 1.4263\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1992 - mse: 1.1992 - val_loss: 1.4264 - val_mse: 1.4264\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1993 - mse: 1.1993 - val_loss: 1.4261 - val_mse: 1.4261\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1992 - mse: 1.1992 - val_loss: 1.4262 - val_mse: 1.4262\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1992 - mse: 1.1992 - val_loss: 1.4262 - val_mse: 1.4262\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1993 - mse: 1.1993 - val_loss: 1.4260 - val_mse: 1.4260\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.4260 - mse: 1.4260\n",
      "\n",
      "Weight: [array([[1.9941766]], dtype=float32), array([1.0080725], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/30\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 4/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 5/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 6/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 7/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 9/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 18/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 19/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 20/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 21/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 22/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 23/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 24/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 25/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 26/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 27/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 28/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 29/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 30/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0117 - mse: 0.0117\n",
      "\n",
      "Weight: [array([[1.9997699]], dtype=float32), array([0.9993205], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.0838 - val_mse: 0.0838\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.0837 - val_mse: 0.0837\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.0836 - val_mse: 0.0836\n",
      "Epoch 4/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.0836 - val_mse: 0.0836\n",
      "Epoch 5/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.0835 - val_mse: 0.0835\n",
      "Epoch 6/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.0835 - val_mse: 0.0835\n",
      "Epoch 7/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.0835 - val_mse: 0.0835\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.0834 - val_mse: 0.0834\n",
      "Epoch 9/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.0837 - val_mse: 0.0837\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.0835 - val_mse: 0.0835\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.0836 - val_mse: 0.0836\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.0836 - val_mse: 0.0836\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.0835 - val_mse: 0.0835\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.0834 - val_mse: 0.0834\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.0835 - val_mse: 0.0835\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.0834 - val_mse: 0.0834\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.0834 - val_mse: 0.0834\n",
      "Epoch 18/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.0835 - val_mse: 0.0835\n",
      "Epoch 19/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.0837 - val_mse: 0.0837\n",
      "Epoch 20/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.0838 - val_mse: 0.0838\n",
      "Epoch 21/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.0836 - val_mse: 0.0836\n",
      "Epoch 22/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.0836 - val_mse: 0.0836\n",
      "Epoch 23/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.0835 - val_mse: 0.0835\n",
      "Epoch 24/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.0835 - val_mse: 0.0835\n",
      "Epoch 25/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.0834 - val_mse: 0.0834\n",
      "Epoch 26/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.0834 - val_mse: 0.0834\n",
      "Epoch 27/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.0835 - val_mse: 0.0835\n",
      "Epoch 28/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.0835 - val_mse: 0.0835\n",
      "Epoch 29/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.0835 - val_mse: 0.0835\n",
      "Epoch 30/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.0836 - val_mse: 0.0836\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0836 - mse: 0.0836\n",
      "\n",
      "Weight: [array([[1.9969213]], dtype=float32), array([0.9949006], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2411 - mse: 0.2411 - val_loss: 0.2465 - val_mse: 0.2465\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2412 - mse: 0.2412 - val_loss: 0.2462 - val_mse: 0.2462\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2411 - mse: 0.2411 - val_loss: 0.2460 - val_mse: 0.2460\n",
      "Epoch 4/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2411 - mse: 0.2411 - val_loss: 0.2459 - val_mse: 0.2459\n",
      "Epoch 5/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2411 - mse: 0.2411 - val_loss: 0.2458 - val_mse: 0.2458\n",
      "Epoch 6/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2456 - val_mse: 0.2456\n",
      "Epoch 7/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2411 - mse: 0.2411 - val_loss: 0.2455 - val_mse: 0.2455\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2454 - val_mse: 0.2454\n",
      "Epoch 9/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2454 - val_mse: 0.2454\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2411 - mse: 0.2411 - val_loss: 0.2454 - val_mse: 0.2454\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2454 - val_mse: 0.2454\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2454 - val_mse: 0.2454\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2454 - val_mse: 0.2454\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2454 - val_mse: 0.2454\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2411 - mse: 0.2411 - val_loss: 0.2453 - val_mse: 0.2453\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2452 - val_mse: 0.2452\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2452 - val_mse: 0.2452\n",
      "Epoch 18/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2411 - mse: 0.2411 - val_loss: 0.2452 - val_mse: 0.2452\n",
      "Epoch 19/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2451 - val_mse: 0.2451\n",
      "Epoch 20/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2452 - val_mse: 0.2452\n",
      "Epoch 21/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2409 - mse: 0.2409 - val_loss: 0.2452 - val_mse: 0.2452\n",
      "Epoch 22/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2452 - val_mse: 0.2452\n",
      "Epoch 23/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2452 - val_mse: 0.2452\n",
      "Epoch 24/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2452 - val_mse: 0.2452\n",
      "Epoch 25/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2452 - val_mse: 0.2452\n",
      "Epoch 26/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2452 - val_mse: 0.2452\n",
      "Epoch 27/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2452 - val_mse: 0.2452\n",
      "Epoch 28/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2411 - mse: 0.2411 - val_loss: 0.2453 - val_mse: 0.2453\n",
      "Epoch 29/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2452 - val_mse: 0.2452\n",
      "Epoch 30/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2453 - val_mse: 0.2453\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2453 - mse: 0.2453\n",
      "\n",
      "Weight: [array([[2.0107229]], dtype=float32), array([1.0058831], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/30\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4884 - mse: 0.4884 - val_loss: 0.5841 - val_mse: 0.5841\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4880 - mse: 0.4880 - val_loss: 0.5847 - val_mse: 0.5847\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4879 - mse: 0.4879 - val_loss: 0.5853 - val_mse: 0.5853\n",
      "Epoch 4/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4879 - mse: 0.4879 - val_loss: 0.5858 - val_mse: 0.5858\n",
      "Epoch 5/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4878 - mse: 0.4878 - val_loss: 0.5859 - val_mse: 0.5859\n",
      "Epoch 6/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4878 - mse: 0.4878 - val_loss: 0.5862 - val_mse: 0.5862\n",
      "Epoch 7/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4878 - mse: 0.4878 - val_loss: 0.5863 - val_mse: 0.5863\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4878 - mse: 0.4878 - val_loss: 0.5863 - val_mse: 0.5863\n",
      "Epoch 9/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4878 - mse: 0.4878 - val_loss: 0.5864 - val_mse: 0.5864\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4879 - mse: 0.4879 - val_loss: 0.5864 - val_mse: 0.5864\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4880 - mse: 0.4880 - val_loss: 0.5865 - val_mse: 0.5865\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4878 - mse: 0.4878 - val_loss: 0.5865 - val_mse: 0.5865\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4878 - mse: 0.4878 - val_loss: 0.5865 - val_mse: 0.5865\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4878 - mse: 0.4878 - val_loss: 0.5865 - val_mse: 0.5865\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4877 - mse: 0.4877 - val_loss: 0.5865 - val_mse: 0.5865\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4880 - mse: 0.4880 - val_loss: 0.5865 - val_mse: 0.5865\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4879 - mse: 0.4879 - val_loss: 0.5865 - val_mse: 0.5865\n",
      "Epoch 18/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4879 - mse: 0.4879 - val_loss: 0.5866 - val_mse: 0.5866\n",
      "Epoch 19/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4877 - mse: 0.4877 - val_loss: 0.5867 - val_mse: 0.5867\n",
      "Epoch 20/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4878 - mse: 0.4878 - val_loss: 0.5867 - val_mse: 0.5867\n",
      "Epoch 21/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4879 - mse: 0.4879 - val_loss: 0.5868 - val_mse: 0.5868\n",
      "Epoch 22/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4878 - mse: 0.4878 - val_loss: 0.5870 - val_mse: 0.5870\n",
      "Epoch 23/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4878 - mse: 0.4878 - val_loss: 0.5871 - val_mse: 0.5871\n",
      "Epoch 24/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4880 - mse: 0.4880 - val_loss: 0.5870 - val_mse: 0.5870\n",
      "Epoch 25/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4881 - mse: 0.4881 - val_loss: 0.5869 - val_mse: 0.5869\n",
      "Epoch 26/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4878 - mse: 0.4878 - val_loss: 0.5869 - val_mse: 0.5869\n",
      "Epoch 27/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4876 - mse: 0.4876 - val_loss: 0.5871 - val_mse: 0.5871\n",
      "Epoch 28/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4880 - mse: 0.4880 - val_loss: 0.5873 - val_mse: 0.5873\n",
      "Epoch 29/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4878 - mse: 0.4878 - val_loss: 0.5872 - val_mse: 0.5872\n",
      "Epoch 30/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4878 - mse: 0.4878 - val_loss: 0.5872 - val_mse: 0.5872\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5872 - mse: 0.5872\n",
      "\n",
      "Weight: [array([[2.0310981]], dtype=float32), array([0.9758497], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8208 - mse: 0.8208 - val_loss: 0.8418 - val_mse: 0.8418\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8197 - mse: 0.8197 - val_loss: 0.8379 - val_mse: 0.8379\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8192 - mse: 0.8192 - val_loss: 0.8364 - val_mse: 0.8364\n",
      "Epoch 4/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8190 - mse: 0.8190 - val_loss: 0.8361 - val_mse: 0.8361\n",
      "Epoch 5/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8185 - mse: 0.8185 - val_loss: 0.8362 - val_mse: 0.8362\n",
      "Epoch 6/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8183 - mse: 0.8183 - val_loss: 0.8370 - val_mse: 0.8370\n",
      "Epoch 7/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8182 - mse: 0.8182 - val_loss: 0.8351 - val_mse: 0.8351\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8181 - mse: 0.8181 - val_loss: 0.8368 - val_mse: 0.8368\n",
      "Epoch 9/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8184 - mse: 0.8184 - val_loss: 0.8354 - val_mse: 0.8354\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8183 - mse: 0.8183 - val_loss: 0.8353 - val_mse: 0.8353\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8183 - mse: 0.8183 - val_loss: 0.8365 - val_mse: 0.8365\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8182 - mse: 0.8182 - val_loss: 0.8364 - val_mse: 0.8364\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8183 - mse: 0.8183 - val_loss: 0.8346 - val_mse: 0.8346\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8180 - mse: 0.8180 - val_loss: 0.8370 - val_mse: 0.8370\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8181 - mse: 0.8181 - val_loss: 0.8360 - val_mse: 0.8360\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8182 - mse: 0.8182 - val_loss: 0.8361 - val_mse: 0.8361\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8182 - mse: 0.8182 - val_loss: 0.8365 - val_mse: 0.8365\n",
      "Epoch 18/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8180 - mse: 0.8180 - val_loss: 0.8375 - val_mse: 0.8375\n",
      "Epoch 19/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8182 - mse: 0.8182 - val_loss: 0.8380 - val_mse: 0.8380\n",
      "Epoch 20/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8184 - mse: 0.8184 - val_loss: 0.8375 - val_mse: 0.8375\n",
      "Epoch 21/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8181 - mse: 0.8181 - val_loss: 0.8366 - val_mse: 0.8366\n",
      "Epoch 22/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8180 - mse: 0.8180 - val_loss: 0.8365 - val_mse: 0.8365\n",
      "Epoch 23/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8184 - mse: 0.8184 - val_loss: 0.8361 - val_mse: 0.8361\n",
      "Epoch 24/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8180 - mse: 0.8180 - val_loss: 0.8367 - val_mse: 0.8367\n",
      "Epoch 25/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8181 - mse: 0.8181 - val_loss: 0.8369 - val_mse: 0.8369\n",
      "Epoch 26/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8179 - mse: 0.8179 - val_loss: 0.8375 - val_mse: 0.8375\n",
      "Epoch 27/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8180 - mse: 0.8180 - val_loss: 0.8392 - val_mse: 0.8392\n",
      "Epoch 28/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8180 - mse: 0.8180 - val_loss: 0.8373 - val_mse: 0.8373\n",
      "Epoch 29/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8182 - mse: 0.8182 - val_loss: 0.8365 - val_mse: 0.8365\n",
      "Epoch 30/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8181 - mse: 0.8181 - val_loss: 0.8381 - val_mse: 0.8381\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.8381 - mse: 0.8381\n",
      "\n",
      "Weight: [array([[1.941011]], dtype=float32), array([0.99411434], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/30\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.2452 - mse: 1.2452 - val_loss: 1.1032 - val_mse: 1.1032\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2449 - mse: 1.2449 - val_loss: 1.1012 - val_mse: 1.1012\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2456 - mse: 1.2456 - val_loss: 1.1008 - val_mse: 1.1008\n",
      "Epoch 4/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2455 - mse: 1.2455 - val_loss: 1.0997 - val_mse: 1.0997\n",
      "Epoch 5/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2453 - mse: 1.2453 - val_loss: 1.0997 - val_mse: 1.0997\n",
      "Epoch 6/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2452 - mse: 1.2452 - val_loss: 1.0994 - val_mse: 1.0994\n",
      "Epoch 7/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2455 - mse: 1.2455 - val_loss: 1.1009 - val_mse: 1.1009\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2452 - mse: 1.2452 - val_loss: 1.1012 - val_mse: 1.1012\n",
      "Epoch 9/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2449 - mse: 1.2449 - val_loss: 1.1004 - val_mse: 1.1004\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2453 - mse: 1.2453 - val_loss: 1.1002 - val_mse: 1.1002\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2450 - mse: 1.2450 - val_loss: 1.0994 - val_mse: 1.0994\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2451 - mse: 1.2451 - val_loss: 1.0997 - val_mse: 1.0997\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2451 - mse: 1.2451 - val_loss: 1.0989 - val_mse: 1.0989\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2449 - mse: 1.2449 - val_loss: 1.0982 - val_mse: 1.0982\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2451 - mse: 1.2451 - val_loss: 1.0986 - val_mse: 1.0986\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2449 - mse: 1.2449 - val_loss: 1.0992 - val_mse: 1.0992\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2453 - mse: 1.2453 - val_loss: 1.0990 - val_mse: 1.0990\n",
      "Epoch 18/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2451 - mse: 1.2451 - val_loss: 1.0996 - val_mse: 1.0996\n",
      "Epoch 19/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2452 - mse: 1.2452 - val_loss: 1.0995 - val_mse: 1.0995\n",
      "Epoch 20/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2451 - mse: 1.2451 - val_loss: 1.1003 - val_mse: 1.1003\n",
      "Epoch 21/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2449 - mse: 1.2449 - val_loss: 1.1002 - val_mse: 1.1002\n",
      "Epoch 22/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2449 - mse: 1.2449 - val_loss: 1.1001 - val_mse: 1.1001\n",
      "Epoch 23/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2447 - mse: 1.2447 - val_loss: 1.0996 - val_mse: 1.0996\n",
      "Epoch 24/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2449 - mse: 1.2449 - val_loss: 1.0996 - val_mse: 1.0996\n",
      "Epoch 25/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2452 - mse: 1.2452 - val_loss: 1.1002 - val_mse: 1.1002\n",
      "Epoch 26/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2451 - mse: 1.2451 - val_loss: 1.0992 - val_mse: 1.0992\n",
      "Epoch 27/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2449 - mse: 1.2449 - val_loss: 1.0992 - val_mse: 1.0992\n",
      "Epoch 28/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2449 - mse: 1.2449 - val_loss: 1.0997 - val_mse: 1.0997\n",
      "Epoch 29/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2450 - mse: 1.2450 - val_loss: 1.0995 - val_mse: 1.0995\n",
      "Epoch 30/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.2448 - mse: 1.2448 - val_loss: 1.1002 - val_mse: 1.1002\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1002 - mse: 1.1002\n",
      "\n",
      "Weight: [array([[1.9511006]], dtype=float32), array([0.99987125], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/30\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 2/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 3/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 4/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 5/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 6/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 7/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 8/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 9/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 10/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 11/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 12/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 13/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 14/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 15/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 16/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 17/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 18/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 19/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 20/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 21/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 22/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 23/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 24/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 25/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 26/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 27/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 28/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 29/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 30/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0097 - mse: 0.0097\n",
      "\n",
      "Weight: [array([[2.00366]], dtype=float32), array([0.9989114], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.1063 - val_mse: 0.1063\n",
      "Epoch 2/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1064 - val_mse: 0.1064\n",
      "Epoch 3/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1062 - val_mse: 0.1062\n",
      "Epoch 4/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1061 - val_mse: 0.1061\n",
      "Epoch 5/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1062 - val_mse: 0.1062\n",
      "Epoch 6/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1062 - val_mse: 0.1062\n",
      "Epoch 7/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1056 - val_mse: 0.1056\n",
      "Epoch 8/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1048 - val_mse: 0.1048\n",
      "Epoch 9/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1053 - val_mse: 0.1053\n",
      "Epoch 10/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1064 - val_mse: 0.1064\n",
      "Epoch 11/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1073 - val_mse: 0.1073\n",
      "Epoch 12/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1065 - val_mse: 0.1065\n",
      "Epoch 13/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1058 - val_mse: 0.1058\n",
      "Epoch 14/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1068 - val_mse: 0.1068\n",
      "Epoch 15/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1076 - val_mse: 0.1076\n",
      "Epoch 16/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.1069 - val_mse: 0.1069\n",
      "Epoch 17/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1064 - val_mse: 0.1064\n",
      "Epoch 18/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1077 - val_mse: 0.1077\n",
      "Epoch 19/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.1072 - val_mse: 0.1072\n",
      "Epoch 20/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1076 - val_mse: 0.1076\n",
      "Epoch 21/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.1075 - val_mse: 0.1075\n",
      "Epoch 22/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.1060 - val_mse: 0.1060\n",
      "Epoch 23/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1056 - val_mse: 0.1056\n",
      "Epoch 24/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1060 - val_mse: 0.1060\n",
      "Epoch 25/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1059 - val_mse: 0.1059\n",
      "Epoch 26/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1063 - val_mse: 0.1063\n",
      "Epoch 27/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0887 - mse: 0.0887 - val_loss: 0.1064 - val_mse: 0.1064\n",
      "Epoch 28/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1053 - val_mse: 0.1053\n",
      "Epoch 29/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1056 - val_mse: 0.1056\n",
      "Epoch 30/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1051 - val_mse: 0.1051\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.1051 - mse: 0.1051\n",
      "\n",
      "Weight: [array([[1.9998668]], dtype=float32), array([1.007305], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/30\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2546 - mse: 0.2546 - val_loss: 0.2701 - val_mse: 0.2701\n",
      "Epoch 2/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2546 - mse: 0.2546 - val_loss: 0.2700 - val_mse: 0.2700\n",
      "Epoch 3/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 4/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2544 - mse: 0.2544 - val_loss: 0.2725 - val_mse: 0.2725\n",
      "Epoch 5/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2733 - val_mse: 0.2733\n",
      "Epoch 6/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2544 - mse: 0.2544 - val_loss: 0.2721 - val_mse: 0.2721\n",
      "Epoch 7/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2719 - val_mse: 0.2719\n",
      "Epoch 8/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2722 - val_mse: 0.2722\n",
      "Epoch 9/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2722 - val_mse: 0.2722\n",
      "Epoch 10/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2719 - val_mse: 0.2719\n",
      "Epoch 11/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2711 - val_mse: 0.2711\n",
      "Epoch 12/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2544 - mse: 0.2544 - val_loss: 0.2725 - val_mse: 0.2725\n",
      "Epoch 13/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2722 - val_mse: 0.2722\n",
      "Epoch 14/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2713 - val_mse: 0.2713\n",
      "Epoch 15/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2719 - val_mse: 0.2719\n",
      "Epoch 16/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2544 - mse: 0.2544 - val_loss: 0.2704 - val_mse: 0.2704\n",
      "Epoch 17/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2728 - val_mse: 0.2728\n",
      "Epoch 18/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2727 - val_mse: 0.2727\n",
      "Epoch 19/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2727 - val_mse: 0.2727\n",
      "Epoch 20/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2544 - mse: 0.2544 - val_loss: 0.2731 - val_mse: 0.2731\n",
      "Epoch 21/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2544 - mse: 0.2544 - val_loss: 0.2716 - val_mse: 0.2716\n",
      "Epoch 22/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2544 - mse: 0.2544 - val_loss: 0.2726 - val_mse: 0.2726\n",
      "Epoch 23/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2544 - mse: 0.2544 - val_loss: 0.2721 - val_mse: 0.2721\n",
      "Epoch 24/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2725 - val_mse: 0.2725\n",
      "Epoch 25/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2716 - val_mse: 0.2716\n",
      "Epoch 26/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2711 - val_mse: 0.2711\n",
      "Epoch 27/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2730 - val_mse: 0.2730\n",
      "Epoch 28/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2730 - val_mse: 0.2730\n",
      "Epoch 29/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.2719 - val_mse: 0.2719\n",
      "Epoch 30/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2544 - mse: 0.2544 - val_loss: 0.2714 - val_mse: 0.2714\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2714 - mse: 0.2714\n",
      "\n",
      "Weight: [array([[2.0017736]], dtype=float32), array([1.0173857], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5074 - mse: 0.5074 - val_loss: 0.6317 - val_mse: 0.6317\n",
      "Epoch 2/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5073 - mse: 0.5073 - val_loss: 0.6310 - val_mse: 0.6310\n",
      "Epoch 3/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5069 - mse: 0.5069 - val_loss: 0.6309 - val_mse: 0.6309\n",
      "Epoch 4/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5067 - mse: 0.5067 - val_loss: 0.6310 - val_mse: 0.6310\n",
      "Epoch 5/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5067 - mse: 0.5067 - val_loss: 0.6315 - val_mse: 0.6315\n",
      "Epoch 6/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5064 - mse: 0.5064 - val_loss: 0.6315 - val_mse: 0.6315\n",
      "Epoch 7/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5065 - mse: 0.5065 - val_loss: 0.6319 - val_mse: 0.6319\n",
      "Epoch 8/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5065 - mse: 0.5065 - val_loss: 0.6319 - val_mse: 0.6319\n",
      "Epoch 9/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5066 - mse: 0.5066 - val_loss: 0.6318 - val_mse: 0.6318\n",
      "Epoch 10/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5064 - mse: 0.5064 - val_loss: 0.6328 - val_mse: 0.6328\n",
      "Epoch 11/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5067 - mse: 0.5067 - val_loss: 0.6321 - val_mse: 0.6321\n",
      "Epoch 12/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5065 - mse: 0.5065 - val_loss: 0.6330 - val_mse: 0.6330\n",
      "Epoch 13/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5067 - mse: 0.5067 - val_loss: 0.6317 - val_mse: 0.6317\n",
      "Epoch 14/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5065 - mse: 0.5065 - val_loss: 0.6316 - val_mse: 0.6316\n",
      "Epoch 15/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5064 - mse: 0.5064 - val_loss: 0.6316 - val_mse: 0.6316\n",
      "Epoch 16/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5065 - mse: 0.5065 - val_loss: 0.6316 - val_mse: 0.6316\n",
      "Epoch 17/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5066 - mse: 0.5066 - val_loss: 0.6314 - val_mse: 0.6314\n",
      "Epoch 18/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5064 - mse: 0.5064 - val_loss: 0.6314 - val_mse: 0.6314\n",
      "Epoch 19/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5065 - mse: 0.5065 - val_loss: 0.6317 - val_mse: 0.6317\n",
      "Epoch 20/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5066 - mse: 0.5066 - val_loss: 0.6317 - val_mse: 0.6317\n",
      "Epoch 21/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5065 - mse: 0.5065 - val_loss: 0.6316 - val_mse: 0.6316\n",
      "Epoch 22/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5065 - mse: 0.5065 - val_loss: 0.6318 - val_mse: 0.6318\n",
      "Epoch 23/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5065 - mse: 0.5065 - val_loss: 0.6318 - val_mse: 0.6318\n",
      "Epoch 24/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5064 - mse: 0.5064 - val_loss: 0.6316 - val_mse: 0.6316\n",
      "Epoch 25/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5067 - mse: 0.5067 - val_loss: 0.6318 - val_mse: 0.6318\n",
      "Epoch 26/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5066 - mse: 0.5066 - val_loss: 0.6316 - val_mse: 0.6316\n",
      "Epoch 27/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5067 - mse: 0.5067 - val_loss: 0.6318 - val_mse: 0.6318\n",
      "Epoch 28/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5064 - mse: 0.5064 - val_loss: 0.6318 - val_mse: 0.6318\n",
      "Epoch 29/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5064 - mse: 0.5064 - val_loss: 0.6319 - val_mse: 0.6319\n",
      "Epoch 30/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5065 - mse: 0.5065 - val_loss: 0.6319 - val_mse: 0.6319\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.6319 - mse: 0.6319\n",
      "\n",
      "Weight: [array([[2.067541]], dtype=float32), array([1.0084115], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/30\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.8304 - mse: 0.8304 - val_loss: 0.7193 - val_mse: 0.7193\n",
      "Epoch 2/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8302 - mse: 0.8302 - val_loss: 0.7158 - val_mse: 0.7158\n",
      "Epoch 3/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8306 - mse: 0.8306 - val_loss: 0.7149 - val_mse: 0.7149\n",
      "Epoch 4/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8309 - mse: 0.8309 - val_loss: 0.7123 - val_mse: 0.7123\n",
      "Epoch 5/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8308 - mse: 0.8308 - val_loss: 0.7117 - val_mse: 0.7117\n",
      "Epoch 6/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8304 - mse: 0.8304 - val_loss: 0.7133 - val_mse: 0.7133\n",
      "Epoch 7/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8302 - mse: 0.8302 - val_loss: 0.7155 - val_mse: 0.7155\n",
      "Epoch 8/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8300 - mse: 0.8300 - val_loss: 0.7167 - val_mse: 0.7167\n",
      "Epoch 9/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8300 - mse: 0.8300 - val_loss: 0.7168 - val_mse: 0.7168\n",
      "Epoch 10/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8306 - mse: 0.8306 - val_loss: 0.7190 - val_mse: 0.7190\n",
      "Epoch 11/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8301 - mse: 0.8301 - val_loss: 0.7223 - val_mse: 0.7223\n",
      "Epoch 12/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8299 - mse: 0.8299 - val_loss: 0.7226 - val_mse: 0.7226\n",
      "Epoch 13/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8301 - mse: 0.8301 - val_loss: 0.7200 - val_mse: 0.7200\n",
      "Epoch 14/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8302 - mse: 0.8302 - val_loss: 0.7185 - val_mse: 0.7185\n",
      "Epoch 15/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8303 - mse: 0.8303 - val_loss: 0.7180 - val_mse: 0.7180\n",
      "Epoch 16/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8300 - mse: 0.8300 - val_loss: 0.7162 - val_mse: 0.7162\n",
      "Epoch 17/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8301 - mse: 0.8301 - val_loss: 0.7186 - val_mse: 0.7186\n",
      "Epoch 18/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8301 - mse: 0.8301 - val_loss: 0.7159 - val_mse: 0.7159\n",
      "Epoch 19/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8303 - mse: 0.8303 - val_loss: 0.7180 - val_mse: 0.7180\n",
      "Epoch 20/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8305 - mse: 0.8305 - val_loss: 0.7195 - val_mse: 0.7195\n",
      "Epoch 21/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8301 - mse: 0.8301 - val_loss: 0.7217 - val_mse: 0.7217\n",
      "Epoch 22/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8302 - mse: 0.8302 - val_loss: 0.7215 - val_mse: 0.7215\n",
      "Epoch 23/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8300 - mse: 0.8300 - val_loss: 0.7197 - val_mse: 0.7197\n",
      "Epoch 24/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8299 - mse: 0.8299 - val_loss: 0.7200 - val_mse: 0.7200\n",
      "Epoch 25/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8300 - mse: 0.8300 - val_loss: 0.7170 - val_mse: 0.7170\n",
      "Epoch 26/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8300 - mse: 0.8300 - val_loss: 0.7193 - val_mse: 0.7193\n",
      "Epoch 27/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8302 - mse: 0.8302 - val_loss: 0.7212 - val_mse: 0.7212\n",
      "Epoch 28/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8299 - mse: 0.8299 - val_loss: 0.7223 - val_mse: 0.7223\n",
      "Epoch 29/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8300 - mse: 0.8300 - val_loss: 0.7235 - val_mse: 0.7235\n",
      "Epoch 30/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8301 - mse: 0.8301 - val_loss: 0.7185 - val_mse: 0.7185\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.7185 - mse: 0.7185\n",
      "\n",
      "Weight: [array([[2.0807984]], dtype=float32), array([0.9871074], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/30\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 1.1975 - mse: 1.1975 - val_loss: 1.6569 - val_mse: 1.6569\n",
      "Epoch 2/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1963 - mse: 1.1963 - val_loss: 1.6576 - val_mse: 1.6576\n",
      "Epoch 3/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1947 - mse: 1.1947 - val_loss: 1.6533 - val_mse: 1.6533\n",
      "Epoch 4/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1941 - mse: 1.1941 - val_loss: 1.6533 - val_mse: 1.6533\n",
      "Epoch 5/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1939 - mse: 1.1939 - val_loss: 1.6526 - val_mse: 1.6526\n",
      "Epoch 6/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1938 - mse: 1.1938 - val_loss: 1.6544 - val_mse: 1.6544\n",
      "Epoch 7/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1934 - mse: 1.1934 - val_loss: 1.6583 - val_mse: 1.6583\n",
      "Epoch 8/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1930 - mse: 1.1930 - val_loss: 1.6579 - val_mse: 1.6579\n",
      "Epoch 9/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1930 - mse: 1.1930 - val_loss: 1.6632 - val_mse: 1.6632\n",
      "Epoch 10/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1936 - mse: 1.1936 - val_loss: 1.6563 - val_mse: 1.6563\n",
      "Epoch 11/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1925 - mse: 1.1925 - val_loss: 1.6573 - val_mse: 1.6573\n",
      "Epoch 12/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1924 - mse: 1.1924 - val_loss: 1.6567 - val_mse: 1.6567\n",
      "Epoch 13/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1925 - mse: 1.1925 - val_loss: 1.6547 - val_mse: 1.6547\n",
      "Epoch 14/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1927 - mse: 1.1927 - val_loss: 1.6606 - val_mse: 1.6606\n",
      "Epoch 15/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1926 - mse: 1.1926 - val_loss: 1.6550 - val_mse: 1.6550\n",
      "Epoch 16/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1928 - mse: 1.1928 - val_loss: 1.6581 - val_mse: 1.6581\n",
      "Epoch 17/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1925 - mse: 1.1925 - val_loss: 1.6594 - val_mse: 1.6594\n",
      "Epoch 18/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1925 - mse: 1.1925 - val_loss: 1.6618 - val_mse: 1.6618\n",
      "Epoch 19/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1928 - mse: 1.1928 - val_loss: 1.6607 - val_mse: 1.6607\n",
      "Epoch 20/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1925 - mse: 1.1925 - val_loss: 1.6607 - val_mse: 1.6607\n",
      "Epoch 21/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1927 - mse: 1.1927 - val_loss: 1.6582 - val_mse: 1.6582\n",
      "Epoch 22/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1928 - mse: 1.1928 - val_loss: 1.6611 - val_mse: 1.6611\n",
      "Epoch 23/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1925 - mse: 1.1925 - val_loss: 1.6597 - val_mse: 1.6597\n",
      "Epoch 24/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1925 - mse: 1.1925 - val_loss: 1.6601 - val_mse: 1.6601\n",
      "Epoch 25/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1926 - mse: 1.1926 - val_loss: 1.6588 - val_mse: 1.6588\n",
      "Epoch 26/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1927 - mse: 1.1927 - val_loss: 1.6614 - val_mse: 1.6614\n",
      "Epoch 27/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1927 - mse: 1.1927 - val_loss: 1.6622 - val_mse: 1.6622\n",
      "Epoch 28/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1927 - mse: 1.1927 - val_loss: 1.6615 - val_mse: 1.6615\n",
      "Epoch 29/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1928 - mse: 1.1928 - val_loss: 1.6597 - val_mse: 1.6597\n",
      "Epoch 30/30\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1926 - mse: 1.1926 - val_loss: 1.6605 - val_mse: 1.6605\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.6605 - mse: 1.6605\n",
      "\n",
      "Weight: [array([[1.9478495]], dtype=float32), array([1.024982], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0125 - val_mse: 0.0125\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0112 - mse: 0.0112\n",
      "\n",
      "Weight: [array([[1.9865466]], dtype=float32), array([0.9958805], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/30\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0799 - mse: 0.0799 - val_loss: 0.0990 - val_mse: 0.0990\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.0993 - val_mse: 0.0993\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.0994 - val_mse: 0.0994\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.0997 - val_mse: 0.0997\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.0997 - val_mse: 0.0997\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0797 - mse: 0.0797 - val_loss: 0.0995 - val_mse: 0.0995\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.0997 - val_mse: 0.0997\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.0996 - val_mse: 0.0996\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.0997 - val_mse: 0.0997\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.0998 - val_mse: 0.0998\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.0999 - val_mse: 0.0999\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.0999 - val_mse: 0.0999\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.1000 - val_mse: 0.1000\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.1001 - val_mse: 0.1001\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.1002 - val_mse: 0.1002\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.1001 - val_mse: 0.1001\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.1003 - val_mse: 0.1003\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.1003 - val_mse: 0.1003\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.1001 - val_mse: 0.1001\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.1001 - val_mse: 0.1001\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.1003 - val_mse: 0.1003\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.1003 - val_mse: 0.1003\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.1001 - val_mse: 0.1001\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.1002 - val_mse: 0.1002\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.1000 - val_mse: 0.1000\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.0999 - val_mse: 0.0999\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.1000 - val_mse: 0.1000\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.0999 - val_mse: 0.0999\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.1000 - val_mse: 0.1000\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.1002 - val_mse: 0.1002\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.1002 - mse: 0.1002\n",
      "\n",
      "Weight: [array([[1.9700544]], dtype=float32), array([1.0121216], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2327 - mse: 0.2327 - val_loss: 0.2266 - val_mse: 0.2266\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2315 - mse: 0.2315 - val_loss: 0.2276 - val_mse: 0.2276\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2308 - mse: 0.2308 - val_loss: 0.2284 - val_mse: 0.2284\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2304 - mse: 0.2304 - val_loss: 0.2289 - val_mse: 0.2289\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2302 - mse: 0.2302 - val_loss: 0.2290 - val_mse: 0.2290\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2300 - mse: 0.2300 - val_loss: 0.2299 - val_mse: 0.2299\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2298 - mse: 0.2298 - val_loss: 0.2299 - val_mse: 0.2299\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2298 - mse: 0.2298 - val_loss: 0.2307 - val_mse: 0.2307\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2297 - mse: 0.2297 - val_loss: 0.2310 - val_mse: 0.2310\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2297 - mse: 0.2297 - val_loss: 0.2313 - val_mse: 0.2313\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2297 - mse: 0.2297 - val_loss: 0.2318 - val_mse: 0.2318\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2297 - mse: 0.2297 - val_loss: 0.2314 - val_mse: 0.2314\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2297 - mse: 0.2297 - val_loss: 0.2316 - val_mse: 0.2316\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2296 - mse: 0.2296 - val_loss: 0.2316 - val_mse: 0.2316\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2296 - mse: 0.2296 - val_loss: 0.2316 - val_mse: 0.2316\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2296 - mse: 0.2296 - val_loss: 0.2319 - val_mse: 0.2319\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2296 - mse: 0.2296 - val_loss: 0.2317 - val_mse: 0.2317\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2296 - mse: 0.2296 - val_loss: 0.2317 - val_mse: 0.2317\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2297 - mse: 0.2297 - val_loss: 0.2321 - val_mse: 0.2321\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2296 - mse: 0.2296 - val_loss: 0.2325 - val_mse: 0.2325\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2296 - mse: 0.2296 - val_loss: 0.2328 - val_mse: 0.2328\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2296 - mse: 0.2296 - val_loss: 0.2321 - val_mse: 0.2321\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2296 - mse: 0.2296 - val_loss: 0.2320 - val_mse: 0.2320\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2296 - mse: 0.2296 - val_loss: 0.2319 - val_mse: 0.2319\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2297 - mse: 0.2297 - val_loss: 0.2313 - val_mse: 0.2313\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2296 - mse: 0.2296 - val_loss: 0.2315 - val_mse: 0.2315\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2296 - mse: 0.2296 - val_loss: 0.2316 - val_mse: 0.2316\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2297 - mse: 0.2297 - val_loss: 0.2320 - val_mse: 0.2320\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2297 - mse: 0.2297 - val_loss: 0.2325 - val_mse: 0.2325\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2297 - mse: 0.2297 - val_loss: 0.2321 - val_mse: 0.2321\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2321 - mse: 0.2321\n",
      "\n",
      "Weight: [array([[2.0589468]], dtype=float32), array([0.9682184], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/30\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4983 - mse: 0.4983 - val_loss: 0.4146 - val_mse: 0.4146\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4969 - mse: 0.4969 - val_loss: 0.4107 - val_mse: 0.4107\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4966 - mse: 0.4966 - val_loss: 0.4070 - val_mse: 0.4070\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4963 - mse: 0.4963 - val_loss: 0.4073 - val_mse: 0.4073\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4963 - mse: 0.4963 - val_loss: 0.4041 - val_mse: 0.4041\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4962 - mse: 0.4962 - val_loss: 0.4024 - val_mse: 0.4024\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4964 - mse: 0.4964 - val_loss: 0.4042 - val_mse: 0.4042\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4964 - mse: 0.4964 - val_loss: 0.4025 - val_mse: 0.4025\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4963 - mse: 0.4963 - val_loss: 0.4073 - val_mse: 0.4073\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4962 - mse: 0.4962 - val_loss: 0.4065 - val_mse: 0.4065\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4962 - mse: 0.4962 - val_loss: 0.4081 - val_mse: 0.4081\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4964 - mse: 0.4964 - val_loss: 0.4074 - val_mse: 0.4074\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4964 - mse: 0.4964 - val_loss: 0.4092 - val_mse: 0.4092\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4964 - mse: 0.4964 - val_loss: 0.4069 - val_mse: 0.4069\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.4963 - mse: 0.4963 - val_loss: 0.4070 - val_mse: 0.4070\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4963 - mse: 0.4963 - val_loss: 0.4069 - val_mse: 0.4069\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4962 - mse: 0.4962 - val_loss: 0.4080 - val_mse: 0.4080\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4963 - mse: 0.4963 - val_loss: 0.4088 - val_mse: 0.4088\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4963 - mse: 0.4963 - val_loss: 0.4066 - val_mse: 0.4066\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4963 - mse: 0.4963 - val_loss: 0.4074 - val_mse: 0.4074\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4961 - mse: 0.4961 - val_loss: 0.4075 - val_mse: 0.4075\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4962 - mse: 0.4962 - val_loss: 0.4072 - val_mse: 0.4072\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4963 - mse: 0.4963 - val_loss: 0.4058 - val_mse: 0.4058\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4962 - mse: 0.4962 - val_loss: 0.4026 - val_mse: 0.4026\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4964 - mse: 0.4964 - val_loss: 0.4027 - val_mse: 0.4027\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4964 - mse: 0.4964 - val_loss: 0.4045 - val_mse: 0.4045\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4962 - mse: 0.4962 - val_loss: 0.4049 - val_mse: 0.4049\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4963 - mse: 0.4963 - val_loss: 0.4059 - val_mse: 0.4059\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4962 - mse: 0.4962 - val_loss: 0.4035 - val_mse: 0.4035\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4963 - mse: 0.4963 - val_loss: 0.4022 - val_mse: 0.4022\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4022 - mse: 0.4022\n",
      "\n",
      "Weight: [array([[2.0117688]], dtype=float32), array([1.0307133], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/30\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.7986 - val_mse: 0.7986\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.7960 - val_mse: 0.7960\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.7962 - val_mse: 0.7962\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.7945 - val_mse: 0.7945\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7554 - mse: 0.7554 - val_loss: 0.7939 - val_mse: 0.7939\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7556 - mse: 0.7556 - val_loss: 0.7945 - val_mse: 0.7945\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.7922 - val_mse: 0.7922\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.7921 - val_mse: 0.7921\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7557 - mse: 0.7557 - val_loss: 0.7918 - val_mse: 0.7918\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.7910 - val_mse: 0.7910\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.7911 - val_mse: 0.7911\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7556 - mse: 0.7556 - val_loss: 0.7929 - val_mse: 0.7929\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7556 - mse: 0.7556 - val_loss: 0.7931 - val_mse: 0.7931\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.7910 - val_mse: 0.7910\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.7922 - val_mse: 0.7922\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.7929 - val_mse: 0.7929\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7554 - mse: 0.7554 - val_loss: 0.7935 - val_mse: 0.7935\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7554 - mse: 0.7554 - val_loss: 0.7934 - val_mse: 0.7934\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7554 - mse: 0.7554 - val_loss: 0.7943 - val_mse: 0.7943\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.7990 - val_mse: 0.7990\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7556 - mse: 0.7556 - val_loss: 0.7980 - val_mse: 0.7980\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7558 - mse: 0.7558 - val_loss: 0.7964 - val_mse: 0.7964\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.7943 - val_mse: 0.7943\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.7951 - val_mse: 0.7951\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7553 - mse: 0.7553 - val_loss: 0.7953 - val_mse: 0.7953\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7553 - mse: 0.7553 - val_loss: 0.7957 - val_mse: 0.7957\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7556 - mse: 0.7556 - val_loss: 0.7938 - val_mse: 0.7938\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7557 - mse: 0.7557 - val_loss: 0.7930 - val_mse: 0.7930\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7554 - mse: 0.7554 - val_loss: 0.7931 - val_mse: 0.7931\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7553 - mse: 0.7553 - val_loss: 0.7938 - val_mse: 0.7938\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.7938 - mse: 0.7938\n",
      "\n",
      "Weight: [array([[1.9937204]], dtype=float32), array([1.0420382], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/30\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.2436 - mse: 1.2436 - val_loss: 1.4919 - val_mse: 1.4919\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2424 - mse: 1.2424 - val_loss: 1.4886 - val_mse: 1.4886\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2422 - mse: 1.2422 - val_loss: 1.4868 - val_mse: 1.4868\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2421 - mse: 1.2421 - val_loss: 1.4879 - val_mse: 1.4879\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2421 - mse: 1.2421 - val_loss: 1.4872 - val_mse: 1.4872\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2422 - mse: 1.2422 - val_loss: 1.4867 - val_mse: 1.4867\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2420 - mse: 1.2420 - val_loss: 1.4851 - val_mse: 1.4851\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2421 - mse: 1.2421 - val_loss: 1.4870 - val_mse: 1.4870\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2420 - mse: 1.2420 - val_loss: 1.4848 - val_mse: 1.4848\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2422 - mse: 1.2422 - val_loss: 1.4838 - val_mse: 1.4838\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2421 - mse: 1.2421 - val_loss: 1.4824 - val_mse: 1.4824\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2419 - mse: 1.2419 - val_loss: 1.4840 - val_mse: 1.4840\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2422 - mse: 1.2422 - val_loss: 1.4821 - val_mse: 1.4821\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2422 - mse: 1.2422 - val_loss: 1.4843 - val_mse: 1.4843\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2422 - mse: 1.2422 - val_loss: 1.4899 - val_mse: 1.4899\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2423 - mse: 1.2423 - val_loss: 1.4894 - val_mse: 1.4894\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2422 - mse: 1.2422 - val_loss: 1.4924 - val_mse: 1.4924\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2422 - mse: 1.2422 - val_loss: 1.4907 - val_mse: 1.4907\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2423 - mse: 1.2423 - val_loss: 1.4905 - val_mse: 1.4905\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2421 - mse: 1.2421 - val_loss: 1.4899 - val_mse: 1.4899\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2421 - mse: 1.2421 - val_loss: 1.4915 - val_mse: 1.4915\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2425 - mse: 1.2425 - val_loss: 1.4872 - val_mse: 1.4872\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2416 - mse: 1.2416 - val_loss: 1.4833 - val_mse: 1.4833\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2426 - mse: 1.2426 - val_loss: 1.4857 - val_mse: 1.4857\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2418 - mse: 1.2418 - val_loss: 1.4879 - val_mse: 1.4879\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2421 - mse: 1.2421 - val_loss: 1.4865 - val_mse: 1.4865\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2416 - mse: 1.2416 - val_loss: 1.4929 - val_mse: 1.4929\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2421 - mse: 1.2421 - val_loss: 1.4899 - val_mse: 1.4899\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2424 - mse: 1.2424 - val_loss: 1.4892 - val_mse: 1.4892\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2419 - mse: 1.2419 - val_loss: 1.4856 - val_mse: 1.4856\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.4856 - mse: 1.4856\n",
      "\n",
      "Weight: [array([[1.987847]], dtype=float32), array([0.9816155], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "N_train = np.zeros(6)\n",
    "sigma = np.zeros(6)\n",
    "score30 = np.zeros((6,6,2))\n",
    "weight30 = np.zeros((6,6,2))\n",
    "\n",
    "history30 = []\n",
    "\n",
    "\n",
    "#GRID SEARCH\n",
    "for i in range(6):\n",
    "    N_train[i] = 500 + i*100\n",
    "    row = []\n",
    "    for j in range(6):\n",
    "        sigma[j] = 0.1 + j*0.2 # noise standard deviation\n",
    "        x_train = np.random.uniform(-1, 1, int(N_train[i]))\n",
    "    \n",
    "        y_train = np.random.normal(m * x_train + b, sigma[j]) # actual measures from which we want to guess regression parameters\n",
    "        y_valid = np.random.normal(m * x_valid + b, sigma[j])\n",
    "        # fit the model using training dataset\n",
    "        # over 30 epochs of 32 batch size each\n",
    "        # report training progress against validation data\n",
    "        print(\"\\n\\nN_train = \",N_train[i])\n",
    "        print(\"Sigma = \",sigma[j])\n",
    "        row.append(model.fit(x=x_train, y=y_train, \n",
    "              batch_size=32, epochs=30,\n",
    "              shuffle=True, # a good idea is to shuffle input before at each epoch\n",
    "              validation_data=(x_valid, y_valid)))\n",
    "        score30[i][j] = model.evaluate(x_valid, y_valid, batch_size=32, verbose=1)\n",
    "        pesi = model.get_weights()\n",
    "        weight30[i][j] = [pesi[0][0][0],pesi[1][0]]\n",
    "        print(\"\\nWeight:\",pesi)  \n",
    "    history30.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruisco una rete neurale con $N_{epochs}=40$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/40\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 2/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 3/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 4/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 5/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 6/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 7/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 8/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 9/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 10/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 11/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 12/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 13/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 14/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 15/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 16/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 17/40\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 18/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 19/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 20/40\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 21/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 22/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 23/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 24/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 25/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 26/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 27/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 28/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 29/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 30/40\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 31/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 32/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 33/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 34/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 35/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 36/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 37/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 38/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 39/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 40/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100\n",
      "\n",
      "Weight: [array([[1.9975387]], dtype=float32), array([1.0038711], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/40\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0808 - mse: 0.0808 - val_loss: 0.0656 - val_mse: 0.0656\n",
      "Epoch 2/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0808 - mse: 0.0808 - val_loss: 0.0653 - val_mse: 0.0653\n",
      "Epoch 3/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0808 - mse: 0.0808 - val_loss: 0.0653 - val_mse: 0.0653\n",
      "Epoch 4/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0807 - mse: 0.0807 - val_loss: 0.0652 - val_mse: 0.0652\n",
      "Epoch 5/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0807 - mse: 0.0807 - val_loss: 0.0651 - val_mse: 0.0651\n",
      "Epoch 6/40\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0807 - mse: 0.0807 - val_loss: 0.0651 - val_mse: 0.0651\n",
      "Epoch 7/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0807 - mse: 0.0807 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 8/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 9/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 10/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 11/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0651 - val_mse: 0.0651\n",
      "Epoch 12/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0651 - val_mse: 0.0651\n",
      "Epoch 13/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0651 - val_mse: 0.0651\n",
      "Epoch 14/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 15/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 16/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 17/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 18/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 19/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 20/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 21/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 22/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0651 - val_mse: 0.0651\n",
      "Epoch 23/40\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 24/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 25/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 26/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 27/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0651 - val_mse: 0.0651\n",
      "Epoch 28/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 29/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 30/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 31/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0651 - val_mse: 0.0651\n",
      "Epoch 32/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0651 - val_mse: 0.0651\n",
      "Epoch 33/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 34/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 35/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 36/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0649 - val_mse: 0.0649\n",
      "Epoch 37/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 38/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 39/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "Epoch 40/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0805 - mse: 0.0805 - val_loss: 0.0650 - val_mse: 0.0650\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0650 - mse: 0.0650\n",
      "\n",
      "Weight: [array([[1.969768]], dtype=float32), array([0.9960577], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/40\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2068 - val_mse: 0.2068\n",
      "Epoch 2/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2517 - mse: 0.2517 - val_loss: 0.2064 - val_mse: 0.2064\n",
      "Epoch 3/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2515 - mse: 0.2515 - val_loss: 0.2060 - val_mse: 0.2060\n",
      "Epoch 4/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2516 - mse: 0.2516 - val_loss: 0.2055 - val_mse: 0.2055\n",
      "Epoch 5/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2514 - mse: 0.2514 - val_loss: 0.2053 - val_mse: 0.2053\n",
      "Epoch 6/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2052 - val_mse: 0.2052\n",
      "Epoch 7/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2050 - val_mse: 0.2050\n",
      "Epoch 8/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2049 - val_mse: 0.2049\n",
      "Epoch 9/40\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2049 - val_mse: 0.2049\n",
      "Epoch 10/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2514 - mse: 0.2514 - val_loss: 0.2047 - val_mse: 0.2047\n",
      "Epoch 11/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2514 - mse: 0.2514 - val_loss: 0.2049 - val_mse: 0.2049\n",
      "Epoch 12/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2048 - val_mse: 0.2048\n",
      "Epoch 13/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2047 - val_mse: 0.2047\n",
      "Epoch 14/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2047 - val_mse: 0.2047\n",
      "Epoch 15/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2514 - mse: 0.2514 - val_loss: 0.2048 - val_mse: 0.2048\n",
      "Epoch 16/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2046 - val_mse: 0.2046\n",
      "Epoch 17/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2514 - mse: 0.2514 - val_loss: 0.2048 - val_mse: 0.2048\n",
      "Epoch 18/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2514 - mse: 0.2514 - val_loss: 0.2046 - val_mse: 0.2046\n",
      "Epoch 19/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2514 - mse: 0.2514 - val_loss: 0.2046 - val_mse: 0.2046\n",
      "Epoch 20/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2044 - val_mse: 0.2044\n",
      "Epoch 21/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 22/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 23/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 24/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2512 - mse: 0.2512 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 25/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2514 - mse: 0.2514 - val_loss: 0.2044 - val_mse: 0.2044\n",
      "Epoch 26/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2514 - mse: 0.2514 - val_loss: 0.2044 - val_mse: 0.2044\n",
      "Epoch 27/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2044 - val_mse: 0.2044\n",
      "Epoch 28/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2514 - mse: 0.2514 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 29/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2046 - val_mse: 0.2046\n",
      "Epoch 30/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 31/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2514 - mse: 0.2514 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 32/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2512 - mse: 0.2512 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 33/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 34/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2514 - mse: 0.2514 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 35/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 36/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2515 - mse: 0.2515 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 37/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2514 - mse: 0.2514 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 38/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2512 - mse: 0.2512 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 39/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2514 - mse: 0.2514 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 40/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2045 - mse: 0.2045\n",
      "\n",
      "Weight: [array([[1.9471786]], dtype=float32), array([1.0169148], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/40\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5231 - mse: 0.5231 - val_loss: 0.4769 - val_mse: 0.4769\n",
      "Epoch 2/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4773 - val_mse: 0.4773\n",
      "Epoch 3/40\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4778 - val_mse: 0.4778\n",
      "Epoch 4/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4781 - val_mse: 0.4781\n",
      "Epoch 5/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4782 - val_mse: 0.4782\n",
      "Epoch 6/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5230 - mse: 0.5230 - val_loss: 0.4780 - val_mse: 0.4780\n",
      "Epoch 7/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4777 - val_mse: 0.4777\n",
      "Epoch 8/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5227 - mse: 0.5227 - val_loss: 0.4778 - val_mse: 0.4778\n",
      "Epoch 9/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4778 - val_mse: 0.4778\n",
      "Epoch 10/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4780 - val_mse: 0.4780\n",
      "Epoch 11/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5229 - mse: 0.5229 - val_loss: 0.4782 - val_mse: 0.4782\n",
      "Epoch 12/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5229 - mse: 0.5229 - val_loss: 0.4781 - val_mse: 0.4781\n",
      "Epoch 13/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5229 - mse: 0.5229 - val_loss: 0.4779 - val_mse: 0.4779\n",
      "Epoch 14/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5230 - mse: 0.5230 - val_loss: 0.4782 - val_mse: 0.4782\n",
      "Epoch 15/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4783 - val_mse: 0.4783\n",
      "Epoch 16/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4781 - val_mse: 0.4781\n",
      "Epoch 17/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5227 - mse: 0.5227 - val_loss: 0.4781 - val_mse: 0.4781\n",
      "Epoch 18/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4781 - val_mse: 0.4781\n",
      "Epoch 19/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5227 - mse: 0.5227 - val_loss: 0.4783 - val_mse: 0.4783\n",
      "Epoch 20/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5229 - mse: 0.5229 - val_loss: 0.4781 - val_mse: 0.4781\n",
      "Epoch 21/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5227 - mse: 0.5227 - val_loss: 0.4780 - val_mse: 0.4780\n",
      "Epoch 22/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5230 - mse: 0.5230 - val_loss: 0.4778 - val_mse: 0.4778\n",
      "Epoch 23/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4780 - val_mse: 0.4780\n",
      "Epoch 24/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5229 - mse: 0.5229 - val_loss: 0.4781 - val_mse: 0.4781\n",
      "Epoch 25/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5227 - mse: 0.5227 - val_loss: 0.4781 - val_mse: 0.4781\n",
      "Epoch 26/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5229 - mse: 0.5229 - val_loss: 0.4779 - val_mse: 0.4779\n",
      "Epoch 27/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5230 - mse: 0.5230 - val_loss: 0.4780 - val_mse: 0.4780\n",
      "Epoch 28/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4780 - val_mse: 0.4780\n",
      "Epoch 29/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5227 - mse: 0.5227 - val_loss: 0.4780 - val_mse: 0.4780\n",
      "Epoch 30/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5229 - mse: 0.5229 - val_loss: 0.4780 - val_mse: 0.4780\n",
      "Epoch 31/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4779 - val_mse: 0.4779\n",
      "Epoch 32/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4778 - val_mse: 0.4778\n",
      "Epoch 33/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5230 - mse: 0.5230 - val_loss: 0.4778 - val_mse: 0.4778\n",
      "Epoch 34/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4779 - val_mse: 0.4779\n",
      "Epoch 35/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4780 - val_mse: 0.4780\n",
      "Epoch 36/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5229 - mse: 0.5229 - val_loss: 0.4780 - val_mse: 0.4780\n",
      "Epoch 37/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4781 - val_mse: 0.4781\n",
      "Epoch 38/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5230 - mse: 0.5230 - val_loss: 0.4779 - val_mse: 0.4779\n",
      "Epoch 39/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4781 - val_mse: 0.4781\n",
      "Epoch 40/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.4779 - val_mse: 0.4779\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4779 - mse: 0.4779\n",
      "\n",
      "Weight: [array([[1.9531478]], dtype=float32), array([1.0061829], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/40\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7860 - mse: 0.7860 - val_loss: 1.1936 - val_mse: 1.1936\n",
      "Epoch 2/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7845 - mse: 0.7845 - val_loss: 1.1938 - val_mse: 1.1938\n",
      "Epoch 3/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7836 - mse: 0.7836 - val_loss: 1.1938 - val_mse: 1.1938\n",
      "Epoch 4/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7828 - mse: 0.7828 - val_loss: 1.1941 - val_mse: 1.1941\n",
      "Epoch 5/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7827 - mse: 0.7827 - val_loss: 1.1945 - val_mse: 1.1945\n",
      "Epoch 6/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7823 - mse: 0.7823 - val_loss: 1.1948 - val_mse: 1.1948\n",
      "Epoch 7/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7824 - mse: 0.7824 - val_loss: 1.1951 - val_mse: 1.1951\n",
      "Epoch 8/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7818 - mse: 0.7818 - val_loss: 1.1955 - val_mse: 1.1955\n",
      "Epoch 9/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7818 - mse: 0.7818 - val_loss: 1.1959 - val_mse: 1.1959\n",
      "Epoch 10/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7816 - mse: 0.7816 - val_loss: 1.1962 - val_mse: 1.1962\n",
      "Epoch 11/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7815 - mse: 0.7815 - val_loss: 1.1965 - val_mse: 1.1965\n",
      "Epoch 12/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7814 - mse: 0.7814 - val_loss: 1.1968 - val_mse: 1.1968\n",
      "Epoch 13/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7813 - mse: 0.7813 - val_loss: 1.1970 - val_mse: 1.1970\n",
      "Epoch 14/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7813 - mse: 0.7813 - val_loss: 1.1972 - val_mse: 1.1972\n",
      "Epoch 15/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7814 - mse: 0.7814 - val_loss: 1.1973 - val_mse: 1.1973\n",
      "Epoch 16/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7814 - mse: 0.7814 - val_loss: 1.1975 - val_mse: 1.1975\n",
      "Epoch 17/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7812 - mse: 0.7812 - val_loss: 1.1976 - val_mse: 1.1976\n",
      "Epoch 18/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7811 - mse: 0.7811 - val_loss: 1.1977 - val_mse: 1.1977\n",
      "Epoch 19/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7811 - mse: 0.7811 - val_loss: 1.1977 - val_mse: 1.1977\n",
      "Epoch 20/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7811 - mse: 0.7811 - val_loss: 1.1978 - val_mse: 1.1978\n",
      "Epoch 21/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7810 - mse: 0.7810 - val_loss: 1.1979 - val_mse: 1.1979\n",
      "Epoch 22/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7815 - mse: 0.7815 - val_loss: 1.1980 - val_mse: 1.1980\n",
      "Epoch 23/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7812 - mse: 0.7812 - val_loss: 1.1981 - val_mse: 1.1981\n",
      "Epoch 24/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7811 - mse: 0.7811 - val_loss: 1.1982 - val_mse: 1.1982\n",
      "Epoch 25/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7812 - mse: 0.7812 - val_loss: 1.1984 - val_mse: 1.1984\n",
      "Epoch 26/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7818 - mse: 0.7818 - val_loss: 1.1985 - val_mse: 1.1985\n",
      "Epoch 27/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7809 - mse: 0.7809 - val_loss: 1.1983 - val_mse: 1.1983\n",
      "Epoch 28/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7811 - mse: 0.7811 - val_loss: 1.1984 - val_mse: 1.1984\n",
      "Epoch 29/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7809 - mse: 0.7809 - val_loss: 1.1985 - val_mse: 1.1985\n",
      "Epoch 30/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7811 - mse: 0.7811 - val_loss: 1.1986 - val_mse: 1.1986\n",
      "Epoch 31/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7813 - mse: 0.7813 - val_loss: 1.1986 - val_mse: 1.1986\n",
      "Epoch 32/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7812 - mse: 0.7812 - val_loss: 1.1985 - val_mse: 1.1985\n",
      "Epoch 33/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7809 - mse: 0.7809 - val_loss: 1.1986 - val_mse: 1.1986\n",
      "Epoch 34/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7810 - mse: 0.7810 - val_loss: 1.1986 - val_mse: 1.1986\n",
      "Epoch 35/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7811 - mse: 0.7811 - val_loss: 1.1986 - val_mse: 1.1986\n",
      "Epoch 36/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7810 - mse: 0.7810 - val_loss: 1.1987 - val_mse: 1.1987\n",
      "Epoch 37/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7809 - mse: 0.7809 - val_loss: 1.1985 - val_mse: 1.1985\n",
      "Epoch 38/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7810 - mse: 0.7810 - val_loss: 1.1987 - val_mse: 1.1987\n",
      "Epoch 39/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7809 - mse: 0.7809 - val_loss: 1.1989 - val_mse: 1.1989\n",
      "Epoch 40/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7809 - mse: 0.7809 - val_loss: 1.1990 - val_mse: 1.1990\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.1990 - mse: 1.1990\n",
      "\n",
      "Weight: [array([[2.0544403]], dtype=float32), array([0.95483893], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/40\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2755 - mse: 1.2755 - val_loss: 1.3441 - val_mse: 1.3441\n",
      "Epoch 2/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2719 - mse: 1.2719 - val_loss: 1.3390 - val_mse: 1.3390\n",
      "Epoch 3/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2692 - mse: 1.2692 - val_loss: 1.3362 - val_mse: 1.3362\n",
      "Epoch 4/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2685 - mse: 1.2685 - val_loss: 1.3341 - val_mse: 1.3341\n",
      "Epoch 5/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2673 - mse: 1.2673 - val_loss: 1.3351 - val_mse: 1.3351\n",
      "Epoch 6/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2671 - mse: 1.2671 - val_loss: 1.3339 - val_mse: 1.3339\n",
      "Epoch 7/40\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.2663 - mse: 1.2663 - val_loss: 1.3351 - val_mse: 1.3351\n",
      "Epoch 8/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2660 - mse: 1.2660 - val_loss: 1.3359 - val_mse: 1.3359\n",
      "Epoch 9/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2666 - mse: 1.2666 - val_loss: 1.3364 - val_mse: 1.3364\n",
      "Epoch 10/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2662 - mse: 1.2662 - val_loss: 1.3388 - val_mse: 1.3388\n",
      "Epoch 11/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2660 - mse: 1.2660 - val_loss: 1.3393 - val_mse: 1.3393\n",
      "Epoch 12/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2657 - mse: 1.2657 - val_loss: 1.3403 - val_mse: 1.3403\n",
      "Epoch 13/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2661 - mse: 1.2661 - val_loss: 1.3422 - val_mse: 1.3422\n",
      "Epoch 14/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2662 - mse: 1.2662 - val_loss: 1.3417 - val_mse: 1.3417\n",
      "Epoch 15/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2657 - mse: 1.2657 - val_loss: 1.3434 - val_mse: 1.3434\n",
      "Epoch 16/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2657 - mse: 1.2657 - val_loss: 1.3442 - val_mse: 1.3442\n",
      "Epoch 17/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2656 - mse: 1.2656 - val_loss: 1.3447 - val_mse: 1.3447\n",
      "Epoch 18/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2655 - mse: 1.2655 - val_loss: 1.3458 - val_mse: 1.3458\n",
      "Epoch 19/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2661 - mse: 1.2661 - val_loss: 1.3445 - val_mse: 1.3445\n",
      "Epoch 20/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2655 - mse: 1.2655 - val_loss: 1.3452 - val_mse: 1.3452\n",
      "Epoch 21/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2653 - mse: 1.2653 - val_loss: 1.3455 - val_mse: 1.3455\n",
      "Epoch 22/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2659 - mse: 1.2659 - val_loss: 1.3458 - val_mse: 1.3458\n",
      "Epoch 23/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2665 - mse: 1.2665 - val_loss: 1.3471 - val_mse: 1.3471\n",
      "Epoch 24/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2656 - mse: 1.2656 - val_loss: 1.3479 - val_mse: 1.3479\n",
      "Epoch 25/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2658 - mse: 1.2658 - val_loss: 1.3478 - val_mse: 1.3478\n",
      "Epoch 26/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2658 - mse: 1.2658 - val_loss: 1.3496 - val_mse: 1.3496\n",
      "Epoch 27/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2655 - mse: 1.2655 - val_loss: 1.3506 - val_mse: 1.3506\n",
      "Epoch 28/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2654 - mse: 1.2654 - val_loss: 1.3506 - val_mse: 1.3506\n",
      "Epoch 29/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2656 - mse: 1.2656 - val_loss: 1.3502 - val_mse: 1.3502\n",
      "Epoch 30/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2665 - mse: 1.2665 - val_loss: 1.3475 - val_mse: 1.3475\n",
      "Epoch 31/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2656 - mse: 1.2656 - val_loss: 1.3484 - val_mse: 1.3484\n",
      "Epoch 32/40\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.2658 - mse: 1.2658 - val_loss: 1.3470 - val_mse: 1.3470\n",
      "Epoch 33/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2656 - mse: 1.2656 - val_loss: 1.3466 - val_mse: 1.3466\n",
      "Epoch 34/40\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.2662 - mse: 1.2662 - val_loss: 1.3460 - val_mse: 1.3460\n",
      "Epoch 35/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2659 - mse: 1.2659 - val_loss: 1.3467 - val_mse: 1.3467\n",
      "Epoch 36/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2654 - mse: 1.2654 - val_loss: 1.3481 - val_mse: 1.3481\n",
      "Epoch 37/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2652 - mse: 1.2652 - val_loss: 1.3492 - val_mse: 1.3492\n",
      "Epoch 38/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2656 - mse: 1.2656 - val_loss: 1.3490 - val_mse: 1.3490\n",
      "Epoch 39/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2656 - mse: 1.2656 - val_loss: 1.3503 - val_mse: 1.3503\n",
      "Epoch 40/40\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2654 - mse: 1.2654 - val_loss: 1.3496 - val_mse: 1.3496\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.3496 - mse: 1.3496\n",
      "\n",
      "Weight: [array([[1.9564527]], dtype=float32), array([1.0505918], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/40\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 2/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 3/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 4/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 5/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 6/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 7/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 8/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 9/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 10/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 11/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 12/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 13/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 14/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 15/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 16/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 17/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 18/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 19/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 20/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 21/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 22/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 23/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 24/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 25/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 26/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 27/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 28/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 29/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 30/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 31/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 32/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 33/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 34/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 35/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 36/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 37/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 38/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 39/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 40/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0061 - mse: 0.0061\n",
      "\n",
      "Weight: [array([[2.0094635]], dtype=float32), array([0.9969154], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/40\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0902 - mse: 0.0902 - val_loss: 0.0637 - val_mse: 0.0637\n",
      "Epoch 2/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0638 - val_mse: 0.0638\n",
      "Epoch 3/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0639 - val_mse: 0.0639\n",
      "Epoch 4/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0640 - val_mse: 0.0640\n",
      "Epoch 5/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0640 - val_mse: 0.0640\n",
      "Epoch 6/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0640 - val_mse: 0.0640\n",
      "Epoch 7/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0640 - val_mse: 0.0640\n",
      "Epoch 8/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0640 - val_mse: 0.0640\n",
      "Epoch 9/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0641 - val_mse: 0.0641\n",
      "Epoch 10/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0641 - val_mse: 0.0641\n",
      "Epoch 11/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0900 - mse: 0.0900 - val_loss: 0.0641 - val_mse: 0.0641\n",
      "Epoch 12/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0641 - val_mse: 0.0641\n",
      "Epoch 13/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0900 - mse: 0.0900 - val_loss: 0.0641 - val_mse: 0.0641\n",
      "Epoch 14/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0641 - val_mse: 0.0641\n",
      "Epoch 15/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0641 - val_mse: 0.0641\n",
      "Epoch 16/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0641 - val_mse: 0.0641\n",
      "Epoch 17/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0641 - val_mse: 0.0641\n",
      "Epoch 18/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0641 - val_mse: 0.0641\n",
      "Epoch 19/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0900 - mse: 0.0900 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 20/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 21/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 22/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0900 - mse: 0.0900 - val_loss: 0.0643 - val_mse: 0.0643\n",
      "Epoch 23/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 24/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 25/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0900 - mse: 0.0900 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 26/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0900 - mse: 0.0900 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 27/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0900 - mse: 0.0900 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 28/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 29/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 30/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0643 - val_mse: 0.0643\n",
      "Epoch 31/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 32/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 33/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 34/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 35/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 36/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 37/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 38/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0643 - val_mse: 0.0643\n",
      "Epoch 39/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "Epoch 40/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0642 - val_mse: 0.0642\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0642 - mse: 0.0642\n",
      "\n",
      "Weight: [array([[2.0144367]], dtype=float32), array([1.0094445], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/40\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2929 - val_mse: 0.2929\n",
      "Epoch 2/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2930 - val_mse: 0.2930\n",
      "Epoch 3/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2790 - mse: 0.2790 - val_loss: 0.2929 - val_mse: 0.2929\n",
      "Epoch 4/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2792 - mse: 0.2792 - val_loss: 0.2929 - val_mse: 0.2929\n",
      "Epoch 5/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2792 - mse: 0.2792 - val_loss: 0.2929 - val_mse: 0.2929\n",
      "Epoch 6/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2929 - val_mse: 0.2929\n",
      "Epoch 7/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2930 - val_mse: 0.2930\n",
      "Epoch 8/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2929 - val_mse: 0.2929\n",
      "Epoch 9/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2928 - val_mse: 0.2928\n",
      "Epoch 10/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2790 - mse: 0.2790 - val_loss: 0.2929 - val_mse: 0.2929\n",
      "Epoch 11/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2929 - val_mse: 0.2929\n",
      "Epoch 12/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2929 - val_mse: 0.2929\n",
      "Epoch 13/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2928 - val_mse: 0.2928\n",
      "Epoch 14/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2792 - mse: 0.2792 - val_loss: 0.2927 - val_mse: 0.2927\n",
      "Epoch 15/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2928 - val_mse: 0.2928\n",
      "Epoch 16/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2927 - val_mse: 0.2927\n",
      "Epoch 17/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2928 - val_mse: 0.2928\n",
      "Epoch 18/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2929 - val_mse: 0.2929\n",
      "Epoch 19/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2929 - val_mse: 0.2929\n",
      "Epoch 20/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2929 - val_mse: 0.2929\n",
      "Epoch 21/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2790 - mse: 0.2790 - val_loss: 0.2928 - val_mse: 0.2928\n",
      "Epoch 22/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2927 - val_mse: 0.2927\n",
      "Epoch 23/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2790 - mse: 0.2790 - val_loss: 0.2927 - val_mse: 0.2927\n",
      "Epoch 24/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2790 - mse: 0.2790 - val_loss: 0.2927 - val_mse: 0.2927\n",
      "Epoch 25/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2926 - val_mse: 0.2926\n",
      "Epoch 26/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2927 - val_mse: 0.2927\n",
      "Epoch 27/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2925 - val_mse: 0.2925\n",
      "Epoch 28/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2792 - mse: 0.2792 - val_loss: 0.2926 - val_mse: 0.2926\n",
      "Epoch 29/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2926 - val_mse: 0.2926\n",
      "Epoch 30/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2793 - mse: 0.2793 - val_loss: 0.2927 - val_mse: 0.2927\n",
      "Epoch 31/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2928 - val_mse: 0.2928\n",
      "Epoch 32/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2929 - val_mse: 0.2929\n",
      "Epoch 33/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2929 - val_mse: 0.2929\n",
      "Epoch 34/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2790 - mse: 0.2790 - val_loss: 0.2927 - val_mse: 0.2927\n",
      "Epoch 35/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2928 - val_mse: 0.2928\n",
      "Epoch 36/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2927 - val_mse: 0.2927\n",
      "Epoch 37/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2926 - val_mse: 0.2926\n",
      "Epoch 38/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2925 - val_mse: 0.2925\n",
      "Epoch 39/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.2928 - val_mse: 0.2928\n",
      "Epoch 40/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2793 - mse: 0.2793 - val_loss: 0.2927 - val_mse: 0.2927\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2927 - mse: 0.2927\n",
      "\n",
      "Weight: [array([[2.0031133]], dtype=float32), array([1.0006671], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/40\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4628 - mse: 0.4628 - val_loss: 0.3552 - val_mse: 0.3552\n",
      "Epoch 2/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4625 - mse: 0.4625 - val_loss: 0.3545 - val_mse: 0.3545\n",
      "Epoch 3/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4623 - mse: 0.4623 - val_loss: 0.3545 - val_mse: 0.3545\n",
      "Epoch 4/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4622 - mse: 0.4622 - val_loss: 0.3538 - val_mse: 0.3538\n",
      "Epoch 5/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.3536 - val_mse: 0.3536\n",
      "Epoch 6/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4622 - mse: 0.4622 - val_loss: 0.3534 - val_mse: 0.3534\n",
      "Epoch 7/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4623 - mse: 0.4623 - val_loss: 0.3529 - val_mse: 0.3529\n",
      "Epoch 8/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4620 - mse: 0.4620 - val_loss: 0.3531 - val_mse: 0.3531\n",
      "Epoch 9/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.3531 - val_mse: 0.3531\n",
      "Epoch 10/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.3532 - val_mse: 0.3532\n",
      "Epoch 11/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4622 - mse: 0.4622 - val_loss: 0.3530 - val_mse: 0.3530\n",
      "Epoch 12/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.3528 - val_mse: 0.3528\n",
      "Epoch 13/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4620 - mse: 0.4620 - val_loss: 0.3529 - val_mse: 0.3529\n",
      "Epoch 14/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4623 - mse: 0.4623 - val_loss: 0.3527 - val_mse: 0.3527\n",
      "Epoch 15/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4619 - mse: 0.4619 - val_loss: 0.3530 - val_mse: 0.3530\n",
      "Epoch 16/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.3528 - val_mse: 0.3528\n",
      "Epoch 17/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4620 - mse: 0.4620 - val_loss: 0.3527 - val_mse: 0.3527\n",
      "Epoch 18/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4620 - mse: 0.4620 - val_loss: 0.3527 - val_mse: 0.3527\n",
      "Epoch 19/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.3525 - val_mse: 0.3525\n",
      "Epoch 20/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.3527 - val_mse: 0.3527\n",
      "Epoch 21/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.3529 - val_mse: 0.3529\n",
      "Epoch 22/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.3528 - val_mse: 0.3528\n",
      "Epoch 23/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4622 - mse: 0.4622 - val_loss: 0.3525 - val_mse: 0.3525\n",
      "Epoch 24/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4624 - mse: 0.4624 - val_loss: 0.3528 - val_mse: 0.3528\n",
      "Epoch 25/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.3527 - val_mse: 0.3527\n",
      "Epoch 26/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4620 - mse: 0.4620 - val_loss: 0.3527 - val_mse: 0.3527\n",
      "Epoch 27/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4620 - mse: 0.4620 - val_loss: 0.3527 - val_mse: 0.3527\n",
      "Epoch 28/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4620 - mse: 0.4620 - val_loss: 0.3528 - val_mse: 0.3528\n",
      "Epoch 29/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.3527 - val_mse: 0.3527\n",
      "Epoch 30/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.3526 - val_mse: 0.3526\n",
      "Epoch 31/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4622 - mse: 0.4622 - val_loss: 0.3524 - val_mse: 0.3524\n",
      "Epoch 32/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.3523 - val_mse: 0.3523\n",
      "Epoch 33/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.3522 - val_mse: 0.3522\n",
      "Epoch 34/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4622 - mse: 0.4622 - val_loss: 0.3524 - val_mse: 0.3524\n",
      "Epoch 35/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4620 - mse: 0.4620 - val_loss: 0.3522 - val_mse: 0.3522\n",
      "Epoch 36/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.3522 - val_mse: 0.3522\n",
      "Epoch 37/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.3522 - val_mse: 0.3522\n",
      "Epoch 38/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 0.3526 - val_mse: 0.3526\n",
      "Epoch 39/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4620 - mse: 0.4620 - val_loss: 0.3527 - val_mse: 0.3527\n",
      "Epoch 40/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4622 - mse: 0.4622 - val_loss: 0.3528 - val_mse: 0.3528\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.3528 - mse: 0.3528\n",
      "\n",
      "Weight: [array([[2.0361452]], dtype=float32), array([0.9965874], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/40\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.7968 - mse: 0.7968 - val_loss: 0.7945 - val_mse: 0.7945\n",
      "Epoch 2/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7967 - mse: 0.7967 - val_loss: 0.7949 - val_mse: 0.7949\n",
      "Epoch 3/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7965 - mse: 0.7965 - val_loss: 0.7954 - val_mse: 0.7954\n",
      "Epoch 4/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7966 - mse: 0.7966 - val_loss: 0.7957 - val_mse: 0.7957\n",
      "Epoch 5/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7965 - mse: 0.7965 - val_loss: 0.7955 - val_mse: 0.7955\n",
      "Epoch 6/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7967 - mse: 0.7967 - val_loss: 0.7944 - val_mse: 0.7944\n",
      "Epoch 7/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7962 - mse: 0.7962 - val_loss: 0.7937 - val_mse: 0.7937\n",
      "Epoch 8/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7963 - mse: 0.7963 - val_loss: 0.7942 - val_mse: 0.7942\n",
      "Epoch 9/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7964 - mse: 0.7964 - val_loss: 0.7940 - val_mse: 0.7940\n",
      "Epoch 10/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7963 - mse: 0.7963 - val_loss: 0.7942 - val_mse: 0.7942\n",
      "Epoch 11/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7963 - mse: 0.7963 - val_loss: 0.7930 - val_mse: 0.7930\n",
      "Epoch 12/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7963 - mse: 0.7963 - val_loss: 0.7926 - val_mse: 0.7926\n",
      "Epoch 13/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7964 - mse: 0.7964 - val_loss: 0.7930 - val_mse: 0.7930\n",
      "Epoch 14/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7966 - mse: 0.7966 - val_loss: 0.7923 - val_mse: 0.7923\n",
      "Epoch 15/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7963 - mse: 0.7963 - val_loss: 0.7923 - val_mse: 0.7923\n",
      "Epoch 16/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7965 - mse: 0.7965 - val_loss: 0.7928 - val_mse: 0.7928\n",
      "Epoch 17/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7966 - mse: 0.7966 - val_loss: 0.7924 - val_mse: 0.7924\n",
      "Epoch 18/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7962 - mse: 0.7962 - val_loss: 0.7924 - val_mse: 0.7924\n",
      "Epoch 19/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7961 - mse: 0.7961 - val_loss: 0.7927 - val_mse: 0.7927\n",
      "Epoch 20/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7966 - mse: 0.7966 - val_loss: 0.7923 - val_mse: 0.7923\n",
      "Epoch 21/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7961 - mse: 0.7961 - val_loss: 0.7927 - val_mse: 0.7927\n",
      "Epoch 22/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7963 - mse: 0.7963 - val_loss: 0.7916 - val_mse: 0.7916\n",
      "Epoch 23/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7968 - mse: 0.7968 - val_loss: 0.7923 - val_mse: 0.7923\n",
      "Epoch 24/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7962 - mse: 0.7962 - val_loss: 0.7917 - val_mse: 0.7917\n",
      "Epoch 25/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7964 - mse: 0.7964 - val_loss: 0.7915 - val_mse: 0.7915\n",
      "Epoch 26/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7962 - mse: 0.7962 - val_loss: 0.7919 - val_mse: 0.7919\n",
      "Epoch 27/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7963 - mse: 0.7963 - val_loss: 0.7924 - val_mse: 0.7924\n",
      "Epoch 28/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7963 - mse: 0.7963 - val_loss: 0.7912 - val_mse: 0.7912\n",
      "Epoch 29/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7964 - mse: 0.7964 - val_loss: 0.7917 - val_mse: 0.7917\n",
      "Epoch 30/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7964 - mse: 0.7964 - val_loss: 0.7919 - val_mse: 0.7919\n",
      "Epoch 31/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7963 - mse: 0.7963 - val_loss: 0.7916 - val_mse: 0.7916\n",
      "Epoch 32/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7962 - mse: 0.7962 - val_loss: 0.7923 - val_mse: 0.7923\n",
      "Epoch 33/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7962 - mse: 0.7962 - val_loss: 0.7918 - val_mse: 0.7918\n",
      "Epoch 34/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7961 - mse: 0.7961 - val_loss: 0.7927 - val_mse: 0.7927\n",
      "Epoch 35/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7964 - mse: 0.7964 - val_loss: 0.7933 - val_mse: 0.7933\n",
      "Epoch 36/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7961 - mse: 0.7961 - val_loss: 0.7936 - val_mse: 0.7936\n",
      "Epoch 37/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7963 - mse: 0.7963 - val_loss: 0.7935 - val_mse: 0.7935\n",
      "Epoch 38/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7963 - mse: 0.7963 - val_loss: 0.7935 - val_mse: 0.7935\n",
      "Epoch 39/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7962 - mse: 0.7962 - val_loss: 0.7935 - val_mse: 0.7935\n",
      "Epoch 40/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.7963 - mse: 0.7963 - val_loss: 0.7936 - val_mse: 0.7936\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.7936 - mse: 0.7936\n",
      "\n",
      "Weight: [array([[1.9939067]], dtype=float32), array([0.9863484], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/40\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 1.1933 - mse: 1.1933 - val_loss: 1.0922 - val_mse: 1.0922\n",
      "Epoch 2/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1923 - mse: 1.1923 - val_loss: 1.0888 - val_mse: 1.0888\n",
      "Epoch 3/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1911 - mse: 1.1911 - val_loss: 1.0854 - val_mse: 1.0854\n",
      "Epoch 4/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1898 - mse: 1.1898 - val_loss: 1.0840 - val_mse: 1.0840\n",
      "Epoch 5/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1899 - mse: 1.1899 - val_loss: 1.0832 - val_mse: 1.0832\n",
      "Epoch 6/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1899 - mse: 1.1899 - val_loss: 1.0836 - val_mse: 1.0836\n",
      "Epoch 7/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1895 - mse: 1.1895 - val_loss: 1.0828 - val_mse: 1.0828\n",
      "Epoch 8/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1895 - mse: 1.1895 - val_loss: 1.0831 - val_mse: 1.0831\n",
      "Epoch 9/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1895 - mse: 1.1895 - val_loss: 1.0829 - val_mse: 1.0829\n",
      "Epoch 10/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1894 - mse: 1.1894 - val_loss: 1.0837 - val_mse: 1.0837\n",
      "Epoch 11/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1894 - mse: 1.1894 - val_loss: 1.0818 - val_mse: 1.0818\n",
      "Epoch 12/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1888 - mse: 1.1888 - val_loss: 1.0808 - val_mse: 1.0808\n",
      "Epoch 13/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1890 - mse: 1.1890 - val_loss: 1.0823 - val_mse: 1.0823\n",
      "Epoch 14/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1891 - mse: 1.1891 - val_loss: 1.0836 - val_mse: 1.0836\n",
      "Epoch 15/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1893 - mse: 1.1893 - val_loss: 1.0831 - val_mse: 1.0831\n",
      "Epoch 16/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1891 - mse: 1.1891 - val_loss: 1.0846 - val_mse: 1.0846\n",
      "Epoch 17/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1892 - mse: 1.1892 - val_loss: 1.0850 - val_mse: 1.0850\n",
      "Epoch 18/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1889 - mse: 1.1889 - val_loss: 1.0832 - val_mse: 1.0832\n",
      "Epoch 19/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1891 - mse: 1.1891 - val_loss: 1.0840 - val_mse: 1.0840\n",
      "Epoch 20/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1895 - mse: 1.1895 - val_loss: 1.0831 - val_mse: 1.0831\n",
      "Epoch 21/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1890 - mse: 1.1890 - val_loss: 1.0832 - val_mse: 1.0832\n",
      "Epoch 22/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1892 - mse: 1.1892 - val_loss: 1.0838 - val_mse: 1.0838\n",
      "Epoch 23/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1891 - mse: 1.1891 - val_loss: 1.0836 - val_mse: 1.0836\n",
      "Epoch 24/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1890 - mse: 1.1890 - val_loss: 1.0846 - val_mse: 1.0846\n",
      "Epoch 25/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1892 - mse: 1.1892 - val_loss: 1.0854 - val_mse: 1.0854\n",
      "Epoch 26/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1890 - mse: 1.1890 - val_loss: 1.0843 - val_mse: 1.0843\n",
      "Epoch 27/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1889 - mse: 1.1889 - val_loss: 1.0845 - val_mse: 1.0845\n",
      "Epoch 28/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1890 - mse: 1.1890 - val_loss: 1.0823 - val_mse: 1.0823\n",
      "Epoch 29/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1888 - mse: 1.1888 - val_loss: 1.0831 - val_mse: 1.0831\n",
      "Epoch 30/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1887 - mse: 1.1887 - val_loss: 1.0843 - val_mse: 1.0843\n",
      "Epoch 31/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1890 - mse: 1.1890 - val_loss: 1.0847 - val_mse: 1.0847\n",
      "Epoch 32/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1894 - mse: 1.1894 - val_loss: 1.0842 - val_mse: 1.0842\n",
      "Epoch 33/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1890 - mse: 1.1890 - val_loss: 1.0843 - val_mse: 1.0843\n",
      "Epoch 34/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1891 - mse: 1.1891 - val_loss: 1.0851 - val_mse: 1.0851\n",
      "Epoch 35/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1893 - mse: 1.1893 - val_loss: 1.0846 - val_mse: 1.0846\n",
      "Epoch 36/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1888 - mse: 1.1888 - val_loss: 1.0847 - val_mse: 1.0847\n",
      "Epoch 37/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1890 - mse: 1.1890 - val_loss: 1.0842 - val_mse: 1.0842\n",
      "Epoch 38/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1890 - mse: 1.1890 - val_loss: 1.0838 - val_mse: 1.0838\n",
      "Epoch 39/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1893 - mse: 1.1893 - val_loss: 1.0854 - val_mse: 1.0854\n",
      "Epoch 40/40\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1893 - mse: 1.1893 - val_loss: 1.0851 - val_mse: 1.0851\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.0851 - mse: 1.0851\n",
      "\n",
      "Weight: [array([[2.0706518]], dtype=float32), array([0.93296987], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/40\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0131 - val_mse: 0.0131\n",
      "Epoch 2/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 3/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 4/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 5/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 6/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 7/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 8/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 9/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 10/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 11/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 12/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 13/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 14/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 15/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 16/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 17/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 18/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 19/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 20/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 21/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 22/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 23/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 24/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 25/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 26/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 27/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 28/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 29/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 30/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 31/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 32/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 33/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 34/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 35/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 36/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 37/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 38/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 39/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 40/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110\n",
      "\n",
      "Weight: [array([[1.9998891]], dtype=float32), array([1.0031106], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/40\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0817 - val_mse: 0.0817\n",
      "Epoch 2/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0816 - val_mse: 0.0816\n",
      "Epoch 3/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0817 - val_mse: 0.0817\n",
      "Epoch 4/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0952 - mse: 0.0952 - val_loss: 0.0816 - val_mse: 0.0816\n",
      "Epoch 5/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 6/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 7/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 8/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0814 - val_mse: 0.0814\n",
      "Epoch 9/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0952 - mse: 0.0952 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 10/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0813 - val_mse: 0.0813\n",
      "Epoch 11/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 12/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 13/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0816 - val_mse: 0.0816\n",
      "Epoch 14/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0816 - val_mse: 0.0816\n",
      "Epoch 15/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0814 - val_mse: 0.0814\n",
      "Epoch 16/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0816 - val_mse: 0.0816\n",
      "Epoch 17/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0816 - val_mse: 0.0816\n",
      "Epoch 18/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0816 - val_mse: 0.0816\n",
      "Epoch 19/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0814 - val_mse: 0.0814\n",
      "Epoch 20/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 21/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 22/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 23/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0816 - val_mse: 0.0816\n",
      "Epoch 24/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 25/40\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 26/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 27/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 28/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 29/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0952 - mse: 0.0952 - val_loss: 0.0816 - val_mse: 0.0816\n",
      "Epoch 30/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0817 - val_mse: 0.0817\n",
      "Epoch 31/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0816 - val_mse: 0.0816\n",
      "Epoch 32/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0816 - val_mse: 0.0816\n",
      "Epoch 33/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 34/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 35/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0814 - val_mse: 0.0814\n",
      "Epoch 36/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 37/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0816 - val_mse: 0.0816\n",
      "Epoch 38/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0816 - val_mse: 0.0816\n",
      "Epoch 39/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 40/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.0816 - val_mse: 0.0816\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0816 - mse: 0.0816\n",
      "\n",
      "Weight: [array([[2.0051582]], dtype=float32), array([1.0040563], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/40\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2544 - mse: 0.2544 - val_loss: 0.2621 - val_mse: 0.2621\n",
      "Epoch 2/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2541 - mse: 0.2541 - val_loss: 0.2608 - val_mse: 0.2608\n",
      "Epoch 3/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2540 - mse: 0.2540 - val_loss: 0.2600 - val_mse: 0.2600\n",
      "Epoch 4/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2539 - mse: 0.2539 - val_loss: 0.2595 - val_mse: 0.2595\n",
      "Epoch 5/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2537 - mse: 0.2537 - val_loss: 0.2593 - val_mse: 0.2593\n",
      "Epoch 6/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2537 - mse: 0.2537 - val_loss: 0.2592 - val_mse: 0.2592\n",
      "Epoch 7/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2537 - mse: 0.2537 - val_loss: 0.2590 - val_mse: 0.2590\n",
      "Epoch 8/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.2586 - val_mse: 0.2586\n",
      "Epoch 9/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.2584 - val_mse: 0.2584\n",
      "Epoch 10/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.2584 - val_mse: 0.2584\n",
      "Epoch 11/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.2581 - val_mse: 0.2581\n",
      "Epoch 12/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2578 - val_mse: 0.2578\n",
      "Epoch 13/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2577 - val_mse: 0.2577\n",
      "Epoch 14/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2579 - val_mse: 0.2579\n",
      "Epoch 15/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2577 - val_mse: 0.2577\n",
      "Epoch 16/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.2577 - val_mse: 0.2577\n",
      "Epoch 17/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.2574 - val_mse: 0.2574\n",
      "Epoch 18/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2534 - mse: 0.2534 - val_loss: 0.2575 - val_mse: 0.2575\n",
      "Epoch 19/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2572 - val_mse: 0.2572\n",
      "Epoch 20/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.2569 - val_mse: 0.2569\n",
      "Epoch 21/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2570 - val_mse: 0.2570\n",
      "Epoch 22/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2570 - val_mse: 0.2570\n",
      "Epoch 23/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2571 - val_mse: 0.2571\n",
      "Epoch 24/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2572 - val_mse: 0.2572\n",
      "Epoch 25/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.2571 - val_mse: 0.2571\n",
      "Epoch 26/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2572 - val_mse: 0.2572\n",
      "Epoch 27/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2571 - val_mse: 0.2571\n",
      "Epoch 28/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2570 - val_mse: 0.2570\n",
      "Epoch 29/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2573 - val_mse: 0.2573\n",
      "Epoch 30/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2572 - val_mse: 0.2572\n",
      "Epoch 31/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.2571 - val_mse: 0.2571\n",
      "Epoch 32/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.2572 - val_mse: 0.2572\n",
      "Epoch 33/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2570 - val_mse: 0.2570\n",
      "Epoch 34/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2571 - val_mse: 0.2571\n",
      "Epoch 35/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2572 - val_mse: 0.2572\n",
      "Epoch 36/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.2572 - val_mse: 0.2572\n",
      "Epoch 37/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.2573 - val_mse: 0.2573\n",
      "Epoch 38/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2573 - val_mse: 0.2573\n",
      "Epoch 39/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2572 - val_mse: 0.2572\n",
      "Epoch 40/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2569 - val_mse: 0.2569\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2569 - mse: 0.2569\n",
      "\n",
      "Weight: [array([[1.9538445]], dtype=float32), array([0.9926161], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/40\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4718 - mse: 0.4718 - val_loss: 0.5987 - val_mse: 0.5987\n",
      "Epoch 2/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4714 - mse: 0.4714 - val_loss: 0.5995 - val_mse: 0.5995\n",
      "Epoch 3/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4711 - mse: 0.4711 - val_loss: 0.6001 - val_mse: 0.6001\n",
      "Epoch 4/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4710 - mse: 0.4710 - val_loss: 0.6005 - val_mse: 0.6005\n",
      "Epoch 5/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4707 - mse: 0.4707 - val_loss: 0.6008 - val_mse: 0.6008\n",
      "Epoch 6/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4706 - mse: 0.4706 - val_loss: 0.6008 - val_mse: 0.6008\n",
      "Epoch 7/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4707 - mse: 0.4707 - val_loss: 0.6010 - val_mse: 0.6010\n",
      "Epoch 8/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4707 - mse: 0.4707 - val_loss: 0.6014 - val_mse: 0.6014\n",
      "Epoch 9/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.6015 - val_mse: 0.6015\n",
      "Epoch 10/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4705 - mse: 0.4705 - val_loss: 0.6015 - val_mse: 0.6015\n",
      "Epoch 11/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.6017 - val_mse: 0.6017\n",
      "Epoch 12/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4705 - mse: 0.4705 - val_loss: 0.6019 - val_mse: 0.6019\n",
      "Epoch 13/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.6018 - val_mse: 0.6018\n",
      "Epoch 14/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.6019 - val_mse: 0.6019\n",
      "Epoch 15/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4703 - mse: 0.4703 - val_loss: 0.6020 - val_mse: 0.6020\n",
      "Epoch 16/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.6025 - val_mse: 0.6025\n",
      "Epoch 17/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.6024 - val_mse: 0.6024\n",
      "Epoch 18/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.6025 - val_mse: 0.6025\n",
      "Epoch 19/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4703 - mse: 0.4703 - val_loss: 0.6025 - val_mse: 0.6025\n",
      "Epoch 20/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.6025 - val_mse: 0.6025\n",
      "Epoch 21/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4705 - mse: 0.4705 - val_loss: 0.6025 - val_mse: 0.6025\n",
      "Epoch 22/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.6026 - val_mse: 0.6026\n",
      "Epoch 23/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4703 - mse: 0.4703 - val_loss: 0.6026 - val_mse: 0.6026\n",
      "Epoch 24/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4705 - mse: 0.4705 - val_loss: 0.6024 - val_mse: 0.6024\n",
      "Epoch 25/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4703 - mse: 0.4703 - val_loss: 0.6027 - val_mse: 0.6027\n",
      "Epoch 26/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.6024 - val_mse: 0.6024\n",
      "Epoch 27/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4705 - mse: 0.4705 - val_loss: 0.6023 - val_mse: 0.6023\n",
      "Epoch 28/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.6024 - val_mse: 0.6024\n",
      "Epoch 29/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4703 - mse: 0.4703 - val_loss: 0.6025 - val_mse: 0.6025\n",
      "Epoch 30/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.6029 - val_mse: 0.6029\n",
      "Epoch 31/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.6029 - val_mse: 0.6029\n",
      "Epoch 32/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4703 - mse: 0.4703 - val_loss: 0.6027 - val_mse: 0.6027\n",
      "Epoch 33/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4706 - mse: 0.4706 - val_loss: 0.6029 - val_mse: 0.6029\n",
      "Epoch 34/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4703 - mse: 0.4703 - val_loss: 0.6028 - val_mse: 0.6028\n",
      "Epoch 35/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.6029 - val_mse: 0.6029\n",
      "Epoch 36/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.6029 - val_mse: 0.6029\n",
      "Epoch 37/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.6026 - val_mse: 0.6026\n",
      "Epoch 38/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4703 - mse: 0.4703 - val_loss: 0.6026 - val_mse: 0.6026\n",
      "Epoch 39/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4706 - mse: 0.4706 - val_loss: 0.6028 - val_mse: 0.6028\n",
      "Epoch 40/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.6029 - val_mse: 0.6029\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.6029 - mse: 0.6029\n",
      "\n",
      "Weight: [array([[2.020956]], dtype=float32), array([0.99191755], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/40\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.7594 - mse: 0.7594 - val_loss: 0.8342 - val_mse: 0.8342\n",
      "Epoch 2/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7578 - mse: 0.7578 - val_loss: 0.8345 - val_mse: 0.8345\n",
      "Epoch 3/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7569 - mse: 0.7569 - val_loss: 0.8352 - val_mse: 0.8352\n",
      "Epoch 4/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7565 - mse: 0.7565 - val_loss: 0.8359 - val_mse: 0.8359\n",
      "Epoch 5/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7566 - mse: 0.7566 - val_loss: 0.8365 - val_mse: 0.8365\n",
      "Epoch 6/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7561 - mse: 0.7561 - val_loss: 0.8372 - val_mse: 0.8372\n",
      "Epoch 7/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7559 - mse: 0.7559 - val_loss: 0.8376 - val_mse: 0.8376\n",
      "Epoch 8/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7558 - mse: 0.7558 - val_loss: 0.8380 - val_mse: 0.8380\n",
      "Epoch 9/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7559 - mse: 0.7559 - val_loss: 0.8383 - val_mse: 0.8383\n",
      "Epoch 10/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7559 - mse: 0.7559 - val_loss: 0.8386 - val_mse: 0.8386\n",
      "Epoch 11/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7558 - mse: 0.7558 - val_loss: 0.8388 - val_mse: 0.8388\n",
      "Epoch 12/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7557 - mse: 0.7557 - val_loss: 0.8390 - val_mse: 0.8390\n",
      "Epoch 13/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7558 - mse: 0.7558 - val_loss: 0.8392 - val_mse: 0.8392\n",
      "Epoch 14/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7557 - mse: 0.7557 - val_loss: 0.8394 - val_mse: 0.8394\n",
      "Epoch 15/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7557 - mse: 0.7557 - val_loss: 0.8395 - val_mse: 0.8395\n",
      "Epoch 16/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7559 - mse: 0.7559 - val_loss: 0.8396 - val_mse: 0.8396\n",
      "Epoch 17/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7560 - mse: 0.7560 - val_loss: 0.8398 - val_mse: 0.8398\n",
      "Epoch 18/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7559 - mse: 0.7559 - val_loss: 0.8399 - val_mse: 0.8399\n",
      "Epoch 19/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7556 - mse: 0.7556 - val_loss: 0.8401 - val_mse: 0.8401\n",
      "Epoch 20/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7559 - mse: 0.7559 - val_loss: 0.8400 - val_mse: 0.8400\n",
      "Epoch 21/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7557 - mse: 0.7557 - val_loss: 0.8401 - val_mse: 0.8401\n",
      "Epoch 22/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7557 - mse: 0.7557 - val_loss: 0.8402 - val_mse: 0.8402\n",
      "Epoch 23/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.8404 - val_mse: 0.8404\n",
      "Epoch 24/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7558 - mse: 0.7558 - val_loss: 0.8405 - val_mse: 0.8405\n",
      "Epoch 25/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7558 - mse: 0.7558 - val_loss: 0.8404 - val_mse: 0.8404\n",
      "Epoch 26/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7556 - mse: 0.7556 - val_loss: 0.8405 - val_mse: 0.8405\n",
      "Epoch 27/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.8405 - val_mse: 0.8405\n",
      "Epoch 28/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7556 - mse: 0.7556 - val_loss: 0.8405 - val_mse: 0.8405\n",
      "Epoch 29/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7557 - mse: 0.7557 - val_loss: 0.8404 - val_mse: 0.8404\n",
      "Epoch 30/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7559 - mse: 0.7559 - val_loss: 0.8404 - val_mse: 0.8404\n",
      "Epoch 31/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7559 - mse: 0.7559 - val_loss: 0.8405 - val_mse: 0.8405\n",
      "Epoch 32/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.8406 - val_mse: 0.8406\n",
      "Epoch 33/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7556 - mse: 0.7556 - val_loss: 0.8406 - val_mse: 0.8406\n",
      "Epoch 34/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7558 - mse: 0.7558 - val_loss: 0.8406 - val_mse: 0.8406\n",
      "Epoch 35/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7559 - mse: 0.7559 - val_loss: 0.8406 - val_mse: 0.8406\n",
      "Epoch 36/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7555 - mse: 0.7555 - val_loss: 0.8406 - val_mse: 0.8406\n",
      "Epoch 37/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7556 - mse: 0.7556 - val_loss: 0.8405 - val_mse: 0.8405\n",
      "Epoch 38/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7556 - mse: 0.7556 - val_loss: 0.8405 - val_mse: 0.8405\n",
      "Epoch 39/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7554 - mse: 0.7554 - val_loss: 0.8406 - val_mse: 0.8406\n",
      "Epoch 40/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7557 - mse: 0.7557 - val_loss: 0.8406 - val_mse: 0.8406\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.8406 - mse: 0.8406\n",
      "\n",
      "Weight: [array([[1.940276]], dtype=float32), array([0.9412702], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/40\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1.1430 - mse: 1.1430 - val_loss: 1.4513 - val_mse: 1.4513\n",
      "Epoch 2/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1408 - mse: 1.1408 - val_loss: 1.4427 - val_mse: 1.4427\n",
      "Epoch 3/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1397 - mse: 1.1397 - val_loss: 1.4360 - val_mse: 1.4360\n",
      "Epoch 4/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1390 - mse: 1.1390 - val_loss: 1.4302 - val_mse: 1.4302\n",
      "Epoch 5/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1381 - mse: 1.1381 - val_loss: 1.4276 - val_mse: 1.4276\n",
      "Epoch 6/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1374 - mse: 1.1374 - val_loss: 1.4248 - val_mse: 1.4248\n",
      "Epoch 7/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1373 - mse: 1.1373 - val_loss: 1.4227 - val_mse: 1.4227\n",
      "Epoch 8/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1368 - mse: 1.1368 - val_loss: 1.4207 - val_mse: 1.4207\n",
      "Epoch 9/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1366 - mse: 1.1366 - val_loss: 1.4188 - val_mse: 1.4188\n",
      "Epoch 10/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1362 - mse: 1.1362 - val_loss: 1.4166 - val_mse: 1.4166\n",
      "Epoch 11/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1366 - mse: 1.1366 - val_loss: 1.4142 - val_mse: 1.4142\n",
      "Epoch 12/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1365 - mse: 1.1365 - val_loss: 1.4128 - val_mse: 1.4128\n",
      "Epoch 13/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1359 - mse: 1.1359 - val_loss: 1.4117 - val_mse: 1.4117\n",
      "Epoch 14/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1361 - mse: 1.1361 - val_loss: 1.4116 - val_mse: 1.4116\n",
      "Epoch 15/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1364 - mse: 1.1364 - val_loss: 1.4113 - val_mse: 1.4113\n",
      "Epoch 16/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1362 - mse: 1.1362 - val_loss: 1.4108 - val_mse: 1.4108\n",
      "Epoch 17/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1359 - mse: 1.1359 - val_loss: 1.4108 - val_mse: 1.4108\n",
      "Epoch 18/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1359 - mse: 1.1359 - val_loss: 1.4115 - val_mse: 1.4115\n",
      "Epoch 19/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1363 - mse: 1.1363 - val_loss: 1.4114 - val_mse: 1.4114\n",
      "Epoch 20/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1361 - mse: 1.1361 - val_loss: 1.4105 - val_mse: 1.4105\n",
      "Epoch 21/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1361 - mse: 1.1361 - val_loss: 1.4121 - val_mse: 1.4121\n",
      "Epoch 22/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1361 - mse: 1.1361 - val_loss: 1.4116 - val_mse: 1.4116\n",
      "Epoch 23/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1364 - mse: 1.1364 - val_loss: 1.4096 - val_mse: 1.4096\n",
      "Epoch 24/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1365 - mse: 1.1365 - val_loss: 1.4086 - val_mse: 1.4086\n",
      "Epoch 25/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1359 - mse: 1.1359 - val_loss: 1.4082 - val_mse: 1.4082\n",
      "Epoch 26/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1360 - mse: 1.1360 - val_loss: 1.4107 - val_mse: 1.4107\n",
      "Epoch 27/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1360 - mse: 1.1360 - val_loss: 1.4115 - val_mse: 1.4115\n",
      "Epoch 28/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1360 - mse: 1.1360 - val_loss: 1.4111 - val_mse: 1.4111\n",
      "Epoch 29/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1361 - mse: 1.1361 - val_loss: 1.4103 - val_mse: 1.4103\n",
      "Epoch 30/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1364 - mse: 1.1364 - val_loss: 1.4116 - val_mse: 1.4116\n",
      "Epoch 31/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1360 - mse: 1.1360 - val_loss: 1.4125 - val_mse: 1.4125\n",
      "Epoch 32/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1361 - mse: 1.1361 - val_loss: 1.4110 - val_mse: 1.4110\n",
      "Epoch 33/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1359 - mse: 1.1359 - val_loss: 1.4102 - val_mse: 1.4102\n",
      "Epoch 34/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1357 - mse: 1.1357 - val_loss: 1.4114 - val_mse: 1.4114\n",
      "Epoch 35/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1357 - mse: 1.1357 - val_loss: 1.4093 - val_mse: 1.4093\n",
      "Epoch 36/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1361 - mse: 1.1361 - val_loss: 1.4082 - val_mse: 1.4082\n",
      "Epoch 37/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1360 - mse: 1.1360 - val_loss: 1.4095 - val_mse: 1.4095\n",
      "Epoch 38/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1360 - mse: 1.1360 - val_loss: 1.4123 - val_mse: 1.4123\n",
      "Epoch 39/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1360 - mse: 1.1360 - val_loss: 1.4136 - val_mse: 1.4136\n",
      "Epoch 40/40\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1359 - mse: 1.1359 - val_loss: 1.4114 - val_mse: 1.4114\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.4114 - mse: 1.4114\n",
      "\n",
      "Weight: [array([[2.0851681]], dtype=float32), array([0.97787124], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/40\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 2/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 3/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 4/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 5/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 6/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 7/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 8/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 9/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 10/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 11/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 12/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 13/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 14/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 15/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 16/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 17/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 18/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 19/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 20/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 21/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 22/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 23/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 24/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 25/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 26/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 27/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 28/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 29/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 30/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 31/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 32/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 33/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 34/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 35/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 36/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 37/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 38/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 39/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 40/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094\n",
      "\n",
      "Weight: [array([[1.9930611]], dtype=float32), array([0.9967355], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/40\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0904 - mse: 0.0904 - val_loss: 0.0559 - val_mse: 0.0559\n",
      "Epoch 2/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0904 - mse: 0.0904 - val_loss: 0.0561 - val_mse: 0.0561\n",
      "Epoch 3/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0904 - mse: 0.0904 - val_loss: 0.0562 - val_mse: 0.0562\n",
      "Epoch 4/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0564 - val_mse: 0.0564\n",
      "Epoch 5/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0566 - val_mse: 0.0566\n",
      "Epoch 6/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0570 - val_mse: 0.0570\n",
      "Epoch 7/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0570 - val_mse: 0.0570\n",
      "Epoch 8/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0572 - val_mse: 0.0572\n",
      "Epoch 9/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0572 - val_mse: 0.0572\n",
      "Epoch 10/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0572 - val_mse: 0.0572\n",
      "Epoch 11/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0571 - val_mse: 0.0571\n",
      "Epoch 12/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0572 - val_mse: 0.0572\n",
      "Epoch 13/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0573 - val_mse: 0.0573\n",
      "Epoch 14/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0573 - val_mse: 0.0573\n",
      "Epoch 15/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0574 - val_mse: 0.0574\n",
      "Epoch 16/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0574 - val_mse: 0.0574\n",
      "Epoch 17/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0575 - val_mse: 0.0575\n",
      "Epoch 18/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0574 - val_mse: 0.0574\n",
      "Epoch 19/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0575 - val_mse: 0.0575\n",
      "Epoch 20/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0575 - val_mse: 0.0575\n",
      "Epoch 21/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0574 - val_mse: 0.0574\n",
      "Epoch 22/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0576 - val_mse: 0.0576\n",
      "Epoch 23/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0576 - val_mse: 0.0576\n",
      "Epoch 24/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0576 - val_mse: 0.0576\n",
      "Epoch 25/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0574 - val_mse: 0.0574\n",
      "Epoch 26/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0576 - val_mse: 0.0576\n",
      "Epoch 27/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0576 - val_mse: 0.0576\n",
      "Epoch 28/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0577 - val_mse: 0.0577\n",
      "Epoch 29/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0577 - val_mse: 0.0577\n",
      "Epoch 30/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0577 - val_mse: 0.0577\n",
      "Epoch 31/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0577 - val_mse: 0.0577\n",
      "Epoch 32/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0578 - val_mse: 0.0578\n",
      "Epoch 33/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0578 - val_mse: 0.0578\n",
      "Epoch 34/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0577 - val_mse: 0.0577\n",
      "Epoch 35/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0577 - val_mse: 0.0577\n",
      "Epoch 36/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0577 - val_mse: 0.0577\n",
      "Epoch 37/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0578 - val_mse: 0.0578\n",
      "Epoch 38/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0577 - val_mse: 0.0577\n",
      "Epoch 39/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0576 - val_mse: 0.0576\n",
      "Epoch 40/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.0576 - val_mse: 0.0576\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0576 - mse: 0.0576\n",
      "\n",
      "Weight: [array([[2.0156662]], dtype=float32), array([0.9915239], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/40\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2654 - mse: 0.2654 - val_loss: 0.2642 - val_mse: 0.2642\n",
      "Epoch 2/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2651 - mse: 0.2651 - val_loss: 0.2639 - val_mse: 0.2639\n",
      "Epoch 3/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2650 - mse: 0.2650 - val_loss: 0.2638 - val_mse: 0.2638\n",
      "Epoch 4/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2638 - val_mse: 0.2638\n",
      "Epoch 5/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2638 - val_mse: 0.2638\n",
      "Epoch 6/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2638 - val_mse: 0.2638\n",
      "Epoch 7/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2638 - val_mse: 0.2638\n",
      "Epoch 8/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2648 - mse: 0.2648 - val_loss: 0.2639 - val_mse: 0.2639\n",
      "Epoch 9/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2639 - val_mse: 0.2639\n",
      "Epoch 10/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2640 - val_mse: 0.2640\n",
      "Epoch 11/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2640 - val_mse: 0.2640\n",
      "Epoch 12/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2648 - mse: 0.2648 - val_loss: 0.2641 - val_mse: 0.2641\n",
      "Epoch 13/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2641 - val_mse: 0.2641\n",
      "Epoch 14/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2648 - mse: 0.2648 - val_loss: 0.2642 - val_mse: 0.2642\n",
      "Epoch 15/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2648 - mse: 0.2648 - val_loss: 0.2641 - val_mse: 0.2641\n",
      "Epoch 16/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2640 - val_mse: 0.2640\n",
      "Epoch 17/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2648 - mse: 0.2648 - val_loss: 0.2640 - val_mse: 0.2640\n",
      "Epoch 18/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2641 - val_mse: 0.2641\n",
      "Epoch 19/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2642 - val_mse: 0.2642\n",
      "Epoch 20/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2642 - val_mse: 0.2642\n",
      "Epoch 21/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2642 - val_mse: 0.2642\n",
      "Epoch 22/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2642 - val_mse: 0.2642\n",
      "Epoch 23/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2642 - val_mse: 0.2642\n",
      "Epoch 24/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2648 - mse: 0.2648 - val_loss: 0.2642 - val_mse: 0.2642\n",
      "Epoch 25/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2643 - val_mse: 0.2643\n",
      "Epoch 26/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2643 - val_mse: 0.2643\n",
      "Epoch 27/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2648 - mse: 0.2648 - val_loss: 0.2642 - val_mse: 0.2642\n",
      "Epoch 28/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2641 - val_mse: 0.2641\n",
      "Epoch 29/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2648 - mse: 0.2648 - val_loss: 0.2641 - val_mse: 0.2641\n",
      "Epoch 30/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2641 - val_mse: 0.2641\n",
      "Epoch 31/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2642 - val_mse: 0.2642\n",
      "Epoch 32/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2648 - mse: 0.2648 - val_loss: 0.2643 - val_mse: 0.2643\n",
      "Epoch 33/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2648 - mse: 0.2648 - val_loss: 0.2643 - val_mse: 0.2643\n",
      "Epoch 34/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2648 - mse: 0.2648 - val_loss: 0.2643 - val_mse: 0.2643\n",
      "Epoch 35/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2648 - mse: 0.2648 - val_loss: 0.2642 - val_mse: 0.2642\n",
      "Epoch 36/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2641 - val_mse: 0.2641\n",
      "Epoch 37/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2641 - val_mse: 0.2641\n",
      "Epoch 38/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2648 - mse: 0.2648 - val_loss: 0.2642 - val_mse: 0.2642\n",
      "Epoch 39/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2648 - mse: 0.2648 - val_loss: 0.2643 - val_mse: 0.2643\n",
      "Epoch 40/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2642 - val_mse: 0.2642\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2642 - mse: 0.2642\n",
      "\n",
      "Weight: [array([[2.036817]], dtype=float32), array([1.0141306], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/40\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.4280 - val_mse: 0.4280\n",
      "Epoch 2/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.4279 - val_mse: 0.4279\n",
      "Epoch 3/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.4283 - val_mse: 0.4283\n",
      "Epoch 4/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.4286 - val_mse: 0.4286\n",
      "Epoch 5/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.4285 - val_mse: 0.4285\n",
      "Epoch 6/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.4287 - val_mse: 0.4287\n",
      "Epoch 7/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.4287 - val_mse: 0.4287\n",
      "Epoch 8/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.4291 - val_mse: 0.4291\n",
      "Epoch 9/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.4296 - val_mse: 0.4296\n",
      "Epoch 10/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.4293 - val_mse: 0.4293\n",
      "Epoch 11/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4930 - mse: 0.4930 - val_loss: 0.4288 - val_mse: 0.4288\n",
      "Epoch 12/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.4288 - val_mse: 0.4288\n",
      "Epoch 13/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4930 - mse: 0.4930 - val_loss: 0.4288 - val_mse: 0.4288\n",
      "Epoch 14/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.4290 - val_mse: 0.4290\n",
      "Epoch 15/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.4291 - val_mse: 0.4291\n",
      "Epoch 16/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4930 - mse: 0.4930 - val_loss: 0.4290 - val_mse: 0.4290\n",
      "Epoch 17/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.4290 - val_mse: 0.4290\n",
      "Epoch 18/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.4287 - val_mse: 0.4287\n",
      "Epoch 19/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.4283 - val_mse: 0.4283\n",
      "Epoch 20/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.4285 - val_mse: 0.4285\n",
      "Epoch 21/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4930 - mse: 0.4930 - val_loss: 0.4285 - val_mse: 0.4285\n",
      "Epoch 22/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.4289 - val_mse: 0.4289\n",
      "Epoch 23/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.4295 - val_mse: 0.4295\n",
      "Epoch 24/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4933 - mse: 0.4933 - val_loss: 0.4291 - val_mse: 0.4291\n",
      "Epoch 25/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.4295 - val_mse: 0.4295\n",
      "Epoch 26/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.4300 - val_mse: 0.4300\n",
      "Epoch 27/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4933 - mse: 0.4933 - val_loss: 0.4296 - val_mse: 0.4296\n",
      "Epoch 28/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4933 - mse: 0.4933 - val_loss: 0.4294 - val_mse: 0.4294\n",
      "Epoch 29/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.4293 - val_mse: 0.4293\n",
      "Epoch 30/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.4291 - val_mse: 0.4291\n",
      "Epoch 31/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.4301 - val_mse: 0.4301\n",
      "Epoch 32/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4933 - mse: 0.4933 - val_loss: 0.4293 - val_mse: 0.4293\n",
      "Epoch 33/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.4288 - val_mse: 0.4288\n",
      "Epoch 34/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.4290 - val_mse: 0.4290\n",
      "Epoch 35/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.4294 - val_mse: 0.4294\n",
      "Epoch 36/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.4286 - val_mse: 0.4286\n",
      "Epoch 37/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.4288 - val_mse: 0.4288\n",
      "Epoch 38/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4930 - mse: 0.4930 - val_loss: 0.4293 - val_mse: 0.4293\n",
      "Epoch 39/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.4294 - val_mse: 0.4294\n",
      "Epoch 40/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4930 - mse: 0.4930 - val_loss: 0.4288 - val_mse: 0.4288\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4288 - mse: 0.4288\n",
      "\n",
      "Weight: [array([[2.0131328]], dtype=float32), array([1.0137513], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/40\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8243 - mse: 0.8243 - val_loss: 0.9703 - val_mse: 0.9703\n",
      "Epoch 2/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8242 - mse: 0.8242 - val_loss: 0.9704 - val_mse: 0.9704\n",
      "Epoch 3/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8240 - mse: 0.8240 - val_loss: 0.9707 - val_mse: 0.9707\n",
      "Epoch 4/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8240 - mse: 0.8240 - val_loss: 0.9707 - val_mse: 0.9707\n",
      "Epoch 5/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8239 - mse: 0.8239 - val_loss: 0.9709 - val_mse: 0.9709\n",
      "Epoch 6/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8241 - mse: 0.8241 - val_loss: 0.9709 - val_mse: 0.9709\n",
      "Epoch 7/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8235 - mse: 0.8235 - val_loss: 0.9712 - val_mse: 0.9712\n",
      "Epoch 8/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8240 - mse: 0.8240 - val_loss: 0.9711 - val_mse: 0.9711\n",
      "Epoch 9/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8240 - mse: 0.8240 - val_loss: 0.9713 - val_mse: 0.9713\n",
      "Epoch 10/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8241 - mse: 0.8241 - val_loss: 0.9714 - val_mse: 0.9714\n",
      "Epoch 11/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8237 - mse: 0.8237 - val_loss: 0.9716 - val_mse: 0.9716\n",
      "Epoch 12/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8239 - mse: 0.8239 - val_loss: 0.9716 - val_mse: 0.9716\n",
      "Epoch 13/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8239 - mse: 0.8239 - val_loss: 0.9714 - val_mse: 0.9714\n",
      "Epoch 14/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8241 - mse: 0.8241 - val_loss: 0.9714 - val_mse: 0.9714\n",
      "Epoch 15/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8239 - mse: 0.8239 - val_loss: 0.9715 - val_mse: 0.9715\n",
      "Epoch 16/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8237 - mse: 0.8237 - val_loss: 0.9713 - val_mse: 0.9713\n",
      "Epoch 17/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8241 - mse: 0.8241 - val_loss: 0.9713 - val_mse: 0.9713\n",
      "Epoch 18/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8237 - mse: 0.8237 - val_loss: 0.9715 - val_mse: 0.9715\n",
      "Epoch 19/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8240 - mse: 0.8240 - val_loss: 0.9714 - val_mse: 0.9714\n",
      "Epoch 20/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8238 - mse: 0.8238 - val_loss: 0.9715 - val_mse: 0.9715\n",
      "Epoch 21/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8238 - mse: 0.8238 - val_loss: 0.9716 - val_mse: 0.9716\n",
      "Epoch 22/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8238 - mse: 0.8238 - val_loss: 0.9716 - val_mse: 0.9716\n",
      "Epoch 23/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8237 - mse: 0.8237 - val_loss: 0.9717 - val_mse: 0.9717\n",
      "Epoch 24/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8238 - mse: 0.8238 - val_loss: 0.9716 - val_mse: 0.9716\n",
      "Epoch 25/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8237 - mse: 0.8237 - val_loss: 0.9716 - val_mse: 0.9716\n",
      "Epoch 26/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8238 - mse: 0.8238 - val_loss: 0.9717 - val_mse: 0.9717\n",
      "Epoch 27/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8239 - mse: 0.8239 - val_loss: 0.9718 - val_mse: 0.9718\n",
      "Epoch 28/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8240 - mse: 0.8240 - val_loss: 0.9718 - val_mse: 0.9718\n",
      "Epoch 29/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8240 - mse: 0.8240 - val_loss: 0.9718 - val_mse: 0.9718\n",
      "Epoch 30/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8238 - mse: 0.8238 - val_loss: 0.9718 - val_mse: 0.9718\n",
      "Epoch 31/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8238 - mse: 0.8238 - val_loss: 0.9718 - val_mse: 0.9718\n",
      "Epoch 32/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8243 - mse: 0.8243 - val_loss: 0.9718 - val_mse: 0.9718\n",
      "Epoch 33/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8237 - mse: 0.8237 - val_loss: 0.9719 - val_mse: 0.9719\n",
      "Epoch 34/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8238 - mse: 0.8238 - val_loss: 0.9719 - val_mse: 0.9719\n",
      "Epoch 35/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8238 - mse: 0.8238 - val_loss: 0.9718 - val_mse: 0.9718\n",
      "Epoch 36/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8238 - mse: 0.8238 - val_loss: 0.9717 - val_mse: 0.9717\n",
      "Epoch 37/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8238 - mse: 0.8238 - val_loss: 0.9717 - val_mse: 0.9717\n",
      "Epoch 38/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8237 - mse: 0.8237 - val_loss: 0.9719 - val_mse: 0.9719\n",
      "Epoch 39/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8238 - mse: 0.8238 - val_loss: 0.9717 - val_mse: 0.9717\n",
      "Epoch 40/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8240 - mse: 0.8240 - val_loss: 0.9717 - val_mse: 0.9717\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.9717 - mse: 0.9717\n",
      "\n",
      "Weight: [array([[2.0471065]], dtype=float32), array([0.99589], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/40\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1541 - mse: 1.1541 - val_loss: 1.2299 - val_mse: 1.2299\n",
      "Epoch 2/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1512 - mse: 1.1512 - val_loss: 1.2368 - val_mse: 1.2368\n",
      "Epoch 3/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1498 - mse: 1.1498 - val_loss: 1.2420 - val_mse: 1.2420\n",
      "Epoch 4/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1488 - mse: 1.1488 - val_loss: 1.2444 - val_mse: 1.2444\n",
      "Epoch 5/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1490 - mse: 1.1490 - val_loss: 1.2462 - val_mse: 1.2462\n",
      "Epoch 6/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1485 - mse: 1.1485 - val_loss: 1.2483 - val_mse: 1.2483\n",
      "Epoch 7/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1483 - mse: 1.1483 - val_loss: 1.2508 - val_mse: 1.2508\n",
      "Epoch 8/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1476 - mse: 1.1476 - val_loss: 1.2517 - val_mse: 1.2517\n",
      "Epoch 9/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1473 - mse: 1.1473 - val_loss: 1.2513 - val_mse: 1.2513\n",
      "Epoch 10/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1476 - mse: 1.1476 - val_loss: 1.2519 - val_mse: 1.2519\n",
      "Epoch 11/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1473 - mse: 1.1473 - val_loss: 1.2539 - val_mse: 1.2539\n",
      "Epoch 12/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1477 - mse: 1.1477 - val_loss: 1.2536 - val_mse: 1.2536\n",
      "Epoch 13/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1473 - mse: 1.1473 - val_loss: 1.2529 - val_mse: 1.2529\n",
      "Epoch 14/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1474 - mse: 1.1474 - val_loss: 1.2542 - val_mse: 1.2542\n",
      "Epoch 15/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1475 - mse: 1.1475 - val_loss: 1.2542 - val_mse: 1.2542\n",
      "Epoch 16/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1474 - mse: 1.1474 - val_loss: 1.2531 - val_mse: 1.2531\n",
      "Epoch 17/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1472 - mse: 1.1472 - val_loss: 1.2540 - val_mse: 1.2540\n",
      "Epoch 18/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1478 - mse: 1.1478 - val_loss: 1.2510 - val_mse: 1.2510\n",
      "Epoch 19/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1474 - mse: 1.1474 - val_loss: 1.2510 - val_mse: 1.2510\n",
      "Epoch 20/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1472 - mse: 1.1472 - val_loss: 1.2519 - val_mse: 1.2519\n",
      "Epoch 21/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1473 - mse: 1.1473 - val_loss: 1.2514 - val_mse: 1.2514\n",
      "Epoch 22/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1473 - mse: 1.1473 - val_loss: 1.2527 - val_mse: 1.2527\n",
      "Epoch 23/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1473 - mse: 1.1473 - val_loss: 1.2516 - val_mse: 1.2516\n",
      "Epoch 24/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1475 - mse: 1.1475 - val_loss: 1.2530 - val_mse: 1.2530\n",
      "Epoch 25/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1475 - mse: 1.1475 - val_loss: 1.2534 - val_mse: 1.2534\n",
      "Epoch 26/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1475 - mse: 1.1475 - val_loss: 1.2530 - val_mse: 1.2530\n",
      "Epoch 27/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1475 - mse: 1.1475 - val_loss: 1.2539 - val_mse: 1.2539\n",
      "Epoch 28/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1474 - mse: 1.1474 - val_loss: 1.2542 - val_mse: 1.2542\n",
      "Epoch 29/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1471 - mse: 1.1471 - val_loss: 1.2527 - val_mse: 1.2527\n",
      "Epoch 30/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1472 - mse: 1.1472 - val_loss: 1.2526 - val_mse: 1.2526\n",
      "Epoch 31/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1471 - mse: 1.1471 - val_loss: 1.2509 - val_mse: 1.2509\n",
      "Epoch 32/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1472 - mse: 1.1472 - val_loss: 1.2505 - val_mse: 1.2505\n",
      "Epoch 33/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1473 - mse: 1.1473 - val_loss: 1.2518 - val_mse: 1.2518\n",
      "Epoch 34/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1475 - mse: 1.1475 - val_loss: 1.2510 - val_mse: 1.2510\n",
      "Epoch 35/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1475 - mse: 1.1475 - val_loss: 1.2517 - val_mse: 1.2517\n",
      "Epoch 36/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1474 - mse: 1.1474 - val_loss: 1.2523 - val_mse: 1.2523\n",
      "Epoch 37/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1474 - mse: 1.1474 - val_loss: 1.2534 - val_mse: 1.2534\n",
      "Epoch 38/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1475 - mse: 1.1475 - val_loss: 1.2564 - val_mse: 1.2564\n",
      "Epoch 39/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1472 - mse: 1.1472 - val_loss: 1.2568 - val_mse: 1.2568\n",
      "Epoch 40/40\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1472 - mse: 1.1472 - val_loss: 1.2552 - val_mse: 1.2552\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.2552 - mse: 1.2552\n",
      "\n",
      "Weight: [array([[1.9229785]], dtype=float32), array([1.0516418], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/40\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 2/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 3/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 4/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 5/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 6/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 7/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 8/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 9/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 10/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 11/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 12/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 13/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 14/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 15/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 16/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 17/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 18/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 19/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 20/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 21/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 22/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 23/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 24/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 25/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 26/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 27/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 28/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 29/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 30/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 31/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 32/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 33/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 34/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 35/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 36/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 37/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 38/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 39/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 40/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0073 - mse: 0.0073\n",
      "\n",
      "Weight: [array([[1.9963225]], dtype=float32), array([1.0011762], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0908 - mse: 0.0908 - val_loss: 0.1098 - val_mse: 0.1098\n",
      "Epoch 2/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1105 - val_mse: 0.1105\n",
      "Epoch 3/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0907 - mse: 0.0907 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 4/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0907 - mse: 0.0907 - val_loss: 0.1104 - val_mse: 0.1104\n",
      "Epoch 5/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1105 - val_mse: 0.1105\n",
      "Epoch 6/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0907 - mse: 0.0907 - val_loss: 0.1107 - val_mse: 0.1107\n",
      "Epoch 7/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1107 - val_mse: 0.1107\n",
      "Epoch 8/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1104 - val_mse: 0.1104\n",
      "Epoch 9/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1102 - val_mse: 0.1102\n",
      "Epoch 10/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1103 - val_mse: 0.1103\n",
      "Epoch 11/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 12/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 13/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 14/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1108 - val_mse: 0.1108\n",
      "Epoch 15/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1108 - val_mse: 0.1108\n",
      "Epoch 16/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 17/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1105 - val_mse: 0.1105\n",
      "Epoch 18/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 19/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1105 - val_mse: 0.1105\n",
      "Epoch 20/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1104 - val_mse: 0.1104\n",
      "Epoch 21/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 22/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1106 - val_mse: 0.1106\n",
      "Epoch 23/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1103 - val_mse: 0.1103\n",
      "Epoch 24/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1102 - val_mse: 0.1102\n",
      "Epoch 25/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1097 - val_mse: 0.1097\n",
      "Epoch 26/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1102 - val_mse: 0.1102\n",
      "Epoch 27/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1105 - val_mse: 0.1105\n",
      "Epoch 28/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1105 - val_mse: 0.1105\n",
      "Epoch 29/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 30/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 31/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 32/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 33/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 34/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 35/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 36/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1113 - val_mse: 0.1113\n",
      "Epoch 37/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0907 - mse: 0.0907 - val_loss: 0.1108 - val_mse: 0.1108\n",
      "Epoch 38/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 39/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1106 - val_mse: 0.1106\n",
      "Epoch 40/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1106 - val_mse: 0.1106\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.1106 - mse: 0.1106\n",
      "\n",
      "Weight: [array([[1.9841887]], dtype=float32), array([0.99014497], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/40\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2318 - val_mse: 0.2318\n",
      "Epoch 2/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.2315 - val_mse: 0.2315\n",
      "Epoch 3/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2317 - val_mse: 0.2317\n",
      "Epoch 4/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.2317 - val_mse: 0.2317\n",
      "Epoch 5/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2316 - val_mse: 0.2316\n",
      "Epoch 6/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2318 - val_mse: 0.2318\n",
      "Epoch 7/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2316 - val_mse: 0.2316\n",
      "Epoch 8/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.2318 - val_mse: 0.2318\n",
      "Epoch 9/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2317 - val_mse: 0.2317\n",
      "Epoch 10/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.2318 - val_mse: 0.2318\n",
      "Epoch 11/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.2316 - val_mse: 0.2316\n",
      "Epoch 12/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.2319 - val_mse: 0.2319\n",
      "Epoch 13/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.2320 - val_mse: 0.2320\n",
      "Epoch 14/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2320 - val_mse: 0.2320\n",
      "Epoch 15/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2318 - val_mse: 0.2318\n",
      "Epoch 16/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.2315 - val_mse: 0.2315\n",
      "Epoch 17/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2319 - val_mse: 0.2319\n",
      "Epoch 18/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2318 - val_mse: 0.2318\n",
      "Epoch 19/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.2315 - val_mse: 0.2315\n",
      "Epoch 20/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2316 - val_mse: 0.2316\n",
      "Epoch 21/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.2317 - val_mse: 0.2317\n",
      "Epoch 22/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2317 - val_mse: 0.2317\n",
      "Epoch 23/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.2316 - val_mse: 0.2316\n",
      "Epoch 24/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.2313 - val_mse: 0.2313\n",
      "Epoch 25/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2313 - val_mse: 0.2313\n",
      "Epoch 26/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2313 - val_mse: 0.2313\n",
      "Epoch 27/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2314 - val_mse: 0.2314\n",
      "Epoch 28/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2314 - val_mse: 0.2314\n",
      "Epoch 29/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2314 - val_mse: 0.2314\n",
      "Epoch 30/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2597 - mse: 0.2597 - val_loss: 0.2315 - val_mse: 0.2315\n",
      "Epoch 31/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.2317 - val_mse: 0.2317\n",
      "Epoch 32/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2316 - val_mse: 0.2316\n",
      "Epoch 33/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.2319 - val_mse: 0.2319\n",
      "Epoch 34/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.2321 - val_mse: 0.2321\n",
      "Epoch 35/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2319 - val_mse: 0.2319\n",
      "Epoch 36/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2322 - val_mse: 0.2322\n",
      "Epoch 37/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2320 - val_mse: 0.2320\n",
      "Epoch 38/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.2318 - val_mse: 0.2318\n",
      "Epoch 39/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2318 - val_mse: 0.2318\n",
      "Epoch 40/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.2316 - val_mse: 0.2316\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2316 - mse: 0.2316\n",
      "\n",
      "Weight: [array([[1.9704641]], dtype=float32), array([0.9982065], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/40\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4967 - mse: 0.4967 - val_loss: 0.5160 - val_mse: 0.5160\n",
      "Epoch 2/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4946 - mse: 0.4946 - val_loss: 0.5147 - val_mse: 0.5147\n",
      "Epoch 3/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4939 - mse: 0.4939 - val_loss: 0.5132 - val_mse: 0.5132\n",
      "Epoch 4/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4935 - mse: 0.4935 - val_loss: 0.5128 - val_mse: 0.5128\n",
      "Epoch 5/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4935 - mse: 0.4935 - val_loss: 0.5134 - val_mse: 0.5134\n",
      "Epoch 6/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4933 - mse: 0.4933 - val_loss: 0.5136 - val_mse: 0.5136\n",
      "Epoch 7/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4933 - mse: 0.4933 - val_loss: 0.5129 - val_mse: 0.5129\n",
      "Epoch 8/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.5130 - val_mse: 0.5130\n",
      "Epoch 9/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.5132 - val_mse: 0.5132\n",
      "Epoch 10/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4933 - mse: 0.4933 - val_loss: 0.5124 - val_mse: 0.5124\n",
      "Epoch 11/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.5124 - val_mse: 0.5124\n",
      "Epoch 12/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.5123 - val_mse: 0.5123\n",
      "Epoch 13/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.5125 - val_mse: 0.5125\n",
      "Epoch 14/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.5125 - val_mse: 0.5125\n",
      "Epoch 15/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4933 - mse: 0.4933 - val_loss: 0.5126 - val_mse: 0.5126\n",
      "Epoch 16/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4933 - mse: 0.4933 - val_loss: 0.5123 - val_mse: 0.5123\n",
      "Epoch 17/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4933 - mse: 0.4933 - val_loss: 0.5128 - val_mse: 0.5128\n",
      "Epoch 18/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.5131 - val_mse: 0.5131\n",
      "Epoch 19/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.5131 - val_mse: 0.5131\n",
      "Epoch 20/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.5132 - val_mse: 0.5132\n",
      "Epoch 21/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.5129 - val_mse: 0.5129\n",
      "Epoch 22/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4933 - mse: 0.4933 - val_loss: 0.5126 - val_mse: 0.5126\n",
      "Epoch 23/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4930 - mse: 0.4930 - val_loss: 0.5127 - val_mse: 0.5127\n",
      "Epoch 24/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.5132 - val_mse: 0.5132\n",
      "Epoch 25/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4930 - mse: 0.4930 - val_loss: 0.5127 - val_mse: 0.5127\n",
      "Epoch 26/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4933 - mse: 0.4933 - val_loss: 0.5131 - val_mse: 0.5131\n",
      "Epoch 27/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.5127 - val_mse: 0.5127\n",
      "Epoch 28/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4933 - mse: 0.4933 - val_loss: 0.5127 - val_mse: 0.5127\n",
      "Epoch 29/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.5129 - val_mse: 0.5129\n",
      "Epoch 30/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.5125 - val_mse: 0.5125\n",
      "Epoch 31/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4933 - mse: 0.4933 - val_loss: 0.5125 - val_mse: 0.5125\n",
      "Epoch 32/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.5132 - val_mse: 0.5132\n",
      "Epoch 33/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.5130 - val_mse: 0.5130\n",
      "Epoch 34/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.5129 - val_mse: 0.5129\n",
      "Epoch 35/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.5136 - val_mse: 0.5136\n",
      "Epoch 36/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.5134 - val_mse: 0.5134\n",
      "Epoch 37/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.5130 - val_mse: 0.5130\n",
      "Epoch 38/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.5128 - val_mse: 0.5128\n",
      "Epoch 39/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4933 - mse: 0.4933 - val_loss: 0.5131 - val_mse: 0.5131\n",
      "Epoch 40/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4932 - mse: 0.4932 - val_loss: 0.5126 - val_mse: 0.5126\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5126 - mse: 0.5126\n",
      "\n",
      "Weight: [array([[1.8955246]], dtype=float32), array([1.0601094], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/40\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.8057 - mse: 0.8057 - val_loss: 0.7703 - val_mse: 0.7703\n",
      "Epoch 2/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8032 - mse: 0.8032 - val_loss: 0.7764 - val_mse: 0.7764\n",
      "Epoch 3/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8011 - mse: 0.8011 - val_loss: 0.7761 - val_mse: 0.7761\n",
      "Epoch 4/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8007 - mse: 0.8007 - val_loss: 0.7770 - val_mse: 0.7770\n",
      "Epoch 5/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8008 - mse: 0.8008 - val_loss: 0.7770 - val_mse: 0.7770\n",
      "Epoch 6/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8008 - mse: 0.8008 - val_loss: 0.7784 - val_mse: 0.7784\n",
      "Epoch 7/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8006 - mse: 0.8006 - val_loss: 0.7814 - val_mse: 0.7814\n",
      "Epoch 8/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8005 - mse: 0.8005 - val_loss: 0.7836 - val_mse: 0.7836\n",
      "Epoch 9/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8008 - mse: 0.8008 - val_loss: 0.7871 - val_mse: 0.7871\n",
      "Epoch 10/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8007 - mse: 0.8007 - val_loss: 0.7855 - val_mse: 0.7855\n",
      "Epoch 11/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8005 - mse: 0.8005 - val_loss: 0.7810 - val_mse: 0.7810\n",
      "Epoch 12/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8008 - mse: 0.8008 - val_loss: 0.7807 - val_mse: 0.7807\n",
      "Epoch 13/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8008 - mse: 0.8008 - val_loss: 0.7802 - val_mse: 0.7802\n",
      "Epoch 14/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8009 - mse: 0.8009 - val_loss: 0.7779 - val_mse: 0.7779\n",
      "Epoch 15/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8006 - mse: 0.8006 - val_loss: 0.7794 - val_mse: 0.7794\n",
      "Epoch 16/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8009 - mse: 0.8009 - val_loss: 0.7830 - val_mse: 0.7830\n",
      "Epoch 17/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8004 - mse: 0.8004 - val_loss: 0.7858 - val_mse: 0.7858\n",
      "Epoch 18/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8006 - mse: 0.8006 - val_loss: 0.7802 - val_mse: 0.7802\n",
      "Epoch 19/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8006 - mse: 0.8006 - val_loss: 0.7828 - val_mse: 0.7828\n",
      "Epoch 20/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8005 - mse: 0.8005 - val_loss: 0.7844 - val_mse: 0.7844\n",
      "Epoch 21/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8005 - mse: 0.8005 - val_loss: 0.7877 - val_mse: 0.7877\n",
      "Epoch 22/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8010 - mse: 0.8010 - val_loss: 0.7864 - val_mse: 0.7864\n",
      "Epoch 23/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8007 - mse: 0.8007 - val_loss: 0.7833 - val_mse: 0.7833\n",
      "Epoch 24/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8007 - mse: 0.8007 - val_loss: 0.7844 - val_mse: 0.7844\n",
      "Epoch 25/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8007 - mse: 0.8007 - val_loss: 0.7873 - val_mse: 0.7873\n",
      "Epoch 26/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8007 - mse: 0.8007 - val_loss: 0.7863 - val_mse: 0.7863\n",
      "Epoch 27/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8008 - mse: 0.8008 - val_loss: 0.7881 - val_mse: 0.7881\n",
      "Epoch 28/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8009 - mse: 0.8009 - val_loss: 0.7876 - val_mse: 0.7876\n",
      "Epoch 29/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8008 - mse: 0.8008 - val_loss: 0.7850 - val_mse: 0.7850\n",
      "Epoch 30/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8006 - mse: 0.8006 - val_loss: 0.7862 - val_mse: 0.7862\n",
      "Epoch 31/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8006 - mse: 0.8006 - val_loss: 0.7898 - val_mse: 0.7898\n",
      "Epoch 32/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8009 - mse: 0.8009 - val_loss: 0.7870 - val_mse: 0.7870\n",
      "Epoch 33/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8009 - mse: 0.8009 - val_loss: 0.7865 - val_mse: 0.7865\n",
      "Epoch 34/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8007 - mse: 0.8007 - val_loss: 0.7858 - val_mse: 0.7858\n",
      "Epoch 35/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8012 - mse: 0.8012 - val_loss: 0.7844 - val_mse: 0.7844\n",
      "Epoch 36/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8006 - mse: 0.8006 - val_loss: 0.7834 - val_mse: 0.7834\n",
      "Epoch 37/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8006 - mse: 0.8006 - val_loss: 0.7833 - val_mse: 0.7833\n",
      "Epoch 38/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8007 - mse: 0.8007 - val_loss: 0.7861 - val_mse: 0.7861\n",
      "Epoch 39/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8009 - mse: 0.8009 - val_loss: 0.7843 - val_mse: 0.7843\n",
      "Epoch 40/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8008 - mse: 0.8008 - val_loss: 0.7906 - val_mse: 0.7906\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.7906 - mse: 0.7906\n",
      "\n",
      "Weight: [array([[1.8888438]], dtype=float32), array([0.9496154], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/40\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 1.2001 - mse: 1.2001 - val_loss: 1.1565 - val_mse: 1.1565\n",
      "Epoch 2/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1982 - mse: 1.1982 - val_loss: 1.1557 - val_mse: 1.1557\n",
      "Epoch 3/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1978 - mse: 1.1978 - val_loss: 1.1443 - val_mse: 1.1443\n",
      "Epoch 4/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1967 - mse: 1.1967 - val_loss: 1.1419 - val_mse: 1.1419\n",
      "Epoch 5/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1962 - mse: 1.1962 - val_loss: 1.1427 - val_mse: 1.1427\n",
      "Epoch 6/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1959 - mse: 1.1959 - val_loss: 1.1397 - val_mse: 1.1397\n",
      "Epoch 7/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1954 - mse: 1.1954 - val_loss: 1.1377 - val_mse: 1.1377\n",
      "Epoch 8/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1953 - mse: 1.1953 - val_loss: 1.1365 - val_mse: 1.1365\n",
      "Epoch 9/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1953 - mse: 1.1953 - val_loss: 1.1416 - val_mse: 1.1416\n",
      "Epoch 10/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1959 - mse: 1.1959 - val_loss: 1.1422 - val_mse: 1.1422\n",
      "Epoch 11/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1958 - mse: 1.1958 - val_loss: 1.1447 - val_mse: 1.1447\n",
      "Epoch 12/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1961 - mse: 1.1961 - val_loss: 1.1368 - val_mse: 1.1368\n",
      "Epoch 13/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1955 - mse: 1.1955 - val_loss: 1.1333 - val_mse: 1.1333\n",
      "Epoch 14/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1957 - mse: 1.1957 - val_loss: 1.1341 - val_mse: 1.1341\n",
      "Epoch 15/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1960 - mse: 1.1960 - val_loss: 1.1392 - val_mse: 1.1392\n",
      "Epoch 16/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1957 - mse: 1.1957 - val_loss: 1.1419 - val_mse: 1.1419\n",
      "Epoch 17/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1959 - mse: 1.1959 - val_loss: 1.1452 - val_mse: 1.1452\n",
      "Epoch 18/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1959 - mse: 1.1959 - val_loss: 1.1424 - val_mse: 1.1424\n",
      "Epoch 19/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1960 - mse: 1.1960 - val_loss: 1.1378 - val_mse: 1.1378\n",
      "Epoch 20/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1957 - mse: 1.1957 - val_loss: 1.1352 - val_mse: 1.1352\n",
      "Epoch 21/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1954 - mse: 1.1954 - val_loss: 1.1344 - val_mse: 1.1344\n",
      "Epoch 22/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1956 - mse: 1.1956 - val_loss: 1.1360 - val_mse: 1.1360\n",
      "Epoch 23/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1958 - mse: 1.1958 - val_loss: 1.1370 - val_mse: 1.1370\n",
      "Epoch 24/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1957 - mse: 1.1957 - val_loss: 1.1351 - val_mse: 1.1351\n",
      "Epoch 25/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1959 - mse: 1.1959 - val_loss: 1.1314 - val_mse: 1.1314\n",
      "Epoch 26/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1957 - mse: 1.1957 - val_loss: 1.1334 - val_mse: 1.1334\n",
      "Epoch 27/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1956 - mse: 1.1956 - val_loss: 1.1339 - val_mse: 1.1339\n",
      "Epoch 28/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1955 - mse: 1.1955 - val_loss: 1.1342 - val_mse: 1.1342\n",
      "Epoch 29/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1955 - mse: 1.1955 - val_loss: 1.1342 - val_mse: 1.1342\n",
      "Epoch 30/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1955 - mse: 1.1955 - val_loss: 1.1335 - val_mse: 1.1335\n",
      "Epoch 31/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1955 - mse: 1.1955 - val_loss: 1.1335 - val_mse: 1.1335\n",
      "Epoch 32/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1959 - mse: 1.1959 - val_loss: 1.1338 - val_mse: 1.1338\n",
      "Epoch 33/40\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.1956 - mse: 1.1956 - val_loss: 1.1381 - val_mse: 1.1381\n",
      "Epoch 34/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1958 - mse: 1.1958 - val_loss: 1.1323 - val_mse: 1.1323\n",
      "Epoch 35/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1955 - mse: 1.1955 - val_loss: 1.1348 - val_mse: 1.1348\n",
      "Epoch 36/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1958 - mse: 1.1958 - val_loss: 1.1337 - val_mse: 1.1337\n",
      "Epoch 37/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1956 - mse: 1.1956 - val_loss: 1.1321 - val_mse: 1.1321\n",
      "Epoch 38/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1960 - mse: 1.1960 - val_loss: 1.1315 - val_mse: 1.1315\n",
      "Epoch 39/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1962 - mse: 1.1962 - val_loss: 1.1343 - val_mse: 1.1343\n",
      "Epoch 40/40\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1955 - mse: 1.1955 - val_loss: 1.1329 - val_mse: 1.1329\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.1329 - mse: 1.1329\n",
      "\n",
      "Weight: [array([[2.0268686]], dtype=float32), array([0.947763], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 5/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 6/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 7/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 8/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 9/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 10/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 11/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 12/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 13/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 14/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 15/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 16/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 17/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 18/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 19/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 20/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 21/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 22/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 23/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 24/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 25/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 26/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 27/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 28/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 29/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 30/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 31/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 32/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 33/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 34/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 35/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 36/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 37/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 38/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 39/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 40/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0103 - mse: 0.0103\n",
      "\n",
      "Weight: [array([[2.007889]], dtype=float32), array([1.0025448], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.0761 - val_mse: 0.0761\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0764 - val_mse: 0.0764\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0766 - val_mse: 0.0766\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.0766 - val_mse: 0.0766\n",
      "Epoch 5/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.0766 - val_mse: 0.0766\n",
      "Epoch 6/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 7/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0766 - val_mse: 0.0766\n",
      "Epoch 8/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 9/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 10/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0770 - val_mse: 0.0770\n",
      "Epoch 11/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0769 - val_mse: 0.0769\n",
      "Epoch 12/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 13/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 14/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 15/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0766 - val_mse: 0.0766\n",
      "Epoch 16/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 17/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 18/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 19/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 20/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 21/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0769 - val_mse: 0.0769\n",
      "Epoch 22/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 23/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0770 - val_mse: 0.0770\n",
      "Epoch 24/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 25/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 26/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 27/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 28/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0769 - val_mse: 0.0769\n",
      "Epoch 29/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.0770 - val_mse: 0.0770\n",
      "Epoch 30/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.0769 - val_mse: 0.0769\n",
      "Epoch 31/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.0769 - val_mse: 0.0769\n",
      "Epoch 32/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 33/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 34/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.0766 - val_mse: 0.0766\n",
      "Epoch 35/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 36/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.0769 - val_mse: 0.0769\n",
      "Epoch 37/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0769 - val_mse: 0.0769\n",
      "Epoch 38/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0769 - val_mse: 0.0769\n",
      "Epoch 39/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0770 - val_mse: 0.0770\n",
      "Epoch 40/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0768 - mse: 0.0768\n",
      "\n",
      "Weight: [array([[2.0145102]], dtype=float32), array([0.9866899], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2868 - val_mse: 0.2868\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2857 - val_mse: 0.2857\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2845 - val_mse: 0.2845\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2855 - val_mse: 0.2855\n",
      "Epoch 5/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2860 - val_mse: 0.2860\n",
      "Epoch 6/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2855 - val_mse: 0.2855\n",
      "Epoch 7/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2844 - val_mse: 0.2844\n",
      "Epoch 8/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2852 - val_mse: 0.2852\n",
      "Epoch 9/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2844 - val_mse: 0.2844\n",
      "Epoch 10/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2835 - val_mse: 0.2835\n",
      "Epoch 11/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2846 - val_mse: 0.2846\n",
      "Epoch 12/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2843 - val_mse: 0.2843\n",
      "Epoch 13/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2850 - val_mse: 0.2850\n",
      "Epoch 14/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2854 - val_mse: 0.2854\n",
      "Epoch 15/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2846 - val_mse: 0.2846\n",
      "Epoch 16/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2840 - val_mse: 0.2840\n",
      "Epoch 17/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2842 - val_mse: 0.2842\n",
      "Epoch 18/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2844 - val_mse: 0.2844\n",
      "Epoch 19/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2839 - val_mse: 0.2839\n",
      "Epoch 20/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2841 - val_mse: 0.2841\n",
      "Epoch 21/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2855 - val_mse: 0.2855\n",
      "Epoch 22/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2849 - val_mse: 0.2849\n",
      "Epoch 23/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2845 - val_mse: 0.2845\n",
      "Epoch 24/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2855 - val_mse: 0.2855\n",
      "Epoch 25/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2850 - val_mse: 0.2850\n",
      "Epoch 26/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2847 - val_mse: 0.2847\n",
      "Epoch 27/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2863 - val_mse: 0.2863\n",
      "Epoch 28/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2868 - val_mse: 0.2868\n",
      "Epoch 29/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2843 - val_mse: 0.2843\n",
      "Epoch 30/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2845 - val_mse: 0.2845\n",
      "Epoch 31/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2843 - val_mse: 0.2843\n",
      "Epoch 32/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2853 - val_mse: 0.2853\n",
      "Epoch 33/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2835 - val_mse: 0.2835\n",
      "Epoch 34/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2842 - val_mse: 0.2842\n",
      "Epoch 35/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2840 - val_mse: 0.2840\n",
      "Epoch 36/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2837 - val_mse: 0.2837\n",
      "Epoch 37/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2846 - val_mse: 0.2846\n",
      "Epoch 38/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2829 - val_mse: 0.2829\n",
      "Epoch 39/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.2824 - val_mse: 0.2824\n",
      "Epoch 40/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.2829 - val_mse: 0.2829\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2829 - mse: 0.2829\n",
      "\n",
      "Weight: [array([[1.99434]], dtype=float32), array([0.9981749], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4796 - mse: 0.4796 - val_loss: 0.3694 - val_mse: 0.3694\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4791 - mse: 0.4791 - val_loss: 0.3688 - val_mse: 0.3688\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4790 - mse: 0.4790 - val_loss: 0.3672 - val_mse: 0.3672\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4790 - mse: 0.4790 - val_loss: 0.3653 - val_mse: 0.3653\n",
      "Epoch 5/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4787 - mse: 0.4787 - val_loss: 0.3645 - val_mse: 0.3645\n",
      "Epoch 6/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.3637 - val_mse: 0.3637\n",
      "Epoch 7/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4787 - mse: 0.4787 - val_loss: 0.3643 - val_mse: 0.3643\n",
      "Epoch 8/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4787 - mse: 0.4787 - val_loss: 0.3640 - val_mse: 0.3640\n",
      "Epoch 9/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.3632 - val_mse: 0.3632\n",
      "Epoch 10/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4787 - mse: 0.4787 - val_loss: 0.3652 - val_mse: 0.3652\n",
      "Epoch 11/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.3638 - val_mse: 0.3638\n",
      "Epoch 12/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.3633 - val_mse: 0.3633\n",
      "Epoch 13/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.3634 - val_mse: 0.3634\n",
      "Epoch 14/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.3641 - val_mse: 0.3641\n",
      "Epoch 15/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.3638 - val_mse: 0.3638\n",
      "Epoch 16/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4787 - mse: 0.4787 - val_loss: 0.3633 - val_mse: 0.3633\n",
      "Epoch 17/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.3634 - val_mse: 0.3634\n",
      "Epoch 18/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4787 - mse: 0.4787 - val_loss: 0.3627 - val_mse: 0.3627\n",
      "Epoch 19/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4787 - mse: 0.4787 - val_loss: 0.3641 - val_mse: 0.3641\n",
      "Epoch 20/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.3645 - val_mse: 0.3645\n",
      "Epoch 21/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4786 - mse: 0.4786 - val_loss: 0.3649 - val_mse: 0.3649\n",
      "Epoch 22/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.3639 - val_mse: 0.3639\n",
      "Epoch 23/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.3639 - val_mse: 0.3639\n",
      "Epoch 24/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4787 - mse: 0.4787 - val_loss: 0.3651 - val_mse: 0.3651\n",
      "Epoch 25/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.3646 - val_mse: 0.3646\n",
      "Epoch 26/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.3634 - val_mse: 0.3634\n",
      "Epoch 27/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.3632 - val_mse: 0.3632\n",
      "Epoch 28/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4786 - mse: 0.4786 - val_loss: 0.3642 - val_mse: 0.3642\n",
      "Epoch 29/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4787 - mse: 0.4787 - val_loss: 0.3632 - val_mse: 0.3632\n",
      "Epoch 30/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4787 - mse: 0.4787 - val_loss: 0.3637 - val_mse: 0.3637\n",
      "Epoch 31/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.3641 - val_mse: 0.3641\n",
      "Epoch 32/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4789 - mse: 0.4789 - val_loss: 0.3639 - val_mse: 0.3639\n",
      "Epoch 33/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4787 - mse: 0.4787 - val_loss: 0.3638 - val_mse: 0.3638\n",
      "Epoch 34/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4787 - mse: 0.4787 - val_loss: 0.3642 - val_mse: 0.3642\n",
      "Epoch 35/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4787 - mse: 0.4787 - val_loss: 0.3630 - val_mse: 0.3630\n",
      "Epoch 36/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4789 - mse: 0.4789 - val_loss: 0.3628 - val_mse: 0.3628\n",
      "Epoch 37/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.3634 - val_mse: 0.3634\n",
      "Epoch 38/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.3634 - val_mse: 0.3634\n",
      "Epoch 39/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4786 - mse: 0.4786 - val_loss: 0.3624 - val_mse: 0.3624\n",
      "Epoch 40/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4789 - mse: 0.4789 - val_loss: 0.3625 - val_mse: 0.3625\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.3625 - mse: 0.3625\n",
      "\n",
      "Weight: [array([[1.9655917]], dtype=float32), array([0.95925224], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/40\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.7493 - mse: 0.7493 - val_loss: 0.9800 - val_mse: 0.9800\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7483 - mse: 0.7483 - val_loss: 0.9806 - val_mse: 0.9806\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7477 - mse: 0.7477 - val_loss: 0.9799 - val_mse: 0.9799\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7474 - mse: 0.7474 - val_loss: 0.9785 - val_mse: 0.9785\n",
      "Epoch 5/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7474 - mse: 0.7474 - val_loss: 0.9775 - val_mse: 0.9775\n",
      "Epoch 6/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7473 - mse: 0.7473 - val_loss: 0.9767 - val_mse: 0.9767\n",
      "Epoch 7/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7471 - mse: 0.7471 - val_loss: 0.9752 - val_mse: 0.9752\n",
      "Epoch 8/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7471 - mse: 0.7471 - val_loss: 0.9753 - val_mse: 0.9753\n",
      "Epoch 9/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7470 - mse: 0.7470 - val_loss: 0.9730 - val_mse: 0.9730\n",
      "Epoch 10/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7471 - mse: 0.7471 - val_loss: 0.9732 - val_mse: 0.9732\n",
      "Epoch 11/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7472 - mse: 0.7472 - val_loss: 0.9733 - val_mse: 0.9733\n",
      "Epoch 12/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7472 - mse: 0.7472 - val_loss: 0.9732 - val_mse: 0.9732\n",
      "Epoch 13/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7472 - mse: 0.7472 - val_loss: 0.9745 - val_mse: 0.9745\n",
      "Epoch 14/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7470 - mse: 0.7470 - val_loss: 0.9730 - val_mse: 0.9730\n",
      "Epoch 15/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7471 - mse: 0.7471 - val_loss: 0.9734 - val_mse: 0.9734\n",
      "Epoch 16/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7470 - mse: 0.7470 - val_loss: 0.9731 - val_mse: 0.9731\n",
      "Epoch 17/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7470 - mse: 0.7470 - val_loss: 0.9728 - val_mse: 0.9728\n",
      "Epoch 18/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7473 - mse: 0.7473 - val_loss: 0.9736 - val_mse: 0.9736\n",
      "Epoch 19/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7471 - mse: 0.7471 - val_loss: 0.9760 - val_mse: 0.9760\n",
      "Epoch 20/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7471 - mse: 0.7471 - val_loss: 0.9739 - val_mse: 0.9739\n",
      "Epoch 21/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7471 - mse: 0.7471 - val_loss: 0.9757 - val_mse: 0.9757\n",
      "Epoch 22/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7472 - mse: 0.7472 - val_loss: 0.9756 - val_mse: 0.9756\n",
      "Epoch 23/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7471 - mse: 0.7471 - val_loss: 0.9743 - val_mse: 0.9743\n",
      "Epoch 24/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7472 - mse: 0.7472 - val_loss: 0.9744 - val_mse: 0.9744\n",
      "Epoch 25/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7470 - mse: 0.7470 - val_loss: 0.9756 - val_mse: 0.9756\n",
      "Epoch 26/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7471 - mse: 0.7471 - val_loss: 0.9758 - val_mse: 0.9758\n",
      "Epoch 27/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7469 - mse: 0.7469 - val_loss: 0.9766 - val_mse: 0.9766\n",
      "Epoch 28/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7474 - mse: 0.7474 - val_loss: 0.9749 - val_mse: 0.9749\n",
      "Epoch 29/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7472 - mse: 0.7472 - val_loss: 0.9745 - val_mse: 0.9745\n",
      "Epoch 30/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7471 - mse: 0.7471 - val_loss: 0.9741 - val_mse: 0.9741\n",
      "Epoch 31/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7473 - mse: 0.7473 - val_loss: 0.9728 - val_mse: 0.9728\n",
      "Epoch 32/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7473 - mse: 0.7473 - val_loss: 0.9723 - val_mse: 0.9723\n",
      "Epoch 33/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7474 - mse: 0.7474 - val_loss: 0.9717 - val_mse: 0.9717\n",
      "Epoch 34/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7473 - mse: 0.7473 - val_loss: 0.9724 - val_mse: 0.9724\n",
      "Epoch 35/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7472 - mse: 0.7472 - val_loss: 0.9717 - val_mse: 0.9717\n",
      "Epoch 36/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7471 - mse: 0.7471 - val_loss: 0.9736 - val_mse: 0.9736\n",
      "Epoch 37/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7470 - mse: 0.7470 - val_loss: 0.9728 - val_mse: 0.9728\n",
      "Epoch 38/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7471 - mse: 0.7471 - val_loss: 0.9728 - val_mse: 0.9728\n",
      "Epoch 39/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7472 - mse: 0.7472 - val_loss: 0.9742 - val_mse: 0.9742\n",
      "Epoch 40/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7472 - mse: 0.7472 - val_loss: 0.9742 - val_mse: 0.9742\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.9742 - mse: 0.9742\n",
      "\n",
      "Weight: [array([[2.0363634]], dtype=float32), array([0.9995061], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1067 - mse: 1.1067 - val_loss: 1.3444 - val_mse: 1.3444\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1062 - mse: 1.1062 - val_loss: 1.3453 - val_mse: 1.3453\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1059 - mse: 1.1059 - val_loss: 1.3509 - val_mse: 1.3509\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1054 - mse: 1.1054 - val_loss: 1.3536 - val_mse: 1.3536\n",
      "Epoch 5/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1052 - mse: 1.1052 - val_loss: 1.3543 - val_mse: 1.3543\n",
      "Epoch 6/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1048 - mse: 1.1048 - val_loss: 1.3552 - val_mse: 1.3552\n",
      "Epoch 7/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1048 - mse: 1.1048 - val_loss: 1.3556 - val_mse: 1.3556\n",
      "Epoch 8/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1049 - mse: 1.1049 - val_loss: 1.3565 - val_mse: 1.3565\n",
      "Epoch 9/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1052 - mse: 1.1052 - val_loss: 1.3553 - val_mse: 1.3553\n",
      "Epoch 10/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1052 - mse: 1.1052 - val_loss: 1.3566 - val_mse: 1.3566\n",
      "Epoch 11/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1051 - mse: 1.1051 - val_loss: 1.3631 - val_mse: 1.3631\n",
      "Epoch 12/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1050 - mse: 1.1050 - val_loss: 1.3653 - val_mse: 1.3653\n",
      "Epoch 13/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1053 - mse: 1.1053 - val_loss: 1.3616 - val_mse: 1.3616\n",
      "Epoch 14/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1049 - mse: 1.1049 - val_loss: 1.3628 - val_mse: 1.3628\n",
      "Epoch 15/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1051 - mse: 1.1051 - val_loss: 1.3599 - val_mse: 1.3599\n",
      "Epoch 16/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1048 - mse: 1.1048 - val_loss: 1.3597 - val_mse: 1.3597\n",
      "Epoch 17/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1052 - mse: 1.1052 - val_loss: 1.3572 - val_mse: 1.3572\n",
      "Epoch 18/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1049 - mse: 1.1049 - val_loss: 1.3625 - val_mse: 1.3625\n",
      "Epoch 19/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1050 - mse: 1.1050 - val_loss: 1.3646 - val_mse: 1.3646\n",
      "Epoch 20/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1050 - mse: 1.1050 - val_loss: 1.3660 - val_mse: 1.3660\n",
      "Epoch 21/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1053 - mse: 1.1053 - val_loss: 1.3667 - val_mse: 1.3667\n",
      "Epoch 22/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1051 - mse: 1.1051 - val_loss: 1.3604 - val_mse: 1.3604\n",
      "Epoch 23/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1051 - mse: 1.1051 - val_loss: 1.3583 - val_mse: 1.3583\n",
      "Epoch 24/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1049 - mse: 1.1049 - val_loss: 1.3612 - val_mse: 1.3612\n",
      "Epoch 25/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1048 - mse: 1.1048 - val_loss: 1.3608 - val_mse: 1.3608\n",
      "Epoch 26/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1049 - mse: 1.1049 - val_loss: 1.3586 - val_mse: 1.3586\n",
      "Epoch 27/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1051 - mse: 1.1051 - val_loss: 1.3593 - val_mse: 1.3593\n",
      "Epoch 28/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1048 - mse: 1.1048 - val_loss: 1.3612 - val_mse: 1.3612\n",
      "Epoch 29/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1050 - mse: 1.1050 - val_loss: 1.3614 - val_mse: 1.3614\n",
      "Epoch 30/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1048 - mse: 1.1048 - val_loss: 1.3601 - val_mse: 1.3601\n",
      "Epoch 31/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1046 - mse: 1.1046 - val_loss: 1.3612 - val_mse: 1.3612\n",
      "Epoch 32/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1048 - mse: 1.1048 - val_loss: 1.3612 - val_mse: 1.3612\n",
      "Epoch 33/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1047 - mse: 1.1047 - val_loss: 1.3604 - val_mse: 1.3604\n",
      "Epoch 34/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1049 - mse: 1.1049 - val_loss: 1.3626 - val_mse: 1.3626\n",
      "Epoch 35/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1048 - mse: 1.1048 - val_loss: 1.3630 - val_mse: 1.3630\n",
      "Epoch 36/40\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1050 - mse: 1.1050 - val_loss: 1.3638 - val_mse: 1.3638\n",
      "Epoch 37/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1048 - mse: 1.1048 - val_loss: 1.3622 - val_mse: 1.3622\n",
      "Epoch 38/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1047 - mse: 1.1047 - val_loss: 1.3603 - val_mse: 1.3603\n",
      "Epoch 39/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1051 - mse: 1.1051 - val_loss: 1.3615 - val_mse: 1.3615\n",
      "Epoch 40/40\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1046 - mse: 1.1046 - val_loss: 1.3579 - val_mse: 1.3579\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3579 - mse: 1.3579\n",
      "\n",
      "Weight: [array([[1.9559534]], dtype=float32), array([1.0039011], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "N_train = np.zeros(6)\n",
    "sigma = np.zeros(6)\n",
    "score40 = np.zeros((6,6,2))\n",
    "weight40 = np.zeros((6,6,2))\n",
    "\n",
    "history40 = []\n",
    "\n",
    "\n",
    "#GRID SEARCH\n",
    "for i in range(6):\n",
    "    N_train[i] = 500 + i*100\n",
    "    row = []\n",
    "    for j in range(6):\n",
    "        sigma[j] = 0.1 + j*0.2 # noise standard deviation\n",
    "        x_train = np.random.uniform(-1, 1, int(N_train[i]))\n",
    "    \n",
    "        y_train = np.random.normal(m * x_train + b, sigma[j]) # actual measures from which we want to guess regression parameters\n",
    "        y_valid = np.random.normal(m * x_valid + b, sigma[j])\n",
    "        # fit the model using training dataset\n",
    "        # over 40 epochs of 32 batch size each\n",
    "        # report training progress against validation data\n",
    "        print(\"\\n\\nN_train = \",N_train[i])\n",
    "        print(\"Sigma = \",sigma[j])\n",
    "        row.append(model.fit(x=x_train, y=y_train, \n",
    "              batch_size=32, epochs=40,\n",
    "              shuffle=True, # a good idea is to shuffle input before at each epoch\n",
    "              validation_data=(x_valid, y_valid)))\n",
    "        score40[i][j] = model.evaluate(x_valid, y_valid, batch_size=32, verbose=1)\n",
    "        pesi = model.get_weights()\n",
    "        weight40[i][j] = [pesi[0][0][0],pesi[1][0]]\n",
    "        print(\"\\nWeight:\",pesi)  \n",
    "    history40.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruisco una rete neurale con $N_{epochs}=50$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0136 - mse: 0.0136\n",
      "\n",
      "Weight: [array([[2.000331]], dtype=float32), array([0.9982874], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0921 - val_loss: 0.0835 - val_mse: 0.0835\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0838 - val_mse: 0.0838\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0840 - val_mse: 0.0840\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0841 - val_mse: 0.0841\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0921 - mse: 0.0921 - val_loss: 0.0841 - val_mse: 0.0841\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0842 - val_mse: 0.0842\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0842 - val_mse: 0.0842\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0842 - val_mse: 0.0842\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0844 - val_mse: 0.0844\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0844 - val_mse: 0.0844\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0844 - val_mse: 0.0844\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0844 - val_mse: 0.0844\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0842 - val_mse: 0.0842\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0842 - val_mse: 0.0842\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0842 - val_mse: 0.0842\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0842 - val_mse: 0.0842\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0842 - val_mse: 0.0842\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0844 - val_mse: 0.0844\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0845 - val_mse: 0.0845\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0844 - val_mse: 0.0844\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0844 - val_mse: 0.0844\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0844 - val_mse: 0.0844\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0842 - val_mse: 0.0842\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0842 - val_mse: 0.0842\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0842 - val_mse: 0.0842\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0845 - val_mse: 0.0845\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0845 - val_mse: 0.0845\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0844 - val_mse: 0.0844\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0844 - val_mse: 0.0844\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0844 - val_mse: 0.0844\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0845 - val_mse: 0.0845\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0844 - val_mse: 0.0844\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0844 - val_mse: 0.0844\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0843 - mse: 0.0843\n",
      "\n",
      "Weight: [array([[2.0053947]], dtype=float32), array([0.9882859], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2731 - mse: 0.2731 - val_loss: 0.2073 - val_mse: 0.2073\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2722 - mse: 0.2722 - val_loss: 0.2069 - val_mse: 0.2069\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2716 - mse: 0.2716 - val_loss: 0.2067 - val_mse: 0.2067\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2713 - mse: 0.2713 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2709 - mse: 0.2709 - val_loss: 0.2064 - val_mse: 0.2064\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2704 - mse: 0.2704 - val_loss: 0.2063 - val_mse: 0.2063\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2702 - mse: 0.2702 - val_loss: 0.2062 - val_mse: 0.2062\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2700 - mse: 0.2700 - val_loss: 0.2061 - val_mse: 0.2061\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2698 - mse: 0.2698 - val_loss: 0.2061 - val_mse: 0.2061\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2699 - mse: 0.2699 - val_loss: 0.2061 - val_mse: 0.2061\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2696 - mse: 0.2696 - val_loss: 0.2061 - val_mse: 0.2061\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2696 - mse: 0.2696 - val_loss: 0.2061 - val_mse: 0.2061\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2695 - mse: 0.2695 - val_loss: 0.2061 - val_mse: 0.2061\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2694 - mse: 0.2694 - val_loss: 0.2061 - val_mse: 0.2061\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2694 - mse: 0.2694 - val_loss: 0.2061 - val_mse: 0.2061\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2694 - mse: 0.2694 - val_loss: 0.2061 - val_mse: 0.2061\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2694 - mse: 0.2694 - val_loss: 0.2062 - val_mse: 0.2062\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2694 - mse: 0.2694 - val_loss: 0.2062 - val_mse: 0.2062\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2693 - mse: 0.2693 - val_loss: 0.2062 - val_mse: 0.2062\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2062 - val_mse: 0.2062\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2694 - mse: 0.2694 - val_loss: 0.2062 - val_mse: 0.2062\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2694 - mse: 0.2694 - val_loss: 0.2063 - val_mse: 0.2063\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2063 - val_mse: 0.2063\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2063 - val_mse: 0.2063\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2063 - val_mse: 0.2063\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2063 - val_mse: 0.2063\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2693 - mse: 0.2693 - val_loss: 0.2063 - val_mse: 0.2063\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2693 - mse: 0.2693 - val_loss: 0.2063 - val_mse: 0.2063\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2064 - val_mse: 0.2064\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2064 - val_mse: 0.2064\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2064 - val_mse: 0.2064\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2064 - val_mse: 0.2064\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2064 - val_mse: 0.2064\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2064 - val_mse: 0.2064\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2064 - val_mse: 0.2064\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2064 - val_mse: 0.2064\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2064 - val_mse: 0.2064\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2691 - mse: 0.2691 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2691 - mse: 0.2691 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2066 - val_mse: 0.2066\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2066 - val_mse: 0.2066\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2693 - mse: 0.2693 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.2066 - val_mse: 0.2066\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2066 - mse: 0.2066\n",
      "\n",
      "Weight: [array([[1.8939764]], dtype=float32), array([0.99398214], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5028 - mse: 0.5028 - val_loss: 0.5804 - val_mse: 0.5804\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5026 - mse: 0.5026 - val_loss: 0.5802 - val_mse: 0.5802\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5024 - mse: 0.5024 - val_loss: 0.5801 - val_mse: 0.5801\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5023 - mse: 0.5023 - val_loss: 0.5799 - val_mse: 0.5799\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5023 - mse: 0.5023 - val_loss: 0.5793 - val_mse: 0.5793\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5019 - mse: 0.5019 - val_loss: 0.5792 - val_mse: 0.5792\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5017 - mse: 0.5017 - val_loss: 0.5789 - val_mse: 0.5789\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5019 - mse: 0.5019 - val_loss: 0.5787 - val_mse: 0.5787\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5017 - mse: 0.5017 - val_loss: 0.5787 - val_mse: 0.5787\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5017 - mse: 0.5017 - val_loss: 0.5787 - val_mse: 0.5787\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5017 - mse: 0.5017 - val_loss: 0.5790 - val_mse: 0.5790\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5016 - mse: 0.5016 - val_loss: 0.5789 - val_mse: 0.5789\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5016 - mse: 0.5016 - val_loss: 0.5787 - val_mse: 0.5787\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5016 - mse: 0.5016 - val_loss: 0.5783 - val_mse: 0.5783\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5014 - mse: 0.5014 - val_loss: 0.5780 - val_mse: 0.5780\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5015 - mse: 0.5015 - val_loss: 0.5784 - val_mse: 0.5784\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5014 - mse: 0.5014 - val_loss: 0.5783 - val_mse: 0.5783\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5018 - mse: 0.5018 - val_loss: 0.5784 - val_mse: 0.5784\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5014 - mse: 0.5014 - val_loss: 0.5779 - val_mse: 0.5779\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5016 - mse: 0.5016 - val_loss: 0.5778 - val_mse: 0.5778\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5014 - mse: 0.5014 - val_loss: 0.5779 - val_mse: 0.5779\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5013 - mse: 0.5013 - val_loss: 0.5779 - val_mse: 0.5779\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5014 - mse: 0.5014 - val_loss: 0.5780 - val_mse: 0.5780\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5019 - mse: 0.5019 - val_loss: 0.5778 - val_mse: 0.5778\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5013 - mse: 0.5013 - val_loss: 0.5779 - val_mse: 0.5779\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5014 - mse: 0.5014 - val_loss: 0.5784 - val_mse: 0.5784\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5013 - mse: 0.5013 - val_loss: 0.5781 - val_mse: 0.5781\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5017 - mse: 0.5017 - val_loss: 0.5778 - val_mse: 0.5778\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5014 - mse: 0.5014 - val_loss: 0.5781 - val_mse: 0.5781\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5016 - mse: 0.5016 - val_loss: 0.5782 - val_mse: 0.5782\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5015 - mse: 0.5015 - val_loss: 0.5778 - val_mse: 0.5778\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5015 - mse: 0.5015 - val_loss: 0.5781 - val_mse: 0.5781\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5016 - mse: 0.5016 - val_loss: 0.5778 - val_mse: 0.5778\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5013 - mse: 0.5013 - val_loss: 0.5777 - val_mse: 0.5777\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5016 - mse: 0.5016 - val_loss: 0.5781 - val_mse: 0.5781\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5013 - mse: 0.5013 - val_loss: 0.5780 - val_mse: 0.5780\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5015 - mse: 0.5015 - val_loss: 0.5779 - val_mse: 0.5779\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5014 - mse: 0.5014 - val_loss: 0.5779 - val_mse: 0.5779\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5016 - mse: 0.5016 - val_loss: 0.5777 - val_mse: 0.5777\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5014 - mse: 0.5014 - val_loss: 0.5778 - val_mse: 0.5778\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5013 - mse: 0.5013 - val_loss: 0.5779 - val_mse: 0.5779\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5014 - mse: 0.5014 - val_loss: 0.5784 - val_mse: 0.5784\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5014 - mse: 0.5014 - val_loss: 0.5778 - val_mse: 0.5778\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5014 - mse: 0.5014 - val_loss: 0.5776 - val_mse: 0.5776\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5014 - mse: 0.5014 - val_loss: 0.5776 - val_mse: 0.5776\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5015 - mse: 0.5015 - val_loss: 0.5781 - val_mse: 0.5781\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5014 - mse: 0.5014 - val_loss: 0.5779 - val_mse: 0.5779\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5014 - mse: 0.5014 - val_loss: 0.5778 - val_mse: 0.5778\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5015 - mse: 0.5015 - val_loss: 0.5777 - val_mse: 0.5777\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5014 - mse: 0.5014 - val_loss: 0.5777 - val_mse: 0.5777\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5777 - mse: 0.5777\n",
      "\n",
      "Weight: [array([[1.9578]], dtype=float32), array([0.9978928], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7874 - mse: 0.7874 - val_loss: 0.5844 - val_mse: 0.5844\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7870 - mse: 0.7870 - val_loss: 0.5863 - val_mse: 0.5863\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7866 - mse: 0.7866 - val_loss: 0.5875 - val_mse: 0.5875\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7865 - mse: 0.7865 - val_loss: 0.5884 - val_mse: 0.5884\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7865 - mse: 0.7865 - val_loss: 0.5904 - val_mse: 0.5904\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7860 - mse: 0.7860 - val_loss: 0.5917 - val_mse: 0.5917\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7857 - mse: 0.7857 - val_loss: 0.5919 - val_mse: 0.5919\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7862 - mse: 0.7862 - val_loss: 0.5924 - val_mse: 0.5924\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7858 - mse: 0.7858 - val_loss: 0.5932 - val_mse: 0.5932\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7859 - mse: 0.7859 - val_loss: 0.5938 - val_mse: 0.5938\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7858 - mse: 0.7858 - val_loss: 0.5925 - val_mse: 0.5925\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7858 - mse: 0.7858 - val_loss: 0.5921 - val_mse: 0.5921\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7861 - mse: 0.7861 - val_loss: 0.5926 - val_mse: 0.5926\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7862 - mse: 0.7862 - val_loss: 0.5942 - val_mse: 0.5942\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7860 - mse: 0.7860 - val_loss: 0.5955 - val_mse: 0.5955\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7862 - mse: 0.7862 - val_loss: 0.5957 - val_mse: 0.5957\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7862 - mse: 0.7862 - val_loss: 0.5955 - val_mse: 0.5955\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7861 - mse: 0.7861 - val_loss: 0.5947 - val_mse: 0.5947\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7863 - mse: 0.7863 - val_loss: 0.5930 - val_mse: 0.5930\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7859 - mse: 0.7859 - val_loss: 0.5933 - val_mse: 0.5933\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7858 - mse: 0.7858 - val_loss: 0.5939 - val_mse: 0.5939\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7858 - mse: 0.7858 - val_loss: 0.5941 - val_mse: 0.5941\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7858 - mse: 0.7858 - val_loss: 0.5934 - val_mse: 0.5934\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7859 - mse: 0.7859 - val_loss: 0.5934 - val_mse: 0.5934\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7857 - mse: 0.7857 - val_loss: 0.5943 - val_mse: 0.5943\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7859 - mse: 0.7859 - val_loss: 0.5941 - val_mse: 0.5941\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7862 - mse: 0.7862 - val_loss: 0.5940 - val_mse: 0.5940\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7863 - mse: 0.7863 - val_loss: 0.5943 - val_mse: 0.5943\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7859 - mse: 0.7859 - val_loss: 0.5937 - val_mse: 0.5937\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7861 - mse: 0.7861 - val_loss: 0.5941 - val_mse: 0.5941\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7860 - mse: 0.7860 - val_loss: 0.5932 - val_mse: 0.5932\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7861 - mse: 0.7861 - val_loss: 0.5942 - val_mse: 0.5942\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7860 - mse: 0.7860 - val_loss: 0.5939 - val_mse: 0.5939\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7860 - mse: 0.7860 - val_loss: 0.5948 - val_mse: 0.5948\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7860 - mse: 0.7860 - val_loss: 0.5945 - val_mse: 0.5945\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7859 - mse: 0.7859 - val_loss: 0.5943 - val_mse: 0.5943\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7862 - mse: 0.7862 - val_loss: 0.5944 - val_mse: 0.5944\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7859 - mse: 0.7859 - val_loss: 0.5954 - val_mse: 0.5954\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7860 - mse: 0.7860 - val_loss: 0.5963 - val_mse: 0.5963\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7860 - mse: 0.7860 - val_loss: 0.5957 - val_mse: 0.5957\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7859 - mse: 0.7859 - val_loss: 0.5949 - val_mse: 0.5949\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7860 - mse: 0.7860 - val_loss: 0.5948 - val_mse: 0.5948\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7859 - mse: 0.7859 - val_loss: 0.5943 - val_mse: 0.5943\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7858 - mse: 0.7858 - val_loss: 0.5944 - val_mse: 0.5944\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7858 - mse: 0.7858 - val_loss: 0.5947 - val_mse: 0.5947\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7860 - mse: 0.7860 - val_loss: 0.5941 - val_mse: 0.5941\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7863 - mse: 0.7863 - val_loss: 0.5944 - val_mse: 0.5944\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7859 - mse: 0.7859 - val_loss: 0.5939 - val_mse: 0.5939\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7862 - mse: 0.7862 - val_loss: 0.5937 - val_mse: 0.5937\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7858 - mse: 0.7858 - val_loss: 0.5945 - val_mse: 0.5945\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5945 - mse: 0.5945\n",
      "\n",
      "Weight: [array([[1.9834523]], dtype=float32), array([0.9565959], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.4049 - mse: 1.4049 - val_loss: 1.3494 - val_mse: 1.3494\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4038 - mse: 1.4038 - val_loss: 1.3480 - val_mse: 1.3480\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4023 - mse: 1.4023 - val_loss: 1.3484 - val_mse: 1.3484\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4012 - mse: 1.4012 - val_loss: 1.3487 - val_mse: 1.3487\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4010 - mse: 1.4010 - val_loss: 1.3500 - val_mse: 1.3500\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4007 - mse: 1.4007 - val_loss: 1.3515 - val_mse: 1.3515\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4001 - mse: 1.4001 - val_loss: 1.3522 - val_mse: 1.3522\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4001 - mse: 1.4001 - val_loss: 1.3527 - val_mse: 1.3527\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4004 - mse: 1.4004 - val_loss: 1.3539 - val_mse: 1.3539\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3997 - mse: 1.3997 - val_loss: 1.3549 - val_mse: 1.3549\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3992 - mse: 1.3992 - val_loss: 1.3570 - val_mse: 1.3570\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3989 - mse: 1.3989 - val_loss: 1.3583 - val_mse: 1.3583\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3991 - mse: 1.3991 - val_loss: 1.3579 - val_mse: 1.3579\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4000 - mse: 1.4000 - val_loss: 1.3579 - val_mse: 1.3579\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3992 - mse: 1.3992 - val_loss: 1.3577 - val_mse: 1.3577\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3990 - mse: 1.3990 - val_loss: 1.3613 - val_mse: 1.3613\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3989 - mse: 1.3989 - val_loss: 1.3611 - val_mse: 1.3611\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3994 - mse: 1.3994 - val_loss: 1.3605 - val_mse: 1.3605\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3993 - mse: 1.3993 - val_loss: 1.3596 - val_mse: 1.3596\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3994 - mse: 1.3994 - val_loss: 1.3608 - val_mse: 1.3608\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3986 - mse: 1.3986 - val_loss: 1.3602 - val_mse: 1.3602\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3988 - mse: 1.3988 - val_loss: 1.3616 - val_mse: 1.3616\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3990 - mse: 1.3990 - val_loss: 1.3631 - val_mse: 1.3631\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3990 - mse: 1.3990 - val_loss: 1.3615 - val_mse: 1.3615\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3989 - mse: 1.3989 - val_loss: 1.3615 - val_mse: 1.3615\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3988 - mse: 1.3988 - val_loss: 1.3623 - val_mse: 1.3623\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3995 - mse: 1.3995 - val_loss: 1.3628 - val_mse: 1.3628\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3989 - mse: 1.3989 - val_loss: 1.3615 - val_mse: 1.3615\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3993 - mse: 1.3993 - val_loss: 1.3618 - val_mse: 1.3618\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3987 - mse: 1.3987 - val_loss: 1.3611 - val_mse: 1.3611\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3988 - mse: 1.3988 - val_loss: 1.3609 - val_mse: 1.3609\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3988 - mse: 1.3988 - val_loss: 1.3610 - val_mse: 1.3610\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3993 - mse: 1.3993 - val_loss: 1.3619 - val_mse: 1.3619\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3988 - mse: 1.3988 - val_loss: 1.3614 - val_mse: 1.3614\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3996 - mse: 1.3996 - val_loss: 1.3622 - val_mse: 1.3622\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3997 - mse: 1.3997 - val_loss: 1.3613 - val_mse: 1.3613\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3991 - mse: 1.3991 - val_loss: 1.3622 - val_mse: 1.3622\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3989 - mse: 1.3989 - val_loss: 1.3618 - val_mse: 1.3618\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3992 - mse: 1.3992 - val_loss: 1.3606 - val_mse: 1.3606\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3990 - mse: 1.3990 - val_loss: 1.3618 - val_mse: 1.3618\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3988 - mse: 1.3988 - val_loss: 1.3614 - val_mse: 1.3614\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3990 - mse: 1.3990 - val_loss: 1.3616 - val_mse: 1.3616\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3997 - mse: 1.3997 - val_loss: 1.3611 - val_mse: 1.3611\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3989 - mse: 1.3989 - val_loss: 1.3629 - val_mse: 1.3629\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3989 - mse: 1.3989 - val_loss: 1.3631 - val_mse: 1.3631\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3997 - mse: 1.3997 - val_loss: 1.3623 - val_mse: 1.3623\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3987 - mse: 1.3987 - val_loss: 1.3628 - val_mse: 1.3628\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3985 - mse: 1.3985 - val_loss: 1.3615 - val_mse: 1.3615\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3987 - mse: 1.3987 - val_loss: 1.3627 - val_mse: 1.3627\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3991 - mse: 1.3991 - val_loss: 1.3633 - val_mse: 1.3633\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3633 - mse: 1.3633\n",
      "\n",
      "Weight: [array([[2.1147027]], dtype=float32), array([0.9831762], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0067 - mse: 0.0067\n",
      "\n",
      "Weight: [array([[2.0015156]], dtype=float32), array([0.99326974], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0839 - mse: 0.0839 - val_loss: 0.1087 - val_mse: 0.1087\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0839 - mse: 0.0839 - val_loss: 0.1087 - val_mse: 0.1087\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0839 - mse: 0.0839 - val_loss: 0.1086 - val_mse: 0.1086\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0839 - mse: 0.0839 - val_loss: 0.1086 - val_mse: 0.1086\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0839 - mse: 0.0839 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0839 - mse: 0.0839 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0839 - mse: 0.0839 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1085 - val_mse: 0.1085\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.1085 - mse: 0.1085\n",
      "\n",
      "Weight: [array([[2.0031419]], dtype=float32), array([1.0053161], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2669 - mse: 0.2669 - val_loss: 0.2776 - val_mse: 0.2776\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2669 - mse: 0.2669 - val_loss: 0.2774 - val_mse: 0.2774\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2668 - mse: 0.2668 - val_loss: 0.2773 - val_mse: 0.2773\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2667 - mse: 0.2667 - val_loss: 0.2771 - val_mse: 0.2771\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2667 - mse: 0.2667 - val_loss: 0.2769 - val_mse: 0.2769\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2769 - val_mse: 0.2769\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2768 - val_mse: 0.2768\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2766 - val_mse: 0.2766\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2767 - val_mse: 0.2767\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2667 - mse: 0.2667 - val_loss: 0.2767 - val_mse: 0.2767\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2765 - val_mse: 0.2765\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2667 - mse: 0.2667 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2765 - val_mse: 0.2765\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2765 - val_mse: 0.2765\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2765 - val_mse: 0.2765\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2765 - val_mse: 0.2765\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2667 - mse: 0.2667 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2667 - mse: 0.2667 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2667 - mse: 0.2667 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2667 - mse: 0.2667 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2665 - mse: 0.2665 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2762 - mse: 0.2762\n",
      "\n",
      "Weight: [array([[1.9760606]], dtype=float32), array([1.0029776], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5119 - mse: 0.5119 - val_loss: 0.4078 - val_mse: 0.4078\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5114 - mse: 0.5114 - val_loss: 0.4083 - val_mse: 0.4083\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5111 - mse: 0.5111 - val_loss: 0.4088 - val_mse: 0.4088\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5112 - mse: 0.5112 - val_loss: 0.4093 - val_mse: 0.4093\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5110 - mse: 0.5110 - val_loss: 0.4098 - val_mse: 0.4098\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5109 - mse: 0.5109 - val_loss: 0.4104 - val_mse: 0.4104\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5108 - mse: 0.5108 - val_loss: 0.4107 - val_mse: 0.4107\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5108 - mse: 0.5108 - val_loss: 0.4111 - val_mse: 0.4111\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5109 - mse: 0.5109 - val_loss: 0.4113 - val_mse: 0.4113\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5110 - mse: 0.5110 - val_loss: 0.4115 - val_mse: 0.4115\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5107 - mse: 0.5107 - val_loss: 0.4118 - val_mse: 0.4118\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5106 - mse: 0.5106 - val_loss: 0.4120 - val_mse: 0.4120\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5110 - mse: 0.5110 - val_loss: 0.4122 - val_mse: 0.4122\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5109 - mse: 0.5109 - val_loss: 0.4124 - val_mse: 0.4124\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5107 - mse: 0.5107 - val_loss: 0.4125 - val_mse: 0.4125\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5107 - mse: 0.5107 - val_loss: 0.4128 - val_mse: 0.4128\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5107 - mse: 0.5107 - val_loss: 0.4129 - val_mse: 0.4129\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5108 - mse: 0.5108 - val_loss: 0.4129 - val_mse: 0.4129\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5107 - mse: 0.5107 - val_loss: 0.4130 - val_mse: 0.4130\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5108 - mse: 0.5108 - val_loss: 0.4131 - val_mse: 0.4131\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5108 - mse: 0.5108 - val_loss: 0.4129 - val_mse: 0.4129\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5107 - mse: 0.5107 - val_loss: 0.4130 - val_mse: 0.4130\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5107 - mse: 0.5107 - val_loss: 0.4130 - val_mse: 0.4130\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5108 - mse: 0.5108 - val_loss: 0.4130 - val_mse: 0.4130\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5107 - mse: 0.5107 - val_loss: 0.4131 - val_mse: 0.4131\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5107 - mse: 0.5107 - val_loss: 0.4131 - val_mse: 0.4131\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5109 - mse: 0.5109 - val_loss: 0.4132 - val_mse: 0.4132\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5107 - mse: 0.5107 - val_loss: 0.4132 - val_mse: 0.4132\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5107 - mse: 0.5107 - val_loss: 0.4133 - val_mse: 0.4133\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5108 - mse: 0.5108 - val_loss: 0.4133 - val_mse: 0.4133\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5106 - mse: 0.5106 - val_loss: 0.4133 - val_mse: 0.4133\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5108 - mse: 0.5108 - val_loss: 0.4134 - val_mse: 0.4134\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5106 - mse: 0.5106 - val_loss: 0.4134 - val_mse: 0.4134\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5106 - mse: 0.5106 - val_loss: 0.4135 - val_mse: 0.4135\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5105 - mse: 0.5105 - val_loss: 0.4135 - val_mse: 0.4135\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5107 - mse: 0.5107 - val_loss: 0.4136 - val_mse: 0.4136\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5110 - mse: 0.5110 - val_loss: 0.4135 - val_mse: 0.4135\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5108 - mse: 0.5108 - val_loss: 0.4135 - val_mse: 0.4135\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5108 - mse: 0.5108 - val_loss: 0.4136 - val_mse: 0.4136\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5107 - mse: 0.5107 - val_loss: 0.4136 - val_mse: 0.4136\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5107 - mse: 0.5107 - val_loss: 0.4135 - val_mse: 0.4135\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5106 - mse: 0.5106 - val_loss: 0.4135 - val_mse: 0.4135\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5106 - mse: 0.5106 - val_loss: 0.4134 - val_mse: 0.4134\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5108 - mse: 0.5108 - val_loss: 0.4135 - val_mse: 0.4135\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5106 - mse: 0.5106 - val_loss: 0.4134 - val_mse: 0.4134\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5107 - mse: 0.5107 - val_loss: 0.4135 - val_mse: 0.4135\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5106 - mse: 0.5106 - val_loss: 0.4134 - val_mse: 0.4134\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5106 - mse: 0.5106 - val_loss: 0.4134 - val_mse: 0.4134\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5109 - mse: 0.5109 - val_loss: 0.4135 - val_mse: 0.4135\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5110 - mse: 0.5110 - val_loss: 0.4135 - val_mse: 0.4135\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4135 - mse: 0.4135\n",
      "\n",
      "Weight: [array([[1.9239278]], dtype=float32), array([0.9869885], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.8563 - mse: 0.8563 - val_loss: 0.7773 - val_mse: 0.7773\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8535 - mse: 0.8535 - val_loss: 0.7724 - val_mse: 0.7724\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8527 - mse: 0.8527 - val_loss: 0.7683 - val_mse: 0.7683\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8519 - mse: 0.8519 - val_loss: 0.7664 - val_mse: 0.7664\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8518 - mse: 0.8518 - val_loss: 0.7637 - val_mse: 0.7637\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8516 - mse: 0.8516 - val_loss: 0.7624 - val_mse: 0.7624\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8514 - mse: 0.8514 - val_loss: 0.7615 - val_mse: 0.7615\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8514 - mse: 0.8514 - val_loss: 0.7609 - val_mse: 0.7609\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8513 - mse: 0.8513 - val_loss: 0.7600 - val_mse: 0.7600\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8514 - mse: 0.8514 - val_loss: 0.7599 - val_mse: 0.7599\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8514 - mse: 0.8514 - val_loss: 0.7599 - val_mse: 0.7599\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8515 - mse: 0.8515 - val_loss: 0.7602 - val_mse: 0.7602\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8516 - mse: 0.8516 - val_loss: 0.7586 - val_mse: 0.7586\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8513 - mse: 0.8513 - val_loss: 0.7589 - val_mse: 0.7589\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8512 - mse: 0.8512 - val_loss: 0.7589 - val_mse: 0.7589\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8513 - mse: 0.8513 - val_loss: 0.7582 - val_mse: 0.7582\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8510 - mse: 0.8510 - val_loss: 0.7585 - val_mse: 0.7585\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8513 - mse: 0.8513 - val_loss: 0.7580 - val_mse: 0.7580\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8513 - mse: 0.8513 - val_loss: 0.7581 - val_mse: 0.7581\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8511 - mse: 0.8511 - val_loss: 0.7584 - val_mse: 0.7584\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8513 - mse: 0.8513 - val_loss: 0.7576 - val_mse: 0.7576\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8514 - mse: 0.8514 - val_loss: 0.7574 - val_mse: 0.7574\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8512 - mse: 0.8512 - val_loss: 0.7573 - val_mse: 0.7573\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8511 - mse: 0.8511 - val_loss: 0.7581 - val_mse: 0.7581\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8511 - mse: 0.8511 - val_loss: 0.7572 - val_mse: 0.7572\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8512 - mse: 0.8512 - val_loss: 0.7571 - val_mse: 0.7571\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8512 - mse: 0.8512 - val_loss: 0.7571 - val_mse: 0.7571\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8516 - mse: 0.8516 - val_loss: 0.7577 - val_mse: 0.7577\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8509 - mse: 0.8509 - val_loss: 0.7576 - val_mse: 0.7576\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8515 - mse: 0.8515 - val_loss: 0.7574 - val_mse: 0.7574\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8513 - mse: 0.8513 - val_loss: 0.7568 - val_mse: 0.7568\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8513 - mse: 0.8513 - val_loss: 0.7575 - val_mse: 0.7575\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8514 - mse: 0.8514 - val_loss: 0.7568 - val_mse: 0.7568\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8513 - mse: 0.8513 - val_loss: 0.7572 - val_mse: 0.7572\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8512 - mse: 0.8512 - val_loss: 0.7568 - val_mse: 0.7568\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8515 - mse: 0.8515 - val_loss: 0.7570 - val_mse: 0.7570\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8511 - mse: 0.8511 - val_loss: 0.7570 - val_mse: 0.7570\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8514 - mse: 0.8514 - val_loss: 0.7574 - val_mse: 0.7574\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8513 - mse: 0.8513 - val_loss: 0.7575 - val_mse: 0.7575\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8514 - mse: 0.8514 - val_loss: 0.7581 - val_mse: 0.7581\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8515 - mse: 0.8515 - val_loss: 0.7587 - val_mse: 0.7587\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8514 - mse: 0.8514 - val_loss: 0.7585 - val_mse: 0.7585\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8515 - mse: 0.8515 - val_loss: 0.7590 - val_mse: 0.7590\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8512 - mse: 0.8512 - val_loss: 0.7581 - val_mse: 0.7581\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8511 - mse: 0.8511 - val_loss: 0.7580 - val_mse: 0.7580\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8513 - mse: 0.8513 - val_loss: 0.7580 - val_mse: 0.7580\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8511 - mse: 0.8511 - val_loss: 0.7577 - val_mse: 0.7577\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8513 - mse: 0.8513 - val_loss: 0.7576 - val_mse: 0.7576\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8511 - mse: 0.8511 - val_loss: 0.7578 - val_mse: 0.7578\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8513 - mse: 0.8513 - val_loss: 0.7574 - val_mse: 0.7574\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.7574 - mse: 0.7574\n",
      "\n",
      "Weight: [array([[1.9761118]], dtype=float32), array([1.0639088], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1.1423 - mse: 1.1423 - val_loss: 1.0817 - val_mse: 1.0817\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1384 - mse: 1.1384 - val_loss: 1.0737 - val_mse: 1.0737\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1358 - mse: 1.1358 - val_loss: 1.0694 - val_mse: 1.0694\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1353 - mse: 1.1353 - val_loss: 1.0655 - val_mse: 1.0655\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1344 - mse: 1.1344 - val_loss: 1.0639 - val_mse: 1.0639\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1339 - mse: 1.1339 - val_loss: 1.0635 - val_mse: 1.0635\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1336 - mse: 1.1336 - val_loss: 1.0618 - val_mse: 1.0618\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1340 - mse: 1.1340 - val_loss: 1.0615 - val_mse: 1.0615\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1335 - mse: 1.1335 - val_loss: 1.0606 - val_mse: 1.0606\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1334 - mse: 1.1334 - val_loss: 1.0606 - val_mse: 1.0606\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1337 - mse: 1.1337 - val_loss: 1.0607 - val_mse: 1.0607\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1339 - mse: 1.1339 - val_loss: 1.0608 - val_mse: 1.0608\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1339 - mse: 1.1339 - val_loss: 1.0607 - val_mse: 1.0607\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1336 - mse: 1.1336 - val_loss: 1.0609 - val_mse: 1.0609\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1335 - mse: 1.1335 - val_loss: 1.0603 - val_mse: 1.0603\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1334 - mse: 1.1334 - val_loss: 1.0602 - val_mse: 1.0602\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1337 - mse: 1.1337 - val_loss: 1.0609 - val_mse: 1.0609\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1334 - mse: 1.1334 - val_loss: 1.0611 - val_mse: 1.0611\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1340 - mse: 1.1340 - val_loss: 1.0609 - val_mse: 1.0609\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1338 - mse: 1.1338 - val_loss: 1.0609 - val_mse: 1.0609\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1336 - mse: 1.1336 - val_loss: 1.0605 - val_mse: 1.0605\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1338 - mse: 1.1338 - val_loss: 1.0600 - val_mse: 1.0600\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1338 - mse: 1.1338 - val_loss: 1.0600 - val_mse: 1.0600\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1336 - mse: 1.1336 - val_loss: 1.0599 - val_mse: 1.0599\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1340 - mse: 1.1340 - val_loss: 1.0609 - val_mse: 1.0609\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1335 - mse: 1.1335 - val_loss: 1.0620 - val_mse: 1.0620\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1337 - mse: 1.1337 - val_loss: 1.0617 - val_mse: 1.0617\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1338 - mse: 1.1338 - val_loss: 1.0620 - val_mse: 1.0620\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1341 - mse: 1.1341 - val_loss: 1.0625 - val_mse: 1.0625\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1333 - mse: 1.1333 - val_loss: 1.0625 - val_mse: 1.0625\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1337 - mse: 1.1337 - val_loss: 1.0620 - val_mse: 1.0620\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1339 - mse: 1.1339 - val_loss: 1.0622 - val_mse: 1.0622\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1337 - mse: 1.1337 - val_loss: 1.0632 - val_mse: 1.0632\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1337 - mse: 1.1337 - val_loss: 1.0629 - val_mse: 1.0629\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1335 - mse: 1.1335 - val_loss: 1.0632 - val_mse: 1.0632\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1338 - mse: 1.1338 - val_loss: 1.0620 - val_mse: 1.0620\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1335 - mse: 1.1335 - val_loss: 1.0613 - val_mse: 1.0613\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1344 - mse: 1.1344 - val_loss: 1.0621 - val_mse: 1.0621\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1334 - mse: 1.1334 - val_loss: 1.0618 - val_mse: 1.0618\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1340 - mse: 1.1340 - val_loss: 1.0608 - val_mse: 1.0608\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1341 - mse: 1.1341 - val_loss: 1.0609 - val_mse: 1.0609\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1338 - mse: 1.1338 - val_loss: 1.0627 - val_mse: 1.0627\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1335 - mse: 1.1335 - val_loss: 1.0618 - val_mse: 1.0618\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1336 - mse: 1.1336 - val_loss: 1.0609 - val_mse: 1.0609\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1340 - mse: 1.1340 - val_loss: 1.0609 - val_mse: 1.0609\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1336 - mse: 1.1336 - val_loss: 1.0608 - val_mse: 1.0608\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1338 - mse: 1.1338 - val_loss: 1.0613 - val_mse: 1.0613\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1335 - mse: 1.1335 - val_loss: 1.0613 - val_mse: 1.0613\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1341 - mse: 1.1341 - val_loss: 1.0617 - val_mse: 1.0617\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1335 - mse: 1.1335 - val_loss: 1.0611 - val_mse: 1.0611\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.0611 - mse: 1.0611\n",
      "\n",
      "Weight: [array([[2.0240319]], dtype=float32), array([0.95791465], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0094 - mse: 0.0094\n",
      "\n",
      "Weight: [array([[1.993068]], dtype=float32), array([0.99703443], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0891 - mse: 0.0891 - val_loss: 0.1123 - val_mse: 0.1123\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0890 - mse: 0.0890 - val_loss: 0.1120 - val_mse: 0.1120\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1119 - val_mse: 0.1119\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1117 - val_mse: 0.1117\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1116 - val_mse: 0.1116\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1115 - val_mse: 0.1115\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1114 - val_mse: 0.1114\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1113 - val_mse: 0.1113\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1113 - val_mse: 0.1113\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.1109 - mse: 0.1109\n",
      "\n",
      "Weight: [array([[2.0166044]], dtype=float32), array([1.0067677], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2522 - mse: 0.2522 - val_loss: 0.2948 - val_mse: 0.2948\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2521 - mse: 0.2521 - val_loss: 0.2951 - val_mse: 0.2951\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2521 - mse: 0.2521 - val_loss: 0.2952 - val_mse: 0.2952\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2520 - mse: 0.2520 - val_loss: 0.2955 - val_mse: 0.2955\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2955 - val_mse: 0.2955\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2520 - mse: 0.2520 - val_loss: 0.2959 - val_mse: 0.2959\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2520 - mse: 0.2520 - val_loss: 0.2960 - val_mse: 0.2960\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2961 - val_mse: 0.2961\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2965 - val_mse: 0.2965\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2963 - val_mse: 0.2963\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2965 - val_mse: 0.2965\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2965 - val_mse: 0.2965\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2966 - val_mse: 0.2966\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2968 - val_mse: 0.2968\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2968 - val_mse: 0.2968\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2968 - val_mse: 0.2968\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2969 - val_mse: 0.2969\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2971 - val_mse: 0.2971\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2973 - val_mse: 0.2973\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2971 - val_mse: 0.2971\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2974 - val_mse: 0.2974\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2974 - val_mse: 0.2974\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2974 - val_mse: 0.2974\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2976 - val_mse: 0.2976\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2517 - mse: 0.2517 - val_loss: 0.2973 - val_mse: 0.2973\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2973 - val_mse: 0.2973\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2974 - val_mse: 0.2974\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2971 - val_mse: 0.2971\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2973 - val_mse: 0.2973\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2974 - val_mse: 0.2974\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2974 - val_mse: 0.2974\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2973 - val_mse: 0.2973\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2974 - val_mse: 0.2974\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2973 - val_mse: 0.2973\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2973 - val_mse: 0.2973\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2975 - val_mse: 0.2975\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2973 - val_mse: 0.2973\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2975 - val_mse: 0.2975\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2977 - val_mse: 0.2977\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2974 - val_mse: 0.2974\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2974 - val_mse: 0.2974\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2972 - val_mse: 0.2972\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2974 - val_mse: 0.2974\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2517 - mse: 0.2517 - val_loss: 0.2976 - val_mse: 0.2976\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2976 - val_mse: 0.2976\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2974 - val_mse: 0.2974\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2973 - val_mse: 0.2973\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2975 - val_mse: 0.2975\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2973 - val_mse: 0.2973\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2975 - val_mse: 0.2975\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2975 - mse: 0.2975\n",
      "\n",
      "Weight: [array([[1.9768387]], dtype=float32), array([1.0042619], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4828 - mse: 0.4828 - val_loss: 0.4732 - val_mse: 0.4732\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4825 - mse: 0.4825 - val_loss: 0.4718 - val_mse: 0.4718\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4821 - mse: 0.4821 - val_loss: 0.4714 - val_mse: 0.4714\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4818 - mse: 0.4818 - val_loss: 0.4710 - val_mse: 0.4710\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4820 - mse: 0.4820 - val_loss: 0.4707 - val_mse: 0.4707\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4818 - mse: 0.4818 - val_loss: 0.4708 - val_mse: 0.4708\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4819 - mse: 0.4819 - val_loss: 0.4707 - val_mse: 0.4707\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4709 - val_mse: 0.4709\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4817 - mse: 0.4817 - val_loss: 0.4713 - val_mse: 0.4713\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4717 - val_mse: 0.4717\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4816 - mse: 0.4816 - val_loss: 0.4713 - val_mse: 0.4713\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4816 - mse: 0.4816 - val_loss: 0.4714 - val_mse: 0.4714\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4816 - mse: 0.4816 - val_loss: 0.4709 - val_mse: 0.4709\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4816 - mse: 0.4816 - val_loss: 0.4710 - val_mse: 0.4710\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4814 - mse: 0.4814 - val_loss: 0.4711 - val_mse: 0.4711\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4814 - mse: 0.4814 - val_loss: 0.4708 - val_mse: 0.4708\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4716 - val_mse: 0.4716\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4715 - val_mse: 0.4715\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4719 - val_mse: 0.4719\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4717 - val_mse: 0.4717\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4814 - mse: 0.4814 - val_loss: 0.4719 - val_mse: 0.4719\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4814 - mse: 0.4814 - val_loss: 0.4714 - val_mse: 0.4714\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4724 - val_mse: 0.4724\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4724 - val_mse: 0.4724\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4814 - mse: 0.4814 - val_loss: 0.4714 - val_mse: 0.4714\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4717 - val_mse: 0.4717\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4711 - val_mse: 0.4711\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4813 - mse: 0.4813 - val_loss: 0.4712 - val_mse: 0.4712\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4709 - val_mse: 0.4709\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4713 - val_mse: 0.4713\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4814 - mse: 0.4814 - val_loss: 0.4707 - val_mse: 0.4707\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4814 - mse: 0.4814 - val_loss: 0.4703 - val_mse: 0.4703\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4706 - val_mse: 0.4706\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4708 - val_mse: 0.4708\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4718 - val_mse: 0.4718\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4816 - mse: 0.4816 - val_loss: 0.4720 - val_mse: 0.4720\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4725 - val_mse: 0.4725\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4712 - val_mse: 0.4712\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4712 - val_mse: 0.4712\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4719 - val_mse: 0.4719\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4814 - mse: 0.4814 - val_loss: 0.4713 - val_mse: 0.4713\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4814 - mse: 0.4814 - val_loss: 0.4716 - val_mse: 0.4716\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4813 - mse: 0.4813 - val_loss: 0.4714 - val_mse: 0.4714\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4814 - mse: 0.4814 - val_loss: 0.4713 - val_mse: 0.4713\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4710 - val_mse: 0.4710\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4814 - mse: 0.4814 - val_loss: 0.4715 - val_mse: 0.4715\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4712 - val_mse: 0.4712\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4816 - mse: 0.4816 - val_loss: 0.4711 - val_mse: 0.4711\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4712 - val_mse: 0.4712\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4716 - val_mse: 0.4716\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4716 - mse: 0.4716\n",
      "\n",
      "Weight: [array([[2.0404825]], dtype=float32), array([0.9836515], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.7384 - mse: 0.7384 - val_loss: 0.7520 - val_mse: 0.7520\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7383 - mse: 0.7383 - val_loss: 0.7516 - val_mse: 0.7516\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7383 - mse: 0.7383 - val_loss: 0.7528 - val_mse: 0.7528\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7382 - mse: 0.7382 - val_loss: 0.7540 - val_mse: 0.7540\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7381 - mse: 0.7381 - val_loss: 0.7548 - val_mse: 0.7548\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7381 - mse: 0.7381 - val_loss: 0.7552 - val_mse: 0.7552\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7382 - mse: 0.7382 - val_loss: 0.7534 - val_mse: 0.7534\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7382 - mse: 0.7382 - val_loss: 0.7519 - val_mse: 0.7519\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7380 - mse: 0.7380 - val_loss: 0.7533 - val_mse: 0.7533\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7383 - mse: 0.7383 - val_loss: 0.7539 - val_mse: 0.7539\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7382 - mse: 0.7382 - val_loss: 0.7537 - val_mse: 0.7537\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7381 - mse: 0.7381 - val_loss: 0.7537 - val_mse: 0.7537\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7382 - mse: 0.7382 - val_loss: 0.7538 - val_mse: 0.7538\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7383 - mse: 0.7383 - val_loss: 0.7524 - val_mse: 0.7524\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7380 - mse: 0.7380 - val_loss: 0.7522 - val_mse: 0.7522\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7381 - mse: 0.7381 - val_loss: 0.7529 - val_mse: 0.7529\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7382 - mse: 0.7382 - val_loss: 0.7521 - val_mse: 0.7521\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7381 - mse: 0.7381 - val_loss: 0.7527 - val_mse: 0.7527\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7382 - mse: 0.7382 - val_loss: 0.7524 - val_mse: 0.7524\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7380 - mse: 0.7380 - val_loss: 0.7529 - val_mse: 0.7529\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7384 - mse: 0.7384 - val_loss: 0.7513 - val_mse: 0.7513\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7380 - mse: 0.7380 - val_loss: 0.7516 - val_mse: 0.7516\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7384 - mse: 0.7384 - val_loss: 0.7516 - val_mse: 0.7516\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7383 - mse: 0.7383 - val_loss: 0.7506 - val_mse: 0.7506\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7382 - mse: 0.7382 - val_loss: 0.7518 - val_mse: 0.7518\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7384 - mse: 0.7384 - val_loss: 0.7515 - val_mse: 0.7515\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7383 - mse: 0.7383 - val_loss: 0.7520 - val_mse: 0.7520\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7382 - mse: 0.7382 - val_loss: 0.7526 - val_mse: 0.7526\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7381 - mse: 0.7381 - val_loss: 0.7532 - val_mse: 0.7532\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7383 - mse: 0.7383 - val_loss: 0.7531 - val_mse: 0.7531\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7381 - mse: 0.7381 - val_loss: 0.7537 - val_mse: 0.7537\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7384 - mse: 0.7384 - val_loss: 0.7528 - val_mse: 0.7528\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7382 - mse: 0.7382 - val_loss: 0.7516 - val_mse: 0.7516\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7381 - mse: 0.7381 - val_loss: 0.7510 - val_mse: 0.7510\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7380 - mse: 0.7380 - val_loss: 0.7507 - val_mse: 0.7507\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7382 - mse: 0.7382 - val_loss: 0.7522 - val_mse: 0.7522\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7381 - mse: 0.7381 - val_loss: 0.7508 - val_mse: 0.7508\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7383 - mse: 0.7383 - val_loss: 0.7513 - val_mse: 0.7513\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7379 - mse: 0.7379 - val_loss: 0.7516 - val_mse: 0.7516\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7384 - mse: 0.7384 - val_loss: 0.7507 - val_mse: 0.7507\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7381 - mse: 0.7381 - val_loss: 0.7511 - val_mse: 0.7511\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7382 - mse: 0.7382 - val_loss: 0.7529 - val_mse: 0.7529\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7383 - mse: 0.7383 - val_loss: 0.7531 - val_mse: 0.7531\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7382 - mse: 0.7382 - val_loss: 0.7530 - val_mse: 0.7530\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7384 - mse: 0.7384 - val_loss: 0.7532 - val_mse: 0.7532\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7383 - mse: 0.7383 - val_loss: 0.7537 - val_mse: 0.7537\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7382 - mse: 0.7382 - val_loss: 0.7539 - val_mse: 0.7539\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7384 - mse: 0.7384 - val_loss: 0.7525 - val_mse: 0.7525\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7383 - mse: 0.7383 - val_loss: 0.7529 - val_mse: 0.7529\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7382 - mse: 0.7382 - val_loss: 0.7534 - val_mse: 0.7534\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.7534 - mse: 0.7534\n",
      "\n",
      "Weight: [array([[2.0242658]], dtype=float32), array([1.0017991], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.2340 - mse: 1.2340 - val_loss: 1.1827 - val_mse: 1.1827\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2339 - mse: 1.2339 - val_loss: 1.1828 - val_mse: 1.1828\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2338 - mse: 1.2338 - val_loss: 1.1836 - val_mse: 1.1836\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2334 - mse: 1.2334 - val_loss: 1.1848 - val_mse: 1.1848\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2339 - mse: 1.2339 - val_loss: 1.1851 - val_mse: 1.1851\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2339 - mse: 1.2339 - val_loss: 1.1863 - val_mse: 1.1863\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2338 - mse: 1.2338 - val_loss: 1.1852 - val_mse: 1.1852\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2340 - mse: 1.2340 - val_loss: 1.1871 - val_mse: 1.1871\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2339 - mse: 1.2339 - val_loss: 1.1870 - val_mse: 1.1870\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2338 - mse: 1.2338 - val_loss: 1.1868 - val_mse: 1.1868\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2336 - mse: 1.2336 - val_loss: 1.1868 - val_mse: 1.1868\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2339 - mse: 1.2339 - val_loss: 1.1878 - val_mse: 1.1878\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2339 - mse: 1.2339 - val_loss: 1.1866 - val_mse: 1.1866\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2339 - mse: 1.2339 - val_loss: 1.1864 - val_mse: 1.1864\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2347 - mse: 1.2347 - val_loss: 1.1858 - val_mse: 1.1858\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2340 - mse: 1.2340 - val_loss: 1.1847 - val_mse: 1.1847\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2336 - mse: 1.2336 - val_loss: 1.1854 - val_mse: 1.1854\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2338 - mse: 1.2338 - val_loss: 1.1855 - val_mse: 1.1855\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2335 - mse: 1.2335 - val_loss: 1.1866 - val_mse: 1.1866\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2337 - mse: 1.2337 - val_loss: 1.1847 - val_mse: 1.1847\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2336 - mse: 1.2336 - val_loss: 1.1857 - val_mse: 1.1857\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2336 - mse: 1.2336 - val_loss: 1.1854 - val_mse: 1.1854\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2336 - mse: 1.2336 - val_loss: 1.1854 - val_mse: 1.1854\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2342 - mse: 1.2342 - val_loss: 1.1870 - val_mse: 1.1870\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2337 - mse: 1.2337 - val_loss: 1.1871 - val_mse: 1.1871\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2342 - mse: 1.2342 - val_loss: 1.1872 - val_mse: 1.1872\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2335 - mse: 1.2335 - val_loss: 1.1869 - val_mse: 1.1869\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2335 - mse: 1.2335 - val_loss: 1.1872 - val_mse: 1.1872\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2339 - mse: 1.2339 - val_loss: 1.1877 - val_mse: 1.1877\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2338 - mse: 1.2338 - val_loss: 1.1858 - val_mse: 1.1858\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2339 - mse: 1.2339 - val_loss: 1.1850 - val_mse: 1.1850\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2338 - mse: 1.2338 - val_loss: 1.1862 - val_mse: 1.1862\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2339 - mse: 1.2339 - val_loss: 1.1849 - val_mse: 1.1849\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2345 - mse: 1.2345 - val_loss: 1.1849 - val_mse: 1.1849\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2342 - mse: 1.2342 - val_loss: 1.1855 - val_mse: 1.1855\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2338 - mse: 1.2338 - val_loss: 1.1850 - val_mse: 1.1850\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2336 - mse: 1.2336 - val_loss: 1.1850 - val_mse: 1.1850\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2337 - mse: 1.2337 - val_loss: 1.1844 - val_mse: 1.1844\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2337 - mse: 1.2337 - val_loss: 1.1838 - val_mse: 1.1838\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2335 - mse: 1.2335 - val_loss: 1.1847 - val_mse: 1.1847\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2337 - mse: 1.2337 - val_loss: 1.1870 - val_mse: 1.1870\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2340 - mse: 1.2340 - val_loss: 1.1865 - val_mse: 1.1865\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2335 - mse: 1.2335 - val_loss: 1.1859 - val_mse: 1.1859\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2335 - mse: 1.2335 - val_loss: 1.1870 - val_mse: 1.1870\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2341 - mse: 1.2341 - val_loss: 1.1880 - val_mse: 1.1880\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2337 - mse: 1.2337 - val_loss: 1.1889 - val_mse: 1.1889\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2337 - mse: 1.2337 - val_loss: 1.1862 - val_mse: 1.1862\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2340 - mse: 1.2340 - val_loss: 1.1859 - val_mse: 1.1859\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2338 - mse: 1.2338 - val_loss: 1.1867 - val_mse: 1.1867\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2341 - mse: 1.2341 - val_loss: 1.1871 - val_mse: 1.1871\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.1871 - mse: 1.1871\n",
      "\n",
      "Weight: [array([[2.0322406]], dtype=float32), array([1.0264163], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0106 - mse: 0.0106\n",
      "\n",
      "Weight: [array([[1.9983599]], dtype=float32), array([0.9960202], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0913 - mse: 0.0913 - val_loss: 0.1482 - val_mse: 0.1482\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0911 - mse: 0.0911 - val_loss: 0.1482 - val_mse: 0.1482\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0911 - mse: 0.0911 - val_loss: 0.1482 - val_mse: 0.1482\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0910 - mse: 0.0910 - val_loss: 0.1482 - val_mse: 0.1482\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0910 - mse: 0.0910 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0910 - mse: 0.0910 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0910 - mse: 0.0910 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.1481 - val_mse: 0.1481\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.1481 - mse: 0.1481\n",
      "\n",
      "Weight: [array([[1.9622301]], dtype=float32), array([0.9979009], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2614 - mse: 0.2614 - val_loss: 0.1940 - val_mse: 0.1940\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2612 - mse: 0.2612 - val_loss: 0.1943 - val_mse: 0.1943\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1946 - val_mse: 0.1946\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1947 - val_mse: 0.1947\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1950 - val_mse: 0.1950\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1950 - val_mse: 0.1950\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1948 - val_mse: 0.1948\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1948 - val_mse: 0.1948\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1948 - val_mse: 0.1948\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1948 - val_mse: 0.1948\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1948 - val_mse: 0.1948\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1951 - val_mse: 0.1951\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1950 - val_mse: 0.1950\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1950 - val_mse: 0.1950\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1950 - val_mse: 0.1950\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1950 - val_mse: 0.1950\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1950 - val_mse: 0.1950\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1948 - val_mse: 0.1948\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2612 - mse: 0.2612 - val_loss: 0.1948 - val_mse: 0.1948\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1948 - val_mse: 0.1948\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1950 - val_mse: 0.1950\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1950 - val_mse: 0.1950\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1951 - val_mse: 0.1951\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1951 - val_mse: 0.1951\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1950 - val_mse: 0.1950\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1950 - val_mse: 0.1950\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1950 - val_mse: 0.1950\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1950 - val_mse: 0.1950\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1950 - val_mse: 0.1950\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.1950 - val_mse: 0.1950\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.1949 - mse: 0.1949\n",
      "\n",
      "Weight: [array([[1.9507166]], dtype=float32), array([0.9773846], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4833 - mse: 0.4833 - val_loss: 0.4242 - val_mse: 0.4242\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4829 - mse: 0.4829 - val_loss: 0.4264 - val_mse: 0.4264\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4826 - mse: 0.4826 - val_loss: 0.4271 - val_mse: 0.4271\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4825 - mse: 0.4825 - val_loss: 0.4288 - val_mse: 0.4288\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4824 - mse: 0.4824 - val_loss: 0.4299 - val_mse: 0.4299\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4825 - mse: 0.4825 - val_loss: 0.4308 - val_mse: 0.4308\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4824 - mse: 0.4824 - val_loss: 0.4311 - val_mse: 0.4311\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4824 - mse: 0.4824 - val_loss: 0.4316 - val_mse: 0.4316\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4822 - mse: 0.4822 - val_loss: 0.4316 - val_mse: 0.4316\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4825 - mse: 0.4825 - val_loss: 0.4314 - val_mse: 0.4314\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4320 - val_mse: 0.4320\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4822 - mse: 0.4822 - val_loss: 0.4322 - val_mse: 0.4322\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4822 - mse: 0.4822 - val_loss: 0.4326 - val_mse: 0.4326\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4322 - val_mse: 0.4322\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4822 - mse: 0.4822 - val_loss: 0.4318 - val_mse: 0.4318\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4325 - val_mse: 0.4325\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4825 - mse: 0.4825 - val_loss: 0.4331 - val_mse: 0.4331\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4331 - val_mse: 0.4331\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4334 - val_mse: 0.4334\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4332 - val_mse: 0.4332\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4339 - val_mse: 0.4339\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4329 - val_mse: 0.4329\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4330 - val_mse: 0.4330\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4326 - val_mse: 0.4326\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4824 - mse: 0.4824 - val_loss: 0.4324 - val_mse: 0.4324\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4324 - val_mse: 0.4324\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4324 - val_mse: 0.4324\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4330 - val_mse: 0.4330\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4824 - mse: 0.4824 - val_loss: 0.4331 - val_mse: 0.4331\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4336 - val_mse: 0.4336\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4331 - val_mse: 0.4331\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4822 - mse: 0.4822 - val_loss: 0.4330 - val_mse: 0.4330\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4328 - val_mse: 0.4328\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4824 - mse: 0.4824 - val_loss: 0.4325 - val_mse: 0.4325\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4822 - mse: 0.4822 - val_loss: 0.4330 - val_mse: 0.4330\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4824 - mse: 0.4824 - val_loss: 0.4328 - val_mse: 0.4328\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4824 - mse: 0.4824 - val_loss: 0.4326 - val_mse: 0.4326\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4328 - val_mse: 0.4328\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4825 - mse: 0.4825 - val_loss: 0.4334 - val_mse: 0.4334\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4329 - val_mse: 0.4329\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4821 - mse: 0.4821 - val_loss: 0.4336 - val_mse: 0.4336\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4331 - val_mse: 0.4331\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4822 - mse: 0.4822 - val_loss: 0.4331 - val_mse: 0.4331\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4822 - mse: 0.4822 - val_loss: 0.4329 - val_mse: 0.4329\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4824 - mse: 0.4824 - val_loss: 0.4329 - val_mse: 0.4329\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4822 - mse: 0.4822 - val_loss: 0.4329 - val_mse: 0.4329\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4824 - mse: 0.4824 - val_loss: 0.4332 - val_mse: 0.4332\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4328 - val_mse: 0.4328\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4824 - mse: 0.4824 - val_loss: 0.4331 - val_mse: 0.4331\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4822 - mse: 0.4822 - val_loss: 0.4334 - val_mse: 0.4334\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4334 - mse: 0.4334\n",
      "\n",
      "Weight: [array([[1.9942482]], dtype=float32), array([1.0062983], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7987 - mse: 0.7987 - val_loss: 0.6525 - val_mse: 0.6525\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7968 - mse: 0.7968 - val_loss: 0.6592 - val_mse: 0.6592\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7959 - mse: 0.7959 - val_loss: 0.6613 - val_mse: 0.6613\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7956 - mse: 0.7956 - val_loss: 0.6633 - val_mse: 0.6633\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7950 - mse: 0.7950 - val_loss: 0.6655 - val_mse: 0.6655\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7946 - mse: 0.7946 - val_loss: 0.6656 - val_mse: 0.6656\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7945 - mse: 0.7945 - val_loss: 0.6666 - val_mse: 0.6666\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7942 - mse: 0.7942 - val_loss: 0.6668 - val_mse: 0.6668\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7940 - mse: 0.7940 - val_loss: 0.6700 - val_mse: 0.6700\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7943 - mse: 0.7943 - val_loss: 0.6684 - val_mse: 0.6684\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7939 - mse: 0.7939 - val_loss: 0.6698 - val_mse: 0.6698\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7940 - mse: 0.7940 - val_loss: 0.6697 - val_mse: 0.6697\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7940 - mse: 0.7940 - val_loss: 0.6691 - val_mse: 0.6691\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7938 - mse: 0.7938 - val_loss: 0.6698 - val_mse: 0.6698\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7940 - mse: 0.7940 - val_loss: 0.6691 - val_mse: 0.6691\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7936 - mse: 0.7936 - val_loss: 0.6699 - val_mse: 0.6699\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7939 - mse: 0.7939 - val_loss: 0.6715 - val_mse: 0.6715\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7938 - mse: 0.7938 - val_loss: 0.6699 - val_mse: 0.6699\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7940 - mse: 0.7940 - val_loss: 0.6684 - val_mse: 0.6684\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7937 - mse: 0.7937 - val_loss: 0.6690 - val_mse: 0.6690\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7938 - mse: 0.7938 - val_loss: 0.6683 - val_mse: 0.6683\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7939 - mse: 0.7939 - val_loss: 0.6703 - val_mse: 0.6703\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7939 - mse: 0.7939 - val_loss: 0.6680 - val_mse: 0.6680\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7939 - mse: 0.7939 - val_loss: 0.6698 - val_mse: 0.6698\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7940 - mse: 0.7940 - val_loss: 0.6688 - val_mse: 0.6688\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7940 - mse: 0.7940 - val_loss: 0.6691 - val_mse: 0.6691\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7938 - mse: 0.7938 - val_loss: 0.6692 - val_mse: 0.6692\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7939 - mse: 0.7939 - val_loss: 0.6698 - val_mse: 0.6698\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7939 - mse: 0.7939 - val_loss: 0.6707 - val_mse: 0.6707\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7939 - mse: 0.7939 - val_loss: 0.6701 - val_mse: 0.6701\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7938 - mse: 0.7938 - val_loss: 0.6711 - val_mse: 0.6711\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7939 - mse: 0.7939 - val_loss: 0.6705 - val_mse: 0.6705\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7939 - mse: 0.7939 - val_loss: 0.6697 - val_mse: 0.6697\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7940 - mse: 0.7940 - val_loss: 0.6695 - val_mse: 0.6695\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7939 - mse: 0.7939 - val_loss: 0.6704 - val_mse: 0.6704\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7938 - mse: 0.7938 - val_loss: 0.6702 - val_mse: 0.6702\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7938 - mse: 0.7938 - val_loss: 0.6713 - val_mse: 0.6713\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7938 - mse: 0.7938 - val_loss: 0.6715 - val_mse: 0.6715\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7938 - mse: 0.7938 - val_loss: 0.6729 - val_mse: 0.6729\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7939 - mse: 0.7939 - val_loss: 0.6676 - val_mse: 0.6676\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7937 - mse: 0.7937 - val_loss: 0.6678 - val_mse: 0.6678\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7937 - mse: 0.7937 - val_loss: 0.6675 - val_mse: 0.6675\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7938 - mse: 0.7938 - val_loss: 0.6696 - val_mse: 0.6696\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7939 - mse: 0.7939 - val_loss: 0.6689 - val_mse: 0.6689\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7938 - mse: 0.7938 - val_loss: 0.6700 - val_mse: 0.6700\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7938 - mse: 0.7938 - val_loss: 0.6684 - val_mse: 0.6684\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7938 - mse: 0.7938 - val_loss: 0.6692 - val_mse: 0.6692\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7938 - mse: 0.7938 - val_loss: 0.6693 - val_mse: 0.6693\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7944 - mse: 0.7944 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7941 - mse: 0.7941 - val_loss: 0.6707 - val_mse: 0.6707\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.6707 - mse: 0.6707\n",
      "\n",
      "Weight: [array([[2.1131823]], dtype=float32), array([0.97347784], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1661 - mse: 1.1661 - val_loss: 0.9953 - val_mse: 0.9953\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1651 - mse: 1.1651 - val_loss: 0.9960 - val_mse: 0.9960\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1643 - mse: 1.1643 - val_loss: 0.9968 - val_mse: 0.9968\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1641 - mse: 1.1641 - val_loss: 0.9975 - val_mse: 0.9975\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1636 - mse: 1.1636 - val_loss: 0.9983 - val_mse: 0.9983\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1640 - mse: 1.1640 - val_loss: 0.9988 - val_mse: 0.9988\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1635 - mse: 1.1635 - val_loss: 0.9992 - val_mse: 0.9992\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1631 - mse: 1.1631 - val_loss: 0.9995 - val_mse: 0.9995\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1631 - mse: 1.1631 - val_loss: 0.9998 - val_mse: 0.9998\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1632 - mse: 1.1632 - val_loss: 1.0000 - val_mse: 1.0000\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1628 - mse: 1.1628 - val_loss: 1.0005 - val_mse: 1.0005\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1631 - mse: 1.1631 - val_loss: 1.0007 - val_mse: 1.0007\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1628 - mse: 1.1628 - val_loss: 1.0010 - val_mse: 1.0010\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1627 - mse: 1.1627 - val_loss: 1.0013 - val_mse: 1.0013\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1629 - mse: 1.1629 - val_loss: 1.0015 - val_mse: 1.0015\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1630 - mse: 1.1630 - val_loss: 1.0015 - val_mse: 1.0015\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1630 - mse: 1.1630 - val_loss: 1.0018 - val_mse: 1.0018\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1626 - mse: 1.1626 - val_loss: 1.0018 - val_mse: 1.0018\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1628 - mse: 1.1628 - val_loss: 1.0016 - val_mse: 1.0016\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1631 - mse: 1.1631 - val_loss: 1.0017 - val_mse: 1.0017\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1629 - mse: 1.1629 - val_loss: 1.0017 - val_mse: 1.0017\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1628 - mse: 1.1628 - val_loss: 1.0021 - val_mse: 1.0021\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1626 - mse: 1.1626 - val_loss: 1.0017 - val_mse: 1.0017\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1632 - mse: 1.1632 - val_loss: 1.0017 - val_mse: 1.0017\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1626 - mse: 1.1626 - val_loss: 1.0017 - val_mse: 1.0017\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1630 - mse: 1.1630 - val_loss: 1.0020 - val_mse: 1.0020\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1625 - mse: 1.1625 - val_loss: 1.0021 - val_mse: 1.0021\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1626 - mse: 1.1626 - val_loss: 1.0023 - val_mse: 1.0023\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1629 - mse: 1.1629 - val_loss: 1.0022 - val_mse: 1.0022\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1630 - mse: 1.1630 - val_loss: 1.0021 - val_mse: 1.0021\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1629 - mse: 1.1629 - val_loss: 1.0022 - val_mse: 1.0022\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1627 - mse: 1.1627 - val_loss: 1.0019 - val_mse: 1.0019\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1628 - mse: 1.1628 - val_loss: 1.0018 - val_mse: 1.0018\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1627 - mse: 1.1627 - val_loss: 1.0016 - val_mse: 1.0016\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1630 - mse: 1.1630 - val_loss: 1.0017 - val_mse: 1.0017\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1630 - mse: 1.1630 - val_loss: 1.0018 - val_mse: 1.0018\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1626 - mse: 1.1626 - val_loss: 1.0021 - val_mse: 1.0021\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1627 - mse: 1.1627 - val_loss: 1.0022 - val_mse: 1.0022\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1631 - mse: 1.1631 - val_loss: 1.0021 - val_mse: 1.0021\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1625 - mse: 1.1625 - val_loss: 1.0021 - val_mse: 1.0021\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1628 - mse: 1.1628 - val_loss: 1.0023 - val_mse: 1.0023\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1629 - mse: 1.1629 - val_loss: 1.0023 - val_mse: 1.0023\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1628 - mse: 1.1628 - val_loss: 1.0022 - val_mse: 1.0022\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1627 - mse: 1.1627 - val_loss: 1.0023 - val_mse: 1.0023\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1631 - mse: 1.1631 - val_loss: 1.0021 - val_mse: 1.0021\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1628 - mse: 1.1628 - val_loss: 1.0022 - val_mse: 1.0022\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1629 - mse: 1.1629 - val_loss: 1.0021 - val_mse: 1.0021\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1625 - mse: 1.1625 - val_loss: 1.0019 - val_mse: 1.0019\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1626 - mse: 1.1626 - val_loss: 1.0017 - val_mse: 1.0017\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1628 - mse: 1.1628 - val_loss: 1.0018 - val_mse: 1.0018\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.0018 - mse: 1.0018\n",
      "\n",
      "Weight: [array([[2.0095468]], dtype=float32), array([0.994123], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0128 - mse: 0.0128\n",
      "\n",
      "Weight: [array([[2.0061858]], dtype=float32), array([0.99845374], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0898 - mse: 0.0898 - val_loss: 0.0763 - val_mse: 0.0763\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0899 - mse: 0.0899 - val_loss: 0.0764 - val_mse: 0.0764\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0898 - mse: 0.0898 - val_loss: 0.0765 - val_mse: 0.0765\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0765 - val_mse: 0.0765\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0766 - val_mse: 0.0766\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0765 - val_mse: 0.0765\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0765 - val_mse: 0.0765\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0765 - val_mse: 0.0765\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0766 - val_mse: 0.0766\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0766 - val_mse: 0.0766\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0766 - val_mse: 0.0766\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0765 - val_mse: 0.0765\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0764 - val_mse: 0.0764\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0898 - mse: 0.0898 - val_loss: 0.0765 - val_mse: 0.0765\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0769 - val_mse: 0.0769\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0770 - val_mse: 0.0770\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0770 - val_mse: 0.0770\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0770 - val_mse: 0.0770\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0770 - val_mse: 0.0770\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0769 - val_mse: 0.0769\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0770 - val_mse: 0.0770\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0765 - val_mse: 0.0765\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0766 - val_mse: 0.0766\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0769 - val_mse: 0.0769\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0766 - val_mse: 0.0766\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0769 - val_mse: 0.0769\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0767 - val_mse: 0.0767\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0767 - mse: 0.0767\n",
      "\n",
      "Weight: [array([[1.9907788]], dtype=float32), array([0.98818225], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2374 - mse: 0.2374 - val_loss: 0.2399 - val_mse: 0.2399\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2368 - mse: 0.2368 - val_loss: 0.2417 - val_mse: 0.2417\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2420 - val_mse: 0.2420\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2413 - val_mse: 0.2413\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2430 - val_mse: 0.2430\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2451 - val_mse: 0.2451\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2435 - val_mse: 0.2435\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2435 - val_mse: 0.2435\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2433 - val_mse: 0.2433\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2430 - val_mse: 0.2430\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2429 - val_mse: 0.2429\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2426 - val_mse: 0.2426\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2431 - val_mse: 0.2431\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2430 - val_mse: 0.2430\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2432 - val_mse: 0.2432\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2425 - val_mse: 0.2425\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2427 - val_mse: 0.2427\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2436 - val_mse: 0.2436\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2435 - val_mse: 0.2435\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2431 - val_mse: 0.2431\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2434 - val_mse: 0.2434\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2438 - val_mse: 0.2438\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2440 - val_mse: 0.2440\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2430 - val_mse: 0.2430\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2418 - val_mse: 0.2418\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2420 - val_mse: 0.2420\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2421 - val_mse: 0.2421\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2420 - val_mse: 0.2420\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2432 - val_mse: 0.2432\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2433 - val_mse: 0.2433\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2425 - val_mse: 0.2425\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2419 - val_mse: 0.2419\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2415 - val_mse: 0.2415\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2422 - val_mse: 0.2422\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2425 - val_mse: 0.2425\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2430 - val_mse: 0.2430\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2429 - val_mse: 0.2429\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2426 - val_mse: 0.2426\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2449 - val_mse: 0.2449\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2435 - val_mse: 0.2435\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2441 - val_mse: 0.2441\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2434 - val_mse: 0.2434\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2431 - val_mse: 0.2431\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2426 - val_mse: 0.2426\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2415 - val_mse: 0.2415\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2361 - mse: 0.2361 - val_loss: 0.2413 - val_mse: 0.2413\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2431 - val_mse: 0.2431\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2415 - val_mse: 0.2415\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2412 - val_mse: 0.2412\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2425 - val_mse: 0.2425\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2425 - mse: 0.2425\n",
      "\n",
      "Weight: [array([[2.0302067]], dtype=float32), array([1.0200715], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4588 - mse: 0.4588 - val_loss: 0.5913 - val_mse: 0.5913\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4585 - mse: 0.4585 - val_loss: 0.5939 - val_mse: 0.5939\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5934 - val_mse: 0.5934\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4584 - mse: 0.4584 - val_loss: 0.5932 - val_mse: 0.5932\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4583 - mse: 0.4583 - val_loss: 0.5939 - val_mse: 0.5939\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4581 - mse: 0.4581 - val_loss: 0.5967 - val_mse: 0.5967\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4583 - mse: 0.4583 - val_loss: 0.5962 - val_mse: 0.5962\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4581 - mse: 0.4581 - val_loss: 0.5990 - val_mse: 0.5990\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4581 - mse: 0.4581 - val_loss: 0.5940 - val_mse: 0.5940\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5948 - val_mse: 0.5948\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5980 - val_mse: 0.5980\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5983 - val_mse: 0.5983\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.6000 - val_mse: 0.6000\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4581 - mse: 0.4581 - val_loss: 0.5940 - val_mse: 0.5940\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4583 - mse: 0.4583 - val_loss: 0.5932 - val_mse: 0.5932\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5931 - val_mse: 0.5931\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4583 - mse: 0.4583 - val_loss: 0.5959 - val_mse: 0.5959\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4581 - mse: 0.4581 - val_loss: 0.6004 - val_mse: 0.6004\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5982 - val_mse: 0.5982\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4581 - mse: 0.4581 - val_loss: 0.5977 - val_mse: 0.5977\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5929 - val_mse: 0.5929\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4581 - mse: 0.4581 - val_loss: 0.5981 - val_mse: 0.5981\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5941 - val_mse: 0.5941\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5971 - val_mse: 0.5971\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4581 - mse: 0.4581 - val_loss: 0.6017 - val_mse: 0.6017\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4583 - mse: 0.4583 - val_loss: 0.5953 - val_mse: 0.5953\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5954 - val_mse: 0.5954\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4581 - mse: 0.4581 - val_loss: 0.5910 - val_mse: 0.5910\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4583 - mse: 0.4583 - val_loss: 0.5919 - val_mse: 0.5919\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5957 - val_mse: 0.5957\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5963 - val_mse: 0.5963\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5928 - val_mse: 0.5928\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5944 - val_mse: 0.5944\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4581 - mse: 0.4581 - val_loss: 0.5976 - val_mse: 0.5976\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5967 - val_mse: 0.5967\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4581 - mse: 0.4581 - val_loss: 0.5989 - val_mse: 0.5989\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5989 - val_mse: 0.5989\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4583 - mse: 0.4583 - val_loss: 0.5990 - val_mse: 0.5990\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5990 - val_mse: 0.5990\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5943 - val_mse: 0.5943\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4581 - mse: 0.4581 - val_loss: 0.5934 - val_mse: 0.5934\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4581 - mse: 0.4581 - val_loss: 0.5979 - val_mse: 0.5979\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5972 - val_mse: 0.5972\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5970 - val_mse: 0.5970\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4581 - mse: 0.4581 - val_loss: 0.5978 - val_mse: 0.5978\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4581 - mse: 0.4581 - val_loss: 0.5991 - val_mse: 0.5991\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5994 - val_mse: 0.5994\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.5974 - val_mse: 0.5974\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4582 - mse: 0.4582 - val_loss: 0.6008 - val_mse: 0.6008\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4583 - mse: 0.4583 - val_loss: 0.6019 - val_mse: 0.6019\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.6019 - mse: 0.6019\n",
      "\n",
      "Weight: [array([[2.0368848]], dtype=float32), array([0.9793196], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7607 - mse: 0.7607 - val_loss: 1.4183 - val_mse: 1.4183\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 1.4187 - val_mse: 1.4187\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 1.4194 - val_mse: 1.4194\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7603 - mse: 0.7603 - val_loss: 1.4212 - val_mse: 1.4212\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7607 - mse: 0.7607 - val_loss: 1.4206 - val_mse: 1.4206\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 1.4197 - val_mse: 1.4197\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 1.4195 - val_mse: 1.4195\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7607 - mse: 0.7607 - val_loss: 1.4204 - val_mse: 1.4204\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7603 - mse: 0.7603 - val_loss: 1.4219 - val_mse: 1.4219\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7606 - mse: 0.7606 - val_loss: 1.4213 - val_mse: 1.4213\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7608 - mse: 0.7608 - val_loss: 1.4198 - val_mse: 1.4198\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7603 - mse: 0.7603 - val_loss: 1.4188 - val_mse: 1.4188\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7604 - mse: 0.7604 - val_loss: 1.4191 - val_mse: 1.4191\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 1.4182 - val_mse: 1.4182\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 1.4188 - val_mse: 1.4188\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 1.4192 - val_mse: 1.4192\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 1.4205 - val_mse: 1.4205\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 1.4191 - val_mse: 1.4191\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7607 - mse: 0.7607 - val_loss: 1.4196 - val_mse: 1.4196\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 1.4178 - val_mse: 1.4178\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7606 - mse: 0.7606 - val_loss: 1.4172 - val_mse: 1.4172\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7609 - mse: 0.7609 - val_loss: 1.4175 - val_mse: 1.4175\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7611 - mse: 0.7611 - val_loss: 1.4187 - val_mse: 1.4187\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7607 - mse: 0.7607 - val_loss: 1.4198 - val_mse: 1.4198\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7607 - mse: 0.7607 - val_loss: 1.4196 - val_mse: 1.4196\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7606 - mse: 0.7606 - val_loss: 1.4200 - val_mse: 1.4200\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7607 - mse: 0.7607 - val_loss: 1.4204 - val_mse: 1.4204\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 1.4213 - val_mse: 1.4213\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7606 - mse: 0.7606 - val_loss: 1.4220 - val_mse: 1.4220\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7607 - mse: 0.7607 - val_loss: 1.4212 - val_mse: 1.4212\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7604 - mse: 0.7604 - val_loss: 1.4217 - val_mse: 1.4217\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7606 - mse: 0.7606 - val_loss: 1.4212 - val_mse: 1.4212\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 1.4212 - val_mse: 1.4212\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 1.4224 - val_mse: 1.4224\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7606 - mse: 0.7606 - val_loss: 1.4204 - val_mse: 1.4204\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7603 - mse: 0.7603 - val_loss: 1.4213 - val_mse: 1.4213\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7604 - mse: 0.7604 - val_loss: 1.4216 - val_mse: 1.4216\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7606 - mse: 0.7606 - val_loss: 1.4209 - val_mse: 1.4209\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 1.4202 - val_mse: 1.4202\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7604 - mse: 0.7604 - val_loss: 1.4188 - val_mse: 1.4188\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7606 - mse: 0.7606 - val_loss: 1.4212 - val_mse: 1.4212\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 1.4209 - val_mse: 1.4209\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 1.4213 - val_mse: 1.4213\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7604 - mse: 0.7604 - val_loss: 1.4209 - val_mse: 1.4209\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7604 - mse: 0.7604 - val_loss: 1.4199 - val_mse: 1.4199\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7603 - mse: 0.7603 - val_loss: 1.4216 - val_mse: 1.4216\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7607 - mse: 0.7607 - val_loss: 1.4200 - val_mse: 1.4200\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7606 - mse: 0.7606 - val_loss: 1.4196 - val_mse: 1.4196\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 1.4199 - val_mse: 1.4199\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7607 - mse: 0.7607 - val_loss: 1.4186 - val_mse: 1.4186\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.4186 - mse: 1.4186\n",
      "\n",
      "Weight: [array([[2.0470588]], dtype=float32), array([0.9752076], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 1.2968 - mse: 1.2968 - val_loss: 1.3537 - val_mse: 1.3537\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2969 - mse: 1.2969 - val_loss: 1.3585 - val_mse: 1.3585\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2964 - mse: 1.2964 - val_loss: 1.3599 - val_mse: 1.3599\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2967 - mse: 1.2967 - val_loss: 1.3678 - val_mse: 1.3678\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2960 - mse: 1.2960 - val_loss: 1.3744 - val_mse: 1.3744\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2962 - mse: 1.2962 - val_loss: 1.3852 - val_mse: 1.3852\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2963 - mse: 1.2963 - val_loss: 1.4005 - val_mse: 1.4005\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2962 - mse: 1.2962 - val_loss: 1.4041 - val_mse: 1.4041\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2963 - mse: 1.2963 - val_loss: 1.4006 - val_mse: 1.4006\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2961 - mse: 1.2961 - val_loss: 1.3934 - val_mse: 1.3934\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2965 - mse: 1.2965 - val_loss: 1.3854 - val_mse: 1.3854\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2959 - mse: 1.2959 - val_loss: 1.3704 - val_mse: 1.3704\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2959 - mse: 1.2959 - val_loss: 1.3759 - val_mse: 1.3759\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2959 - mse: 1.2959 - val_loss: 1.3680 - val_mse: 1.3680\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2963 - mse: 1.2963 - val_loss: 1.3766 - val_mse: 1.3766\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2964 - mse: 1.2964 - val_loss: 1.3722 - val_mse: 1.3722\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2961 - mse: 1.2961 - val_loss: 1.3835 - val_mse: 1.3835\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2961 - mse: 1.2961 - val_loss: 1.3939 - val_mse: 1.3939\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2963 - mse: 1.2963 - val_loss: 1.3830 - val_mse: 1.3830\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2958 - mse: 1.2958 - val_loss: 1.3695 - val_mse: 1.3695\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2962 - mse: 1.2962 - val_loss: 1.3631 - val_mse: 1.3631\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2966 - mse: 1.2966 - val_loss: 1.3816 - val_mse: 1.3816\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2960 - mse: 1.2960 - val_loss: 1.3823 - val_mse: 1.3823\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2959 - mse: 1.2959 - val_loss: 1.3834 - val_mse: 1.3834\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2959 - mse: 1.2959 - val_loss: 1.3762 - val_mse: 1.3762\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2967 - mse: 1.2967 - val_loss: 1.3848 - val_mse: 1.3848\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2964 - mse: 1.2964 - val_loss: 1.3775 - val_mse: 1.3775\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2961 - mse: 1.2961 - val_loss: 1.3777 - val_mse: 1.3777\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2961 - mse: 1.2961 - val_loss: 1.3821 - val_mse: 1.3821\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2961 - mse: 1.2961 - val_loss: 1.3780 - val_mse: 1.3780\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2964 - mse: 1.2964 - val_loss: 1.3854 - val_mse: 1.3854\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2958 - mse: 1.2958 - val_loss: 1.3843 - val_mse: 1.3843\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2961 - mse: 1.2961 - val_loss: 1.3802 - val_mse: 1.3802\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2961 - mse: 1.2961 - val_loss: 1.3831 - val_mse: 1.3831\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2961 - mse: 1.2961 - val_loss: 1.3719 - val_mse: 1.3719\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2961 - mse: 1.2961 - val_loss: 1.3779 - val_mse: 1.3779\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2961 - mse: 1.2961 - val_loss: 1.3834 - val_mse: 1.3834\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2960 - mse: 1.2960 - val_loss: 1.3829 - val_mse: 1.3829\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2959 - mse: 1.2959 - val_loss: 1.3797 - val_mse: 1.3797\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2963 - mse: 1.2963 - val_loss: 1.3661 - val_mse: 1.3661\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2967 - mse: 1.2967 - val_loss: 1.3734 - val_mse: 1.3734\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2967 - mse: 1.2967 - val_loss: 1.3771 - val_mse: 1.3771\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2959 - mse: 1.2959 - val_loss: 1.3737 - val_mse: 1.3737\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2966 - mse: 1.2966 - val_loss: 1.3875 - val_mse: 1.3875\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2964 - mse: 1.2964 - val_loss: 1.3773 - val_mse: 1.3773\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2964 - mse: 1.2964 - val_loss: 1.3924 - val_mse: 1.3924\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2961 - mse: 1.2961 - val_loss: 1.3865 - val_mse: 1.3865\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2961 - mse: 1.2961 - val_loss: 1.3882 - val_mse: 1.3882\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2960 - mse: 1.2960 - val_loss: 1.3970 - val_mse: 1.3970\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.2960 - mse: 1.2960 - val_loss: 1.3989 - val_mse: 1.3989\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.3989 - mse: 1.3989\n",
      "\n",
      "Weight: [array([[1.9975263]], dtype=float32), array([0.95952815], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0092 - mse: 0.0092\n",
      "\n",
      "Weight: [array([[1.9929336]], dtype=float32), array([0.9966053], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0831 - mse: 0.0831 - val_loss: 0.0883 - val_mse: 0.0883\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0831 - mse: 0.0831 - val_loss: 0.0884 - val_mse: 0.0884\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0831 - mse: 0.0831 - val_loss: 0.0886 - val_mse: 0.0886\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0889 - val_mse: 0.0889\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0888 - val_mse: 0.0888\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0831 - mse: 0.0831 - val_loss: 0.0888 - val_mse: 0.0888\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0887 - val_mse: 0.0887\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0888 - val_mse: 0.0888\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0887 - val_mse: 0.0887\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0889 - val_mse: 0.0889\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0888 - val_mse: 0.0888\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0890 - val_mse: 0.0890\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0831 - mse: 0.0831 - val_loss: 0.0890 - val_mse: 0.0890\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0891 - val_mse: 0.0891\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0891 - val_mse: 0.0891\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0890 - val_mse: 0.0890\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0891 - val_mse: 0.0891\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0892 - val_mse: 0.0892\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0831 - mse: 0.0831 - val_loss: 0.0892 - val_mse: 0.0892\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0891 - val_mse: 0.0891\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0892 - val_mse: 0.0892\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0891 - val_mse: 0.0891\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0892 - val_mse: 0.0892\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0892 - val_mse: 0.0892\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0893 - val_mse: 0.0893\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0893 - val_mse: 0.0893\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0892 - val_mse: 0.0892\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0894 - val_mse: 0.0894\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0894 - val_mse: 0.0894\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0891 - val_mse: 0.0891\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0889 - val_mse: 0.0889\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0831 - mse: 0.0831 - val_loss: 0.0889 - val_mse: 0.0889\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0889 - val_mse: 0.0889\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0890 - val_mse: 0.0890\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0889 - val_mse: 0.0889\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0889 - val_mse: 0.0889\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0831 - mse: 0.0831 - val_loss: 0.0891 - val_mse: 0.0891\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0890 - val_mse: 0.0890\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0891 - val_mse: 0.0891\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0890 - val_mse: 0.0890\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0890 - val_mse: 0.0890\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0891 - val_mse: 0.0891\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0892 - val_mse: 0.0892\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0891 - val_mse: 0.0891\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0891 - val_mse: 0.0891\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0831 - mse: 0.0831 - val_loss: 0.0892 - val_mse: 0.0892\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0889 - val_mse: 0.0889\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0889 - val_mse: 0.0889\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0889 - val_mse: 0.0889\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0888 - val_mse: 0.0888\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0888 - mse: 0.0888\n",
      "\n",
      "Weight: [array([[2.0011039]], dtype=float32), array([0.98922324], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2516 - val_mse: 0.2516\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2408 - mse: 0.2408 - val_loss: 0.2525 - val_mse: 0.2525\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2408 - mse: 0.2408 - val_loss: 0.2529 - val_mse: 0.2529\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2408 - mse: 0.2408 - val_loss: 0.2528 - val_mse: 0.2528\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2530 - val_mse: 0.2530\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2535 - val_mse: 0.2535\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2528 - val_mse: 0.2528\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2528 - val_mse: 0.2528\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2408 - mse: 0.2408 - val_loss: 0.2523 - val_mse: 0.2523\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2529 - val_mse: 0.2529\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2518 - val_mse: 0.2518\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2519 - val_mse: 0.2519\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2519 - val_mse: 0.2519\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2408 - mse: 0.2408 - val_loss: 0.2515 - val_mse: 0.2515\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2524 - val_mse: 0.2524\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2408 - mse: 0.2408 - val_loss: 0.2517 - val_mse: 0.2517\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2515 - val_mse: 0.2515\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2520 - val_mse: 0.2520\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2522 - val_mse: 0.2522\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2531 - val_mse: 0.2531\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2530 - val_mse: 0.2530\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2531 - val_mse: 0.2531\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2408 - mse: 0.2408 - val_loss: 0.2528 - val_mse: 0.2528\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2408 - mse: 0.2408 - val_loss: 0.2531 - val_mse: 0.2531\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2528 - val_mse: 0.2528\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2515 - val_mse: 0.2515\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2523 - val_mse: 0.2523\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2526 - val_mse: 0.2526\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2519 - val_mse: 0.2519\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2522 - val_mse: 0.2522\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2538 - val_mse: 0.2538\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2537 - val_mse: 0.2537\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2535 - val_mse: 0.2535\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2524 - val_mse: 0.2524\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2517 - val_mse: 0.2517\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2523 - val_mse: 0.2523\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2408 - mse: 0.2408 - val_loss: 0.2530 - val_mse: 0.2530\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2524 - val_mse: 0.2524\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2519 - val_mse: 0.2519\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2518 - val_mse: 0.2518\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2530 - val_mse: 0.2530\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2515 - val_mse: 0.2515\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2408 - mse: 0.2408 - val_loss: 0.2523 - val_mse: 0.2523\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2525 - val_mse: 0.2525\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2518 - val_mse: 0.2518\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2523 - val_mse: 0.2523\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2526 - val_mse: 0.2526\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2526 - val_mse: 0.2526\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2408 - mse: 0.2408 - val_loss: 0.2518 - val_mse: 0.2518\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2530 - val_mse: 0.2530\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2530 - mse: 0.2530\n",
      "\n",
      "Weight: [array([[1.9948971]], dtype=float32), array([0.974651], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5046 - mse: 0.5046 - val_loss: 0.4756 - val_mse: 0.4756\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5042 - mse: 0.5042 - val_loss: 0.4748 - val_mse: 0.4748\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5041 - mse: 0.5041 - val_loss: 0.4745 - val_mse: 0.4745\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5041 - mse: 0.5041 - val_loss: 0.4744 - val_mse: 0.4744\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5040 - mse: 0.5040 - val_loss: 0.4742 - val_mse: 0.4742\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5041 - mse: 0.5041 - val_loss: 0.4746 - val_mse: 0.4746\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5041 - mse: 0.5041 - val_loss: 0.4742 - val_mse: 0.4742\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5042 - mse: 0.5042 - val_loss: 0.4738 - val_mse: 0.4738\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5041 - mse: 0.5041 - val_loss: 0.4739 - val_mse: 0.4739\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5041 - mse: 0.5041 - val_loss: 0.4739 - val_mse: 0.4739\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5041 - mse: 0.5041 - val_loss: 0.4744 - val_mse: 0.4744\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5039 - mse: 0.5039 - val_loss: 0.4748 - val_mse: 0.4748\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5040 - mse: 0.5040 - val_loss: 0.4745 - val_mse: 0.4745\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5040 - mse: 0.5040 - val_loss: 0.4742 - val_mse: 0.4742\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5040 - mse: 0.5040 - val_loss: 0.4746 - val_mse: 0.4746\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5041 - mse: 0.5041 - val_loss: 0.4747 - val_mse: 0.4747\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5042 - mse: 0.5042 - val_loss: 0.4746 - val_mse: 0.4746\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5041 - mse: 0.5041 - val_loss: 0.4743 - val_mse: 0.4743\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5041 - mse: 0.5041 - val_loss: 0.4746 - val_mse: 0.4746\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5041 - mse: 0.5041 - val_loss: 0.4749 - val_mse: 0.4749\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5040 - mse: 0.5040 - val_loss: 0.4751 - val_mse: 0.4751\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5040 - mse: 0.5040 - val_loss: 0.4755 - val_mse: 0.4755\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5041 - mse: 0.5041 - val_loss: 0.4753 - val_mse: 0.4753\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5039 - mse: 0.5039 - val_loss: 0.4745 - val_mse: 0.4745\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5039 - mse: 0.5039 - val_loss: 0.4745 - val_mse: 0.4745\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5040 - mse: 0.5040 - val_loss: 0.4748 - val_mse: 0.4748\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5040 - mse: 0.5040 - val_loss: 0.4746 - val_mse: 0.4746\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5042 - mse: 0.5042 - val_loss: 0.4750 - val_mse: 0.4750\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5042 - mse: 0.5042 - val_loss: 0.4751 - val_mse: 0.4751\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5040 - mse: 0.5040 - val_loss: 0.4750 - val_mse: 0.4750\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5039 - mse: 0.5039 - val_loss: 0.4755 - val_mse: 0.4755\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5041 - mse: 0.5041 - val_loss: 0.4750 - val_mse: 0.4750\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5041 - mse: 0.5041 - val_loss: 0.4748 - val_mse: 0.4748\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5040 - mse: 0.5040 - val_loss: 0.4743 - val_mse: 0.4743\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5039 - mse: 0.5039 - val_loss: 0.4744 - val_mse: 0.4744\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5039 - mse: 0.5039 - val_loss: 0.4745 - val_mse: 0.4745\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5040 - mse: 0.5040 - val_loss: 0.4736 - val_mse: 0.4736\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5043 - mse: 0.5043 - val_loss: 0.4739 - val_mse: 0.4739\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5039 - mse: 0.5039 - val_loss: 0.4742 - val_mse: 0.4742\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5040 - mse: 0.5040 - val_loss: 0.4751 - val_mse: 0.4751\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5040 - mse: 0.5040 - val_loss: 0.4745 - val_mse: 0.4745\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5041 - mse: 0.5041 - val_loss: 0.4746 - val_mse: 0.4746\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5039 - mse: 0.5039 - val_loss: 0.4746 - val_mse: 0.4746\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5040 - mse: 0.5040 - val_loss: 0.4745 - val_mse: 0.4745\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5040 - mse: 0.5040 - val_loss: 0.4748 - val_mse: 0.4748\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5039 - mse: 0.5039 - val_loss: 0.4743 - val_mse: 0.4743\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5041 - mse: 0.5041 - val_loss: 0.4746 - val_mse: 0.4746\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5041 - mse: 0.5041 - val_loss: 0.4748 - val_mse: 0.4748\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5040 - mse: 0.5040 - val_loss: 0.4755 - val_mse: 0.4755\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5039 - mse: 0.5039 - val_loss: 0.4752 - val_mse: 0.4752\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4752 - mse: 0.4752\n",
      "\n",
      "Weight: [array([[2.0063176]], dtype=float32), array([1.0075884], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8112 - mse: 0.8112 - val_loss: 0.7345 - val_mse: 0.7345\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8106 - mse: 0.8106 - val_loss: 0.7373 - val_mse: 0.7373\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8102 - mse: 0.8102 - val_loss: 0.7368 - val_mse: 0.7368\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8100 - mse: 0.8100 - val_loss: 0.7376 - val_mse: 0.7376\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8100 - mse: 0.8100 - val_loss: 0.7382 - val_mse: 0.7382\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8100 - mse: 0.8100 - val_loss: 0.7366 - val_mse: 0.7366\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8098 - mse: 0.8098 - val_loss: 0.7371 - val_mse: 0.7371\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8096 - mse: 0.8096 - val_loss: 0.7371 - val_mse: 0.7371\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8097 - mse: 0.8097 - val_loss: 0.7375 - val_mse: 0.7375\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8098 - mse: 0.8098 - val_loss: 0.7362 - val_mse: 0.7362\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8097 - mse: 0.8097 - val_loss: 0.7367 - val_mse: 0.7367\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8099 - mse: 0.8099 - val_loss: 0.7376 - val_mse: 0.7376\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8097 - mse: 0.8097 - val_loss: 0.7375 - val_mse: 0.7375\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8096 - mse: 0.8096 - val_loss: 0.7373 - val_mse: 0.7373\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8096 - mse: 0.8096 - val_loss: 0.7370 - val_mse: 0.7370\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8098 - mse: 0.8098 - val_loss: 0.7388 - val_mse: 0.7388\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8099 - mse: 0.8099 - val_loss: 0.7372 - val_mse: 0.7372\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8098 - mse: 0.8098 - val_loss: 0.7359 - val_mse: 0.7359\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8096 - mse: 0.8096 - val_loss: 0.7353 - val_mse: 0.7353\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8100 - mse: 0.8100 - val_loss: 0.7364 - val_mse: 0.7364\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8099 - mse: 0.8099 - val_loss: 0.7367 - val_mse: 0.7367\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8098 - mse: 0.8098 - val_loss: 0.7377 - val_mse: 0.7377\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8099 - mse: 0.8099 - val_loss: 0.7393 - val_mse: 0.7393\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8102 - mse: 0.8102 - val_loss: 0.7368 - val_mse: 0.7368\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8100 - mse: 0.8100 - val_loss: 0.7376 - val_mse: 0.7376\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8097 - mse: 0.8097 - val_loss: 0.7364 - val_mse: 0.7364\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8099 - mse: 0.8099 - val_loss: 0.7387 - val_mse: 0.7387\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8098 - mse: 0.8098 - val_loss: 0.7383 - val_mse: 0.7383\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8100 - mse: 0.8100 - val_loss: 0.7381 - val_mse: 0.7381\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8097 - mse: 0.8097 - val_loss: 0.7376 - val_mse: 0.7376\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8100 - mse: 0.8100 - val_loss: 0.7390 - val_mse: 0.7390\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8098 - mse: 0.8098 - val_loss: 0.7376 - val_mse: 0.7376\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8097 - mse: 0.8097 - val_loss: 0.7383 - val_mse: 0.7383\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8098 - mse: 0.8098 - val_loss: 0.7373 - val_mse: 0.7373\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8097 - mse: 0.8097 - val_loss: 0.7371 - val_mse: 0.7371\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8098 - mse: 0.8098 - val_loss: 0.7370 - val_mse: 0.7370\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8097 - mse: 0.8097 - val_loss: 0.7360 - val_mse: 0.7360\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8098 - mse: 0.8098 - val_loss: 0.7359 - val_mse: 0.7359\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8098 - mse: 0.8098 - val_loss: 0.7355 - val_mse: 0.7355\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8097 - mse: 0.8097 - val_loss: 0.7383 - val_mse: 0.7383\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8100 - mse: 0.8100 - val_loss: 0.7374 - val_mse: 0.7374\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8099 - mse: 0.8099 - val_loss: 0.7388 - val_mse: 0.7388\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8098 - mse: 0.8098 - val_loss: 0.7398 - val_mse: 0.7398\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8098 - mse: 0.8098 - val_loss: 0.7396 - val_mse: 0.7396\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8098 - mse: 0.8098 - val_loss: 0.7380 - val_mse: 0.7380\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8099 - mse: 0.8099 - val_loss: 0.7371 - val_mse: 0.7371\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8100 - mse: 0.8100 - val_loss: 0.7361 - val_mse: 0.7361\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8100 - mse: 0.8100 - val_loss: 0.7367 - val_mse: 0.7367\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8100 - mse: 0.8100 - val_loss: 0.7364 - val_mse: 0.7364\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8097 - mse: 0.8097 - val_loss: 0.7368 - val_mse: 0.7368\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.7368 - mse: 0.7368\n",
      "\n",
      "Weight: [array([[2.067652]], dtype=float32), array([0.98265815], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1505 - mse: 1.1505 - val_loss: 1.3579 - val_mse: 1.3579\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1461 - mse: 1.1461 - val_loss: 1.3531 - val_mse: 1.3531\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1444 - mse: 1.1444 - val_loss: 1.3522 - val_mse: 1.3522\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1436 - mse: 1.1436 - val_loss: 1.3512 - val_mse: 1.3512\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1429 - mse: 1.1429 - val_loss: 1.3528 - val_mse: 1.3528\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1430 - mse: 1.1430 - val_loss: 1.3535 - val_mse: 1.3535\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1426 - mse: 1.1426 - val_loss: 1.3527 - val_mse: 1.3527\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1426 - mse: 1.1426 - val_loss: 1.3560 - val_mse: 1.3560\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1426 - mse: 1.1426 - val_loss: 1.3566 - val_mse: 1.3566\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1425 - mse: 1.1425 - val_loss: 1.3574 - val_mse: 1.3574\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1427 - mse: 1.1427 - val_loss: 1.3573 - val_mse: 1.3573\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1426 - mse: 1.1426 - val_loss: 1.3567 - val_mse: 1.3567\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1425 - mse: 1.1425 - val_loss: 1.3543 - val_mse: 1.3543\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1425 - mse: 1.1425 - val_loss: 1.3566 - val_mse: 1.3566\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1424 - mse: 1.1424 - val_loss: 1.3570 - val_mse: 1.3570\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1423 - mse: 1.1423 - val_loss: 1.3563 - val_mse: 1.3563\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1426 - mse: 1.1426 - val_loss: 1.3565 - val_mse: 1.3565\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1428 - mse: 1.1428 - val_loss: 1.3579 - val_mse: 1.3579\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1425 - mse: 1.1425 - val_loss: 1.3559 - val_mse: 1.3559\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1424 - mse: 1.1424 - val_loss: 1.3553 - val_mse: 1.3553\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1424 - mse: 1.1424 - val_loss: 1.3565 - val_mse: 1.3565\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1428 - mse: 1.1428 - val_loss: 1.3563 - val_mse: 1.3563\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1423 - mse: 1.1423 - val_loss: 1.3597 - val_mse: 1.3597\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1426 - mse: 1.1426 - val_loss: 1.3558 - val_mse: 1.3558\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1424 - mse: 1.1424 - val_loss: 1.3566 - val_mse: 1.3566\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1423 - mse: 1.1423 - val_loss: 1.3543 - val_mse: 1.3543\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1426 - mse: 1.1426 - val_loss: 1.3535 - val_mse: 1.3535\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1427 - mse: 1.1427 - val_loss: 1.3556 - val_mse: 1.3556\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1428 - mse: 1.1428 - val_loss: 1.3563 - val_mse: 1.3563\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1424 - mse: 1.1424 - val_loss: 1.3565 - val_mse: 1.3565\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1427 - mse: 1.1427 - val_loss: 1.3572 - val_mse: 1.3572\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1425 - mse: 1.1425 - val_loss: 1.3540 - val_mse: 1.3540\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1427 - mse: 1.1427 - val_loss: 1.3550 - val_mse: 1.3550\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1425 - mse: 1.1425 - val_loss: 1.3542 - val_mse: 1.3542\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1425 - mse: 1.1425 - val_loss: 1.3536 - val_mse: 1.3536\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1427 - mse: 1.1427 - val_loss: 1.3552 - val_mse: 1.3552\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1427 - mse: 1.1427 - val_loss: 1.3564 - val_mse: 1.3564\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1423 - mse: 1.1423 - val_loss: 1.3556 - val_mse: 1.3556\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1425 - mse: 1.1425 - val_loss: 1.3545 - val_mse: 1.3545\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1429 - mse: 1.1429 - val_loss: 1.3558 - val_mse: 1.3558\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1427 - mse: 1.1427 - val_loss: 1.3580 - val_mse: 1.3580\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1428 - mse: 1.1428 - val_loss: 1.3574 - val_mse: 1.3574\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1427 - mse: 1.1427 - val_loss: 1.3588 - val_mse: 1.3588\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1426 - mse: 1.1426 - val_loss: 1.3576 - val_mse: 1.3576\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1427 - mse: 1.1427 - val_loss: 1.3566 - val_mse: 1.3566\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1422 - mse: 1.1422 - val_loss: 1.3607 - val_mse: 1.3607\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1430 - mse: 1.1430 - val_loss: 1.3577 - val_mse: 1.3577\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1423 - mse: 1.1423 - val_loss: 1.3562 - val_mse: 1.3562\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1428 - mse: 1.1428 - val_loss: 1.3558 - val_mse: 1.3558\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1424 - mse: 1.1424 - val_loss: 1.3547 - val_mse: 1.3547\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.3547 - mse: 1.3547\n",
      "\n",
      "Weight: [array([[1.9559537]], dtype=float32), array([1.0794297], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "N_train = np.zeros(6)\n",
    "sigma = np.zeros(6)\n",
    "score50 = np.zeros((6,6,2))\n",
    "weight50 = np.zeros((6,6,2))\n",
    "\n",
    "history50 = []\n",
    "\n",
    "\n",
    "#GRID SEARCH\n",
    "for i in range(6):\n",
    "    N_train[i] = 500 + i*100\n",
    "    row = []\n",
    "    for j in range(6):\n",
    "        sigma[j] = 0.1 + j*0.2 # noise standard deviation\n",
    "        x_train = np.random.uniform(-1, 1, int(N_train[i]))\n",
    "    \n",
    "        y_train = np.random.normal(m * x_train + b, sigma[j]) # actual measures from which we want to guess regression parameters\n",
    "        y_valid = np.random.normal(m * x_valid + b, sigma[j])\n",
    "        # fit the model using training dataset\n",
    "        # over 50 epochs of 32 batch size each\n",
    "        # report training progress against validation data\n",
    "        print(\"\\n\\nN_train = \",N_train[i])\n",
    "        print(\"Sigma = \",sigma[j])\n",
    "        row.append(model.fit(x=x_train, y=y_train, \n",
    "              batch_size=32, epochs=50,\n",
    "              shuffle=True, # a good idea is to shuffle input before at each epoch\n",
    "              validation_data=(x_valid, y_valid)))\n",
    "        score50[i][j] = model.evaluate(x_valid, y_valid, batch_size=32, verbose=1)\n",
    "        pesi = model.get_weights()\n",
    "        weight50[i][j] = [pesi[0][0][0],pesi[1][0]]\n",
    "        print(\"\\nWeight:\",pesi)  \n",
    "    history50.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruisco una rete neurale con $N_{epochs}=60$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/60\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 2/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 3/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 4/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 5/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 6/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 7/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 8/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 9/60\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 10/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 11/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 12/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 13/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 14/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 15/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 16/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 17/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 18/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 19/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 20/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 21/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 22/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 23/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 24/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 25/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 26/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 27/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 28/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 29/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 30/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 31/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 32/60\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 33/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 34/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 35/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 36/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 37/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 38/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 39/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 40/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 41/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 42/60\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 43/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 44/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 45/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 46/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 47/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 48/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 49/60\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 50/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 51/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 52/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 53/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 54/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 55/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 56/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 57/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 58/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 59/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 60/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098\n",
      "\n",
      "Weight: [array([[1.9811473]], dtype=float32), array([1.0064634], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/60\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0742 - val_mse: 0.0742\n",
      "Epoch 2/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0742 - val_mse: 0.0742\n",
      "Epoch 3/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0868 - mse: 0.0868 - val_loss: 0.0742 - val_mse: 0.0742\n",
      "Epoch 4/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0742 - val_mse: 0.0742\n",
      "Epoch 5/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0742 - val_mse: 0.0742\n",
      "Epoch 6/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 7/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 8/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 9/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 10/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 11/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 12/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 13/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 14/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 15/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 16/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 17/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 18/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 19/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 20/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 21/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0866 - mse: 0.0866 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 22/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 23/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 24/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 25/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 26/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 27/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 28/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 29/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 30/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 31/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 32/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 33/60\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 34/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 35/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 36/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 37/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 38/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 39/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 40/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 41/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 42/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 43/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 44/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 45/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 46/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 47/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 48/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 49/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 50/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 51/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 52/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 53/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 54/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 55/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 56/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 57/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 58/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 59/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 60/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0744 - mse: 0.0744\n",
      "\n",
      "Weight: [array([[1.9956615]], dtype=float32), array([1.0029105], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/60\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2408 - mse: 0.2408 - val_loss: 0.2460 - val_mse: 0.2460\n",
      "Epoch 2/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2407 - mse: 0.2407 - val_loss: 0.2469 - val_mse: 0.2469\n",
      "Epoch 3/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2405 - mse: 0.2405 - val_loss: 0.2476 - val_mse: 0.2476\n",
      "Epoch 4/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2403 - mse: 0.2403 - val_loss: 0.2483 - val_mse: 0.2483\n",
      "Epoch 5/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2403 - mse: 0.2403 - val_loss: 0.2492 - val_mse: 0.2492\n",
      "Epoch 6/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2401 - mse: 0.2401 - val_loss: 0.2496 - val_mse: 0.2496\n",
      "Epoch 7/60\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2399 - mse: 0.2399 - val_loss: 0.2504 - val_mse: 0.2504\n",
      "Epoch 8/60\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2399 - mse: 0.2399 - val_loss: 0.2505 - val_mse: 0.2505\n",
      "Epoch 9/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2398 - mse: 0.2398 - val_loss: 0.2507 - val_mse: 0.2507\n",
      "Epoch 10/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2399 - mse: 0.2399 - val_loss: 0.2508 - val_mse: 0.2508\n",
      "Epoch 11/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2398 - mse: 0.2398 - val_loss: 0.2511 - val_mse: 0.2511\n",
      "Epoch 12/60\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2398 - mse: 0.2398 - val_loss: 0.2515 - val_mse: 0.2515\n",
      "Epoch 13/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2398 - mse: 0.2398 - val_loss: 0.2518 - val_mse: 0.2518\n",
      "Epoch 14/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2398 - mse: 0.2398 - val_loss: 0.2520 - val_mse: 0.2520\n",
      "Epoch 15/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2523 - val_mse: 0.2523\n",
      "Epoch 16/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2524 - val_mse: 0.2524\n",
      "Epoch 17/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2527 - val_mse: 0.2527\n",
      "Epoch 18/60\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2527 - val_mse: 0.2527\n",
      "Epoch 19/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2529 - val_mse: 0.2529\n",
      "Epoch 20/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2526 - val_mse: 0.2526\n",
      "Epoch 21/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2527 - val_mse: 0.2527\n",
      "Epoch 22/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2530 - val_mse: 0.2530\n",
      "Epoch 23/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2530 - val_mse: 0.2530\n",
      "Epoch 24/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2533 - val_mse: 0.2533\n",
      "Epoch 25/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2535 - val_mse: 0.2535\n",
      "Epoch 26/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2536 - val_mse: 0.2536\n",
      "Epoch 27/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2538 - val_mse: 0.2538\n",
      "Epoch 28/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2533 - val_mse: 0.2533\n",
      "Epoch 29/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2398 - mse: 0.2398 - val_loss: 0.2538 - val_mse: 0.2538\n",
      "Epoch 30/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2539 - val_mse: 0.2539\n",
      "Epoch 31/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2534 - val_mse: 0.2534\n",
      "Epoch 32/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2539 - val_mse: 0.2539\n",
      "Epoch 33/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2540 - val_mse: 0.2540\n",
      "Epoch 34/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2540 - val_mse: 0.2540\n",
      "Epoch 35/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2539 - val_mse: 0.2539\n",
      "Epoch 36/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2541 - val_mse: 0.2541\n",
      "Epoch 37/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2540 - val_mse: 0.2540\n",
      "Epoch 38/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2538 - val_mse: 0.2538\n",
      "Epoch 39/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2537 - val_mse: 0.2537\n",
      "Epoch 40/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2539 - val_mse: 0.2539\n",
      "Epoch 41/60\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2538 - val_mse: 0.2538\n",
      "Epoch 42/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2540 - val_mse: 0.2540\n",
      "Epoch 43/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2542 - val_mse: 0.2542\n",
      "Epoch 44/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2540 - val_mse: 0.2540\n",
      "Epoch 45/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2538 - val_mse: 0.2538\n",
      "Epoch 46/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2537 - val_mse: 0.2537\n",
      "Epoch 47/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2536 - val_mse: 0.2536\n",
      "Epoch 48/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2539 - val_mse: 0.2539\n",
      "Epoch 49/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2541 - val_mse: 0.2541\n",
      "Epoch 50/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2540 - val_mse: 0.2540\n",
      "Epoch 51/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2537 - val_mse: 0.2537\n",
      "Epoch 52/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2538 - val_mse: 0.2538\n",
      "Epoch 53/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2540 - val_mse: 0.2540\n",
      "Epoch 54/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2537 - val_mse: 0.2537\n",
      "Epoch 55/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2538 - val_mse: 0.2538\n",
      "Epoch 56/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2537 - val_mse: 0.2537\n",
      "Epoch 57/60\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2536 - val_mse: 0.2536\n",
      "Epoch 58/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2535 - val_mse: 0.2535\n",
      "Epoch 59/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2535 - val_mse: 0.2535\n",
      "Epoch 60/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2537 - val_mse: 0.2537\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2537 - mse: 0.2537\n",
      "\n",
      "Weight: [array([[2.0565476]], dtype=float32), array([1.0176421], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/60\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5169 - mse: 0.5169 - val_loss: 0.4792 - val_mse: 0.4792\n",
      "Epoch 2/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4796 - val_mse: 0.4796\n",
      "Epoch 3/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5167 - mse: 0.5167 - val_loss: 0.4800 - val_mse: 0.4800\n",
      "Epoch 4/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5165 - mse: 0.5165 - val_loss: 0.4804 - val_mse: 0.4804\n",
      "Epoch 5/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5165 - mse: 0.5165 - val_loss: 0.4808 - val_mse: 0.4808\n",
      "Epoch 6/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5162 - mse: 0.5162 - val_loss: 0.4812 - val_mse: 0.4812\n",
      "Epoch 7/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4814 - val_mse: 0.4814\n",
      "Epoch 8/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4814 - val_mse: 0.4814\n",
      "Epoch 9/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4814 - val_mse: 0.4814\n",
      "Epoch 10/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5161 - mse: 0.5161 - val_loss: 0.4813 - val_mse: 0.4813\n",
      "Epoch 11/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5165 - mse: 0.5165 - val_loss: 0.4813 - val_mse: 0.4813\n",
      "Epoch 12/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4813 - val_mse: 0.4813\n",
      "Epoch 13/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4816 - val_mse: 0.4816\n",
      "Epoch 14/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4818 - val_mse: 0.4818\n",
      "Epoch 15/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4818 - val_mse: 0.4818\n",
      "Epoch 16/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5162 - mse: 0.5162 - val_loss: 0.4818 - val_mse: 0.4818\n",
      "Epoch 17/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5162 - mse: 0.5162 - val_loss: 0.4819 - val_mse: 0.4819\n",
      "Epoch 18/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4817 - val_mse: 0.4817\n",
      "Epoch 19/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4817 - val_mse: 0.4817\n",
      "Epoch 20/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4822 - val_mse: 0.4822\n",
      "Epoch 21/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5162 - mse: 0.5162 - val_loss: 0.4821 - val_mse: 0.4821\n",
      "Epoch 22/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4819 - val_mse: 0.4819\n",
      "Epoch 23/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5165 - mse: 0.5165 - val_loss: 0.4820 - val_mse: 0.4820\n",
      "Epoch 24/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5165 - mse: 0.5165 - val_loss: 0.4821 - val_mse: 0.4821\n",
      "Epoch 25/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4823 - val_mse: 0.4823\n",
      "Epoch 26/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4820 - val_mse: 0.4820\n",
      "Epoch 27/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4818 - val_mse: 0.4818\n",
      "Epoch 28/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4819 - val_mse: 0.4819\n",
      "Epoch 29/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4819 - val_mse: 0.4819\n",
      "Epoch 30/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4815 - val_mse: 0.4815\n",
      "Epoch 31/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4818 - val_mse: 0.4818\n",
      "Epoch 32/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4815 - val_mse: 0.4815\n",
      "Epoch 33/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4815 - val_mse: 0.4815\n",
      "Epoch 34/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4820 - val_mse: 0.4820\n",
      "Epoch 35/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4823 - val_mse: 0.4823\n",
      "Epoch 36/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4822 - val_mse: 0.4822\n",
      "Epoch 37/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5166 - mse: 0.5166 - val_loss: 0.4819 - val_mse: 0.4819\n",
      "Epoch 38/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4818 - val_mse: 0.4818\n",
      "Epoch 39/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5162 - mse: 0.5162 - val_loss: 0.4818 - val_mse: 0.4818\n",
      "Epoch 40/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5167 - mse: 0.5167 - val_loss: 0.4819 - val_mse: 0.4819\n",
      "Epoch 41/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4820 - val_mse: 0.4820\n",
      "Epoch 42/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4815 - val_mse: 0.4815\n",
      "Epoch 43/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5165 - mse: 0.5165 - val_loss: 0.4814 - val_mse: 0.4814\n",
      "Epoch 44/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4814 - val_mse: 0.4814\n",
      "Epoch 45/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4815 - val_mse: 0.4815\n",
      "Epoch 46/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5162 - mse: 0.5162 - val_loss: 0.4814 - val_mse: 0.4814\n",
      "Epoch 47/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4816 - val_mse: 0.4816\n",
      "Epoch 48/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4814 - val_mse: 0.4814\n",
      "Epoch 49/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5165 - mse: 0.5165 - val_loss: 0.4819 - val_mse: 0.4819\n",
      "Epoch 50/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4818 - val_mse: 0.4818\n",
      "Epoch 51/60\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5165 - mse: 0.5165 - val_loss: 0.4819 - val_mse: 0.4819\n",
      "Epoch 52/60\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5162 - mse: 0.5162 - val_loss: 0.4817 - val_mse: 0.4817\n",
      "Epoch 53/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5162 - mse: 0.5162 - val_loss: 0.4819 - val_mse: 0.4819\n",
      "Epoch 54/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4817 - val_mse: 0.4817\n",
      "Epoch 55/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4814 - val_mse: 0.4814\n",
      "Epoch 56/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5165 - mse: 0.5165 - val_loss: 0.4812 - val_mse: 0.4812\n",
      "Epoch 57/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4811 - val_mse: 0.4811\n",
      "Epoch 58/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4813 - val_mse: 0.4813\n",
      "Epoch 59/60\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.4815 - val_mse: 0.4815\n",
      "Epoch 60/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 0.4816 - val_mse: 0.4816\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4816 - mse: 0.4816\n",
      "\n",
      "Weight: [array([[2.0748124]], dtype=float32), array([0.99274635], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/60\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8650 - mse: 0.8650 - val_loss: 0.7152 - val_mse: 0.7152\n",
      "Epoch 2/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8643 - mse: 0.8643 - val_loss: 0.7147 - val_mse: 0.7147\n",
      "Epoch 3/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8636 - mse: 0.8636 - val_loss: 0.7146 - val_mse: 0.7146\n",
      "Epoch 4/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8633 - mse: 0.8633 - val_loss: 0.7144 - val_mse: 0.7144\n",
      "Epoch 5/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8633 - mse: 0.8633 - val_loss: 0.7146 - val_mse: 0.7146\n",
      "Epoch 6/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8637 - mse: 0.8637 - val_loss: 0.7146 - val_mse: 0.7146\n",
      "Epoch 7/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8633 - mse: 0.8633 - val_loss: 0.7147 - val_mse: 0.7147\n",
      "Epoch 8/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8633 - mse: 0.8633 - val_loss: 0.7149 - val_mse: 0.7149\n",
      "Epoch 9/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8630 - mse: 0.8630 - val_loss: 0.7149 - val_mse: 0.7149\n",
      "Epoch 10/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8630 - mse: 0.8630 - val_loss: 0.7151 - val_mse: 0.7151\n",
      "Epoch 11/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8632 - mse: 0.8632 - val_loss: 0.7151 - val_mse: 0.7151\n",
      "Epoch 12/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8631 - mse: 0.8631 - val_loss: 0.7152 - val_mse: 0.7152\n",
      "Epoch 13/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8632 - mse: 0.8632 - val_loss: 0.7152 - val_mse: 0.7152\n",
      "Epoch 14/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8637 - mse: 0.8637 - val_loss: 0.7152 - val_mse: 0.7152\n",
      "Epoch 15/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8635 - mse: 0.8635 - val_loss: 0.7152 - val_mse: 0.7152\n",
      "Epoch 16/60\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.8632 - mse: 0.8632 - val_loss: 0.7155 - val_mse: 0.7155\n",
      "Epoch 17/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8633 - mse: 0.8633 - val_loss: 0.7155 - val_mse: 0.7155\n",
      "Epoch 18/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8635 - mse: 0.8635 - val_loss: 0.7152 - val_mse: 0.7152\n",
      "Epoch 19/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8633 - mse: 0.8633 - val_loss: 0.7151 - val_mse: 0.7151\n",
      "Epoch 20/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8632 - mse: 0.8632 - val_loss: 0.7151 - val_mse: 0.7151\n",
      "Epoch 21/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8633 - mse: 0.8633 - val_loss: 0.7150 - val_mse: 0.7150\n",
      "Epoch 22/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8640 - mse: 0.8640 - val_loss: 0.7151 - val_mse: 0.7151\n",
      "Epoch 23/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8634 - mse: 0.8634 - val_loss: 0.7152 - val_mse: 0.7152\n",
      "Epoch 24/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8631 - mse: 0.8631 - val_loss: 0.7152 - val_mse: 0.7152\n",
      "Epoch 25/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8633 - mse: 0.8633 - val_loss: 0.7153 - val_mse: 0.7153\n",
      "Epoch 26/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8633 - mse: 0.8633 - val_loss: 0.7153 - val_mse: 0.7153\n",
      "Epoch 27/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8633 - mse: 0.8633 - val_loss: 0.7153 - val_mse: 0.7153\n",
      "Epoch 28/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8633 - mse: 0.8633 - val_loss: 0.7156 - val_mse: 0.7156\n",
      "Epoch 29/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8630 - mse: 0.8630 - val_loss: 0.7155 - val_mse: 0.7155\n",
      "Epoch 30/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8631 - mse: 0.8631 - val_loss: 0.7156 - val_mse: 0.7156\n",
      "Epoch 31/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8631 - mse: 0.8631 - val_loss: 0.7155 - val_mse: 0.7155\n",
      "Epoch 32/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8631 - mse: 0.8631 - val_loss: 0.7156 - val_mse: 0.7156\n",
      "Epoch 33/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8629 - mse: 0.8629 - val_loss: 0.7156 - val_mse: 0.7156\n",
      "Epoch 34/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8634 - mse: 0.8634 - val_loss: 0.7155 - val_mse: 0.7155\n",
      "Epoch 35/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8635 - mse: 0.8635 - val_loss: 0.7155 - val_mse: 0.7155\n",
      "Epoch 36/60\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.8630 - mse: 0.8630 - val_loss: 0.7154 - val_mse: 0.7154\n",
      "Epoch 37/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8632 - mse: 0.8632 - val_loss: 0.7156 - val_mse: 0.7156\n",
      "Epoch 38/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8635 - mse: 0.8635 - val_loss: 0.7155 - val_mse: 0.7155\n",
      "Epoch 39/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8633 - mse: 0.8633 - val_loss: 0.7156 - val_mse: 0.7156\n",
      "Epoch 40/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8629 - mse: 0.8629 - val_loss: 0.7157 - val_mse: 0.7157\n",
      "Epoch 41/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8632 - mse: 0.8632 - val_loss: 0.7158 - val_mse: 0.7158\n",
      "Epoch 42/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8636 - mse: 0.8636 - val_loss: 0.7159 - val_mse: 0.7159\n",
      "Epoch 43/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8629 - mse: 0.8629 - val_loss: 0.7159 - val_mse: 0.7159\n",
      "Epoch 44/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8629 - mse: 0.8629 - val_loss: 0.7160 - val_mse: 0.7160\n",
      "Epoch 45/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8631 - mse: 0.8631 - val_loss: 0.7159 - val_mse: 0.7159\n",
      "Epoch 46/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8636 - mse: 0.8636 - val_loss: 0.7159 - val_mse: 0.7159\n",
      "Epoch 47/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8630 - mse: 0.8630 - val_loss: 0.7159 - val_mse: 0.7159\n",
      "Epoch 48/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8632 - mse: 0.8632 - val_loss: 0.7160 - val_mse: 0.7160\n",
      "Epoch 49/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8634 - mse: 0.8634 - val_loss: 0.7159 - val_mse: 0.7159\n",
      "Epoch 50/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8633 - mse: 0.8633 - val_loss: 0.7160 - val_mse: 0.7160\n",
      "Epoch 51/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8631 - mse: 0.8631 - val_loss: 0.7159 - val_mse: 0.7159\n",
      "Epoch 52/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8633 - mse: 0.8633 - val_loss: 0.7159 - val_mse: 0.7159\n",
      "Epoch 53/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8633 - mse: 0.8633 - val_loss: 0.7158 - val_mse: 0.7158\n",
      "Epoch 54/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8630 - mse: 0.8630 - val_loss: 0.7159 - val_mse: 0.7159\n",
      "Epoch 55/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8634 - mse: 0.8634 - val_loss: 0.7159 - val_mse: 0.7159\n",
      "Epoch 56/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8635 - mse: 0.8635 - val_loss: 0.7158 - val_mse: 0.7158\n",
      "Epoch 57/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8635 - mse: 0.8635 - val_loss: 0.7160 - val_mse: 0.7160\n",
      "Epoch 58/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8631 - mse: 0.8631 - val_loss: 0.7158 - val_mse: 0.7158\n",
      "Epoch 59/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8634 - mse: 0.8634 - val_loss: 0.7157 - val_mse: 0.7157\n",
      "Epoch 60/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8634 - mse: 0.8634 - val_loss: 0.7158 - val_mse: 0.7158\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.7158 - mse: 0.7158\n",
      "\n",
      "Weight: [array([[2.0501463]], dtype=float32), array([0.94673944], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  500.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/60\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.3368 - mse: 1.3368 - val_loss: 0.7035 - val_mse: 0.7035\n",
      "Epoch 2/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3325 - mse: 1.3325 - val_loss: 0.7061 - val_mse: 0.7061\n",
      "Epoch 3/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3308 - mse: 1.3308 - val_loss: 0.7086 - val_mse: 0.7086\n",
      "Epoch 4/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3293 - mse: 1.3293 - val_loss: 0.7111 - val_mse: 0.7111\n",
      "Epoch 5/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3285 - mse: 1.3285 - val_loss: 0.7135 - val_mse: 0.7135\n",
      "Epoch 6/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3277 - mse: 1.3277 - val_loss: 0.7162 - val_mse: 0.7162\n",
      "Epoch 7/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3274 - mse: 1.3274 - val_loss: 0.7165 - val_mse: 0.7165\n",
      "Epoch 8/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3276 - mse: 1.3276 - val_loss: 0.7178 - val_mse: 0.7178\n",
      "Epoch 9/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3274 - mse: 1.3274 - val_loss: 0.7182 - val_mse: 0.7182\n",
      "Epoch 10/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3275 - mse: 1.3275 - val_loss: 0.7188 - val_mse: 0.7188\n",
      "Epoch 11/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3273 - mse: 1.3273 - val_loss: 0.7197 - val_mse: 0.7197\n",
      "Epoch 12/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3275 - mse: 1.3275 - val_loss: 0.7198 - val_mse: 0.7198\n",
      "Epoch 13/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3274 - mse: 1.3274 - val_loss: 0.7198 - val_mse: 0.7198\n",
      "Epoch 14/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3274 - mse: 1.3274 - val_loss: 0.7200 - val_mse: 0.7200\n",
      "Epoch 15/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3276 - mse: 1.3276 - val_loss: 0.7207 - val_mse: 0.7207\n",
      "Epoch 16/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3278 - mse: 1.3278 - val_loss: 0.7203 - val_mse: 0.7203\n",
      "Epoch 17/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3273 - mse: 1.3273 - val_loss: 0.7207 - val_mse: 0.7207\n",
      "Epoch 18/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3272 - mse: 1.3272 - val_loss: 0.7204 - val_mse: 0.7204\n",
      "Epoch 19/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3277 - mse: 1.3277 - val_loss: 0.7188 - val_mse: 0.7188\n",
      "Epoch 20/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3273 - mse: 1.3273 - val_loss: 0.7188 - val_mse: 0.7188\n",
      "Epoch 21/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3277 - mse: 1.3277 - val_loss: 0.7205 - val_mse: 0.7205\n",
      "Epoch 22/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3275 - mse: 1.3275 - val_loss: 0.7210 - val_mse: 0.7210\n",
      "Epoch 23/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3273 - mse: 1.3273 - val_loss: 0.7199 - val_mse: 0.7199\n",
      "Epoch 24/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3274 - mse: 1.3274 - val_loss: 0.7196 - val_mse: 0.7196\n",
      "Epoch 25/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3279 - mse: 1.3279 - val_loss: 0.7199 - val_mse: 0.7199\n",
      "Epoch 26/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3275 - mse: 1.3275 - val_loss: 0.7201 - val_mse: 0.7201\n",
      "Epoch 27/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3275 - mse: 1.3275 - val_loss: 0.7217 - val_mse: 0.7217\n",
      "Epoch 28/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3273 - mse: 1.3273 - val_loss: 0.7223 - val_mse: 0.7223\n",
      "Epoch 29/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3278 - mse: 1.3278 - val_loss: 0.7222 - val_mse: 0.7222\n",
      "Epoch 30/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3274 - mse: 1.3274 - val_loss: 0.7217 - val_mse: 0.7217\n",
      "Epoch 31/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3273 - mse: 1.3273 - val_loss: 0.7213 - val_mse: 0.7213\n",
      "Epoch 32/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3279 - mse: 1.3279 - val_loss: 0.7217 - val_mse: 0.7217\n",
      "Epoch 33/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3273 - mse: 1.3273 - val_loss: 0.7214 - val_mse: 0.7214\n",
      "Epoch 34/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3274 - mse: 1.3274 - val_loss: 0.7212 - val_mse: 0.7212\n",
      "Epoch 35/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3278 - mse: 1.3278 - val_loss: 0.7228 - val_mse: 0.7228\n",
      "Epoch 36/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3276 - mse: 1.3276 - val_loss: 0.7226 - val_mse: 0.7226\n",
      "Epoch 37/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3276 - mse: 1.3276 - val_loss: 0.7218 - val_mse: 0.7218\n",
      "Epoch 38/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3276 - mse: 1.3276 - val_loss: 0.7213 - val_mse: 0.7213\n",
      "Epoch 39/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3274 - mse: 1.3274 - val_loss: 0.7205 - val_mse: 0.7205\n",
      "Epoch 40/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3275 - mse: 1.3275 - val_loss: 0.7200 - val_mse: 0.7200\n",
      "Epoch 41/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3279 - mse: 1.3279 - val_loss: 0.7204 - val_mse: 0.7204\n",
      "Epoch 42/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3273 - mse: 1.3273 - val_loss: 0.7205 - val_mse: 0.7205\n",
      "Epoch 43/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3276 - mse: 1.3276 - val_loss: 0.7206 - val_mse: 0.7206\n",
      "Epoch 44/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3276 - mse: 1.3276 - val_loss: 0.7204 - val_mse: 0.7204\n",
      "Epoch 45/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3272 - mse: 1.3272 - val_loss: 0.7203 - val_mse: 0.7203\n",
      "Epoch 46/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3275 - mse: 1.3275 - val_loss: 0.7199 - val_mse: 0.7199\n",
      "Epoch 47/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3274 - mse: 1.3274 - val_loss: 0.7196 - val_mse: 0.7196\n",
      "Epoch 48/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3272 - mse: 1.3272 - val_loss: 0.7198 - val_mse: 0.7198\n",
      "Epoch 49/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3277 - mse: 1.3277 - val_loss: 0.7186 - val_mse: 0.7186\n",
      "Epoch 50/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3280 - mse: 1.3280 - val_loss: 0.7178 - val_mse: 0.7178\n",
      "Epoch 51/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3279 - mse: 1.3279 - val_loss: 0.7179 - val_mse: 0.7179\n",
      "Epoch 52/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3273 - mse: 1.3273 - val_loss: 0.7170 - val_mse: 0.7170\n",
      "Epoch 53/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3277 - mse: 1.3277 - val_loss: 0.7195 - val_mse: 0.7195\n",
      "Epoch 54/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3277 - mse: 1.3277 - val_loss: 0.7183 - val_mse: 0.7183\n",
      "Epoch 55/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3274 - mse: 1.3274 - val_loss: 0.7190 - val_mse: 0.7190\n",
      "Epoch 56/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3272 - mse: 1.3272 - val_loss: 0.7188 - val_mse: 0.7188\n",
      "Epoch 57/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3270 - mse: 1.3270 - val_loss: 0.7192 - val_mse: 0.7192\n",
      "Epoch 58/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3278 - mse: 1.3278 - val_loss: 0.7198 - val_mse: 0.7198\n",
      "Epoch 59/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3274 - mse: 1.3274 - val_loss: 0.7191 - val_mse: 0.7191\n",
      "Epoch 60/60\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3276 - mse: 1.3276 - val_loss: 0.7181 - val_mse: 0.7181\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.7181 - mse: 0.7181\n",
      "\n",
      "Weight: [array([[2.0501432]], dtype=float32), array([1.0496081], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/60\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0123 - val_mse: 0.0123\n",
      "Epoch 2/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 3/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 4/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 5/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 6/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 7/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 8/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 9/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 10/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 11/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 12/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 13/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 14/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 15/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 16/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 17/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 18/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 19/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 20/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 21/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 22/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 23/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 24/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 25/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 26/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 27/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 28/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 29/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 30/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 31/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 32/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 33/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 34/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 35/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 36/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 37/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 38/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 39/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 40/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 41/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 42/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 43/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 44/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 45/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 46/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 47/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 48/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 49/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 50/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 51/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 52/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 53/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 54/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 55/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 56/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 57/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 58/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 59/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 60/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0103 - mse: 0.0103\n",
      "\n",
      "Weight: [array([[2.0009332]], dtype=float32), array([0.9993886], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/60\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0911 - val_mse: 0.0911\n",
      "Epoch 2/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0910 - val_mse: 0.0910\n",
      "Epoch 3/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0910 - val_mse: 0.0910\n",
      "Epoch 4/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0910 - val_mse: 0.0910\n",
      "Epoch 5/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 6/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 7/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 8/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0953 - mse: 0.0953 - val_loss: 0.0910 - val_mse: 0.0910\n",
      "Epoch 9/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0953 - mse: 0.0953 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 10/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0953 - mse: 0.0953 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 11/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 12/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 13/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0953 - mse: 0.0953 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 14/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 15/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0953 - mse: 0.0953 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 16/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 17/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 18/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 19/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 20/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 21/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 22/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 23/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 24/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 25/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0953 - mse: 0.0953 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 26/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 27/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 28/60\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 29/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0953 - mse: 0.0953 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 30/60\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 31/60\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 32/60\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 33/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 34/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 35/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 36/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 37/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 38/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 39/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 40/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 41/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0907 - val_mse: 0.0907\n",
      "Epoch 42/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0907 - val_mse: 0.0907\n",
      "Epoch 43/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0907 - val_mse: 0.0907\n",
      "Epoch 44/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0907 - val_mse: 0.0907\n",
      "Epoch 45/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 46/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 47/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 48/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 49/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 50/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 51/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 52/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 53/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 54/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 55/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0953 - mse: 0.0953 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 56/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0953 - mse: 0.0953 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 57/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 58/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 59/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0910 - val_mse: 0.0910\n",
      "Epoch 60/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0909 - mse: 0.0909\n",
      "\n",
      "Weight: [array([[2.0008004]], dtype=float32), array([0.99597347], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/60\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2639 - mse: 0.2639 - val_loss: 0.2566 - val_mse: 0.2566\n",
      "Epoch 2/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2639 - mse: 0.2639 - val_loss: 0.2568 - val_mse: 0.2568\n",
      "Epoch 3/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2568 - val_mse: 0.2568\n",
      "Epoch 4/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2639 - mse: 0.2639 - val_loss: 0.2567 - val_mse: 0.2567\n",
      "Epoch 5/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2568 - val_mse: 0.2568\n",
      "Epoch 6/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2640 - mse: 0.2640 - val_loss: 0.2569 - val_mse: 0.2569\n",
      "Epoch 7/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2569 - val_mse: 0.2569\n",
      "Epoch 8/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2569 - val_mse: 0.2569\n",
      "Epoch 9/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2637 - mse: 0.2637 - val_loss: 0.2571 - val_mse: 0.2571\n",
      "Epoch 10/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2572 - val_mse: 0.2572\n",
      "Epoch 11/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2572 - val_mse: 0.2572\n",
      "Epoch 12/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2571 - val_mse: 0.2571\n",
      "Epoch 13/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2573 - val_mse: 0.2573\n",
      "Epoch 14/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2574 - val_mse: 0.2574\n",
      "Epoch 15/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2574 - val_mse: 0.2574\n",
      "Epoch 16/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2574 - val_mse: 0.2574\n",
      "Epoch 17/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2573 - val_mse: 0.2573\n",
      "Epoch 18/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2573 - val_mse: 0.2573\n",
      "Epoch 19/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2573 - val_mse: 0.2573\n",
      "Epoch 20/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2575 - val_mse: 0.2575\n",
      "Epoch 21/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2575 - val_mse: 0.2575\n",
      "Epoch 22/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2574 - val_mse: 0.2574\n",
      "Epoch 23/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2639 - mse: 0.2639 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 24/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2577 - val_mse: 0.2577\n",
      "Epoch 25/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2639 - mse: 0.2639 - val_loss: 0.2577 - val_mse: 0.2577\n",
      "Epoch 26/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2637 - mse: 0.2637 - val_loss: 0.2577 - val_mse: 0.2577\n",
      "Epoch 27/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2637 - mse: 0.2637 - val_loss: 0.2577 - val_mse: 0.2577\n",
      "Epoch 28/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2577 - val_mse: 0.2577\n",
      "Epoch 29/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2637 - mse: 0.2637 - val_loss: 0.2578 - val_mse: 0.2578\n",
      "Epoch 30/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2578 - val_mse: 0.2578\n",
      "Epoch 31/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2639 - mse: 0.2639 - val_loss: 0.2577 - val_mse: 0.2577\n",
      "Epoch 32/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2577 - val_mse: 0.2577\n",
      "Epoch 33/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 34/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2639 - mse: 0.2639 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 35/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 36/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 37/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 38/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2575 - val_mse: 0.2575\n",
      "Epoch 39/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2639 - mse: 0.2639 - val_loss: 0.2574 - val_mse: 0.2574\n",
      "Epoch 40/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2575 - val_mse: 0.2575\n",
      "Epoch 41/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2574 - val_mse: 0.2574\n",
      "Epoch 42/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2639 - mse: 0.2639 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 43/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2639 - mse: 0.2639 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 44/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2639 - mse: 0.2639 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 45/60\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2639 - mse: 0.2639 - val_loss: 0.2577 - val_mse: 0.2577\n",
      "Epoch 46/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2577 - val_mse: 0.2577\n",
      "Epoch 47/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 48/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2637 - mse: 0.2637 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 49/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 50/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2637 - mse: 0.2637 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 51/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2575 - val_mse: 0.2575\n",
      "Epoch 52/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 53/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2639 - mse: 0.2639 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 54/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 55/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2639 - mse: 0.2639 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 56/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2577 - val_mse: 0.2577\n",
      "Epoch 57/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2578 - val_mse: 0.2578\n",
      "Epoch 58/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 59/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "Epoch 60/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2576 - val_mse: 0.2576\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2576 - mse: 0.2576\n",
      "\n",
      "Weight: [array([[2.018344]], dtype=float32), array([0.98875594], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/60\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4833 - mse: 0.4833 - val_loss: 0.5391 - val_mse: 0.5391\n",
      "Epoch 2/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4831 - mse: 0.4831 - val_loss: 0.5394 - val_mse: 0.5394\n",
      "Epoch 3/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4831 - mse: 0.4831 - val_loss: 0.5396 - val_mse: 0.5396\n",
      "Epoch 4/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4831 - mse: 0.4831 - val_loss: 0.5399 - val_mse: 0.5399\n",
      "Epoch 5/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5402 - val_mse: 0.5402\n",
      "Epoch 6/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4832 - mse: 0.4832 - val_loss: 0.5406 - val_mse: 0.5406\n",
      "Epoch 7/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4831 - mse: 0.4831 - val_loss: 0.5407 - val_mse: 0.5407\n",
      "Epoch 8/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5407 - val_mse: 0.5407\n",
      "Epoch 9/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5407 - val_mse: 0.5407\n",
      "Epoch 10/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5409 - val_mse: 0.5409\n",
      "Epoch 11/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4829 - mse: 0.4829 - val_loss: 0.5410 - val_mse: 0.5410\n",
      "Epoch 12/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5408 - val_mse: 0.5408\n",
      "Epoch 13/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4832 - mse: 0.4832 - val_loss: 0.5409 - val_mse: 0.5409\n",
      "Epoch 14/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4829 - mse: 0.4829 - val_loss: 0.5409 - val_mse: 0.5409\n",
      "Epoch 15/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5409 - val_mse: 0.5409\n",
      "Epoch 16/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5412 - val_mse: 0.5412\n",
      "Epoch 17/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4829 - mse: 0.4829 - val_loss: 0.5415 - val_mse: 0.5415\n",
      "Epoch 18/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4831 - mse: 0.4831 - val_loss: 0.5416 - val_mse: 0.5416\n",
      "Epoch 19/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4833 - mse: 0.4833 - val_loss: 0.5416 - val_mse: 0.5416\n",
      "Epoch 20/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5417 - val_mse: 0.5417\n",
      "Epoch 21/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5416 - val_mse: 0.5416\n",
      "Epoch 22/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5415 - val_mse: 0.5415\n",
      "Epoch 23/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5414 - val_mse: 0.5414\n",
      "Epoch 24/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5415 - val_mse: 0.5415\n",
      "Epoch 25/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5416 - val_mse: 0.5416\n",
      "Epoch 26/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5415 - val_mse: 0.5415\n",
      "Epoch 27/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5414 - val_mse: 0.5414\n",
      "Epoch 28/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5415 - val_mse: 0.5415\n",
      "Epoch 29/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5416 - val_mse: 0.5416\n",
      "Epoch 30/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4831 - mse: 0.4831 - val_loss: 0.5415 - val_mse: 0.5415\n",
      "Epoch 31/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5415 - val_mse: 0.5415\n",
      "Epoch 32/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4832 - mse: 0.4832 - val_loss: 0.5416 - val_mse: 0.5416\n",
      "Epoch 33/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4832 - mse: 0.4832 - val_loss: 0.5417 - val_mse: 0.5417\n",
      "Epoch 34/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5418 - val_mse: 0.5418\n",
      "Epoch 35/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5418 - val_mse: 0.5418\n",
      "Epoch 36/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4832 - mse: 0.4832 - val_loss: 0.5418 - val_mse: 0.5418\n",
      "Epoch 37/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5416 - val_mse: 0.5416\n",
      "Epoch 38/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5414 - val_mse: 0.5414\n",
      "Epoch 39/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4831 - mse: 0.4831 - val_loss: 0.5416 - val_mse: 0.5416\n",
      "Epoch 40/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4829 - mse: 0.4829 - val_loss: 0.5414 - val_mse: 0.5414\n",
      "Epoch 41/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4829 - mse: 0.4829 - val_loss: 0.5414 - val_mse: 0.5414\n",
      "Epoch 42/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4831 - mse: 0.4831 - val_loss: 0.5414 - val_mse: 0.5414\n",
      "Epoch 43/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5414 - val_mse: 0.5414\n",
      "Epoch 44/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4831 - mse: 0.4831 - val_loss: 0.5415 - val_mse: 0.5415\n",
      "Epoch 45/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4829 - mse: 0.4829 - val_loss: 0.5415 - val_mse: 0.5415\n",
      "Epoch 46/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4832 - mse: 0.4832 - val_loss: 0.5416 - val_mse: 0.5416\n",
      "Epoch 47/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4832 - mse: 0.4832 - val_loss: 0.5417 - val_mse: 0.5417\n",
      "Epoch 48/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5415 - val_mse: 0.5415\n",
      "Epoch 49/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5415 - val_mse: 0.5415\n",
      "Epoch 50/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4830 - mse: 0.4830 - val_loss: 0.5413 - val_mse: 0.5413\n",
      "Epoch 51/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4831 - mse: 0.4831 - val_loss: 0.5413 - val_mse: 0.5413\n",
      "Epoch 52/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4832 - mse: 0.4832 - val_loss: 0.5411 - val_mse: 0.5411\n",
      "Epoch 53/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4831 - mse: 0.4831 - val_loss: 0.5415 - val_mse: 0.5415\n",
      "Epoch 54/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4832 - mse: 0.4832 - val_loss: 0.5417 - val_mse: 0.5417\n",
      "Epoch 55/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4831 - mse: 0.4831 - val_loss: 0.5418 - val_mse: 0.5418\n",
      "Epoch 56/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4831 - mse: 0.4831 - val_loss: 0.5419 - val_mse: 0.5419\n",
      "Epoch 57/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4829 - mse: 0.4829 - val_loss: 0.5418 - val_mse: 0.5418\n",
      "Epoch 58/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4831 - mse: 0.4831 - val_loss: 0.5418 - val_mse: 0.5418\n",
      "Epoch 59/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4832 - mse: 0.4832 - val_loss: 0.5419 - val_mse: 0.5419\n",
      "Epoch 60/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4832 - mse: 0.4832 - val_loss: 0.5420 - val_mse: 0.5420\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5420 - mse: 0.5420\n",
      "\n",
      "Weight: [array([[1.9946315]], dtype=float32), array([0.9716042], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/60\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.8267 - mse: 0.8267 - val_loss: 0.9680 - val_mse: 0.9680\n",
      "Epoch 2/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8263 - mse: 0.8263 - val_loss: 0.9700 - val_mse: 0.9700\n",
      "Epoch 3/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8259 - mse: 0.8259 - val_loss: 0.9714 - val_mse: 0.9714\n",
      "Epoch 4/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8259 - mse: 0.8259 - val_loss: 0.9726 - val_mse: 0.9726\n",
      "Epoch 5/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8260 - mse: 0.8260 - val_loss: 0.9732 - val_mse: 0.9732\n",
      "Epoch 6/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8260 - mse: 0.8260 - val_loss: 0.9744 - val_mse: 0.9744\n",
      "Epoch 7/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8260 - mse: 0.8260 - val_loss: 0.9756 - val_mse: 0.9756\n",
      "Epoch 8/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8257 - mse: 0.8257 - val_loss: 0.9746 - val_mse: 0.9746\n",
      "Epoch 9/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8258 - mse: 0.8258 - val_loss: 0.9745 - val_mse: 0.9745\n",
      "Epoch 10/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8259 - mse: 0.8259 - val_loss: 0.9742 - val_mse: 0.9742\n",
      "Epoch 11/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8259 - mse: 0.8259 - val_loss: 0.9749 - val_mse: 0.9749\n",
      "Epoch 12/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8256 - mse: 0.8256 - val_loss: 0.9751 - val_mse: 0.9751\n",
      "Epoch 13/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8259 - mse: 0.8259 - val_loss: 0.9746 - val_mse: 0.9746\n",
      "Epoch 14/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8259 - mse: 0.8259 - val_loss: 0.9751 - val_mse: 0.9751\n",
      "Epoch 15/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8260 - mse: 0.8260 - val_loss: 0.9746 - val_mse: 0.9746\n",
      "Epoch 16/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8259 - mse: 0.8259 - val_loss: 0.9735 - val_mse: 0.9735\n",
      "Epoch 17/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8257 - mse: 0.8257 - val_loss: 0.9739 - val_mse: 0.9739\n",
      "Epoch 18/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8261 - mse: 0.8261 - val_loss: 0.9748 - val_mse: 0.9748\n",
      "Epoch 19/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8259 - mse: 0.8259 - val_loss: 0.9751 - val_mse: 0.9751\n",
      "Epoch 20/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8258 - mse: 0.8258 - val_loss: 0.9761 - val_mse: 0.9761\n",
      "Epoch 21/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8257 - mse: 0.8257 - val_loss: 0.9754 - val_mse: 0.9754\n",
      "Epoch 22/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8257 - mse: 0.8257 - val_loss: 0.9752 - val_mse: 0.9752\n",
      "Epoch 23/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8262 - mse: 0.8262 - val_loss: 0.9761 - val_mse: 0.9761\n",
      "Epoch 24/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8260 - mse: 0.8260 - val_loss: 0.9764 - val_mse: 0.9764\n",
      "Epoch 25/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8260 - mse: 0.8260 - val_loss: 0.9764 - val_mse: 0.9764\n",
      "Epoch 26/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8260 - mse: 0.8260 - val_loss: 0.9746 - val_mse: 0.9746\n",
      "Epoch 27/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8257 - mse: 0.8257 - val_loss: 0.9742 - val_mse: 0.9742\n",
      "Epoch 28/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8258 - mse: 0.8258 - val_loss: 0.9740 - val_mse: 0.9740\n",
      "Epoch 29/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8260 - mse: 0.8260 - val_loss: 0.9756 - val_mse: 0.9756\n",
      "Epoch 30/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8258 - mse: 0.8258 - val_loss: 0.9754 - val_mse: 0.9754\n",
      "Epoch 31/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8258 - mse: 0.8258 - val_loss: 0.9744 - val_mse: 0.9744\n",
      "Epoch 32/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8258 - mse: 0.8258 - val_loss: 0.9743 - val_mse: 0.9743\n",
      "Epoch 33/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8259 - mse: 0.8259 - val_loss: 0.9745 - val_mse: 0.9745\n",
      "Epoch 34/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8259 - mse: 0.8259 - val_loss: 0.9738 - val_mse: 0.9738\n",
      "Epoch 35/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8260 - mse: 0.8260 - val_loss: 0.9745 - val_mse: 0.9745\n",
      "Epoch 36/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8258 - mse: 0.8258 - val_loss: 0.9738 - val_mse: 0.9738\n",
      "Epoch 37/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8258 - mse: 0.8258 - val_loss: 0.9748 - val_mse: 0.9748\n",
      "Epoch 38/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8259 - mse: 0.8259 - val_loss: 0.9745 - val_mse: 0.9745\n",
      "Epoch 39/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8262 - mse: 0.8262 - val_loss: 0.9755 - val_mse: 0.9755\n",
      "Epoch 40/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8257 - mse: 0.8257 - val_loss: 0.9745 - val_mse: 0.9745\n",
      "Epoch 41/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8259 - mse: 0.8259 - val_loss: 0.9747 - val_mse: 0.9747\n",
      "Epoch 42/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8259 - mse: 0.8259 - val_loss: 0.9750 - val_mse: 0.9750\n",
      "Epoch 43/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8256 - mse: 0.8256 - val_loss: 0.9736 - val_mse: 0.9736\n",
      "Epoch 44/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8260 - mse: 0.8260 - val_loss: 0.9742 - val_mse: 0.9742\n",
      "Epoch 45/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8261 - mse: 0.8261 - val_loss: 0.9739 - val_mse: 0.9739\n",
      "Epoch 46/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8261 - mse: 0.8261 - val_loss: 0.9741 - val_mse: 0.9741\n",
      "Epoch 47/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8256 - mse: 0.8256 - val_loss: 0.9741 - val_mse: 0.9741\n",
      "Epoch 48/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8256 - mse: 0.8256 - val_loss: 0.9746 - val_mse: 0.9746\n",
      "Epoch 49/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8256 - mse: 0.8256 - val_loss: 0.9758 - val_mse: 0.9758\n",
      "Epoch 50/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8262 - mse: 0.8262 - val_loss: 0.9749 - val_mse: 0.9749\n",
      "Epoch 51/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8256 - mse: 0.8256 - val_loss: 0.9749 - val_mse: 0.9749\n",
      "Epoch 52/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8257 - mse: 0.8257 - val_loss: 0.9754 - val_mse: 0.9754\n",
      "Epoch 53/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8262 - mse: 0.8262 - val_loss: 0.9751 - val_mse: 0.9751\n",
      "Epoch 54/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8258 - mse: 0.8258 - val_loss: 0.9749 - val_mse: 0.9749\n",
      "Epoch 55/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8258 - mse: 0.8258 - val_loss: 0.9744 - val_mse: 0.9744\n",
      "Epoch 56/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8259 - mse: 0.8259 - val_loss: 0.9746 - val_mse: 0.9746\n",
      "Epoch 57/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8261 - mse: 0.8261 - val_loss: 0.9750 - val_mse: 0.9750\n",
      "Epoch 58/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8259 - mse: 0.8259 - val_loss: 0.9755 - val_mse: 0.9755\n",
      "Epoch 59/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8258 - mse: 0.8258 - val_loss: 0.9756 - val_mse: 0.9756\n",
      "Epoch 60/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.8258 - mse: 0.8258 - val_loss: 0.9756 - val_mse: 0.9756\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.9756 - mse: 0.9756\n",
      "\n",
      "Weight: [array([[2.0264869]], dtype=float32), array([0.9565293], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  600.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/60\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 1.1428 - mse: 1.1428 - val_loss: 0.7544 - val_mse: 0.7544\n",
      "Epoch 2/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1401 - mse: 1.1401 - val_loss: 0.7612 - val_mse: 0.7612\n",
      "Epoch 3/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1384 - mse: 1.1384 - val_loss: 0.7671 - val_mse: 0.7671\n",
      "Epoch 4/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1378 - mse: 1.1378 - val_loss: 0.7716 - val_mse: 0.7716\n",
      "Epoch 5/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1372 - mse: 1.1372 - val_loss: 0.7738 - val_mse: 0.7738\n",
      "Epoch 6/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1374 - mse: 1.1374 - val_loss: 0.7760 - val_mse: 0.7760\n",
      "Epoch 7/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1373 - mse: 1.1373 - val_loss: 0.7752 - val_mse: 0.7752\n",
      "Epoch 8/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1373 - mse: 1.1373 - val_loss: 0.7751 - val_mse: 0.7751\n",
      "Epoch 9/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1375 - mse: 1.1375 - val_loss: 0.7757 - val_mse: 0.7757\n",
      "Epoch 10/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1373 - mse: 1.1373 - val_loss: 0.7769 - val_mse: 0.7769\n",
      "Epoch 11/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1380 - mse: 1.1380 - val_loss: 0.7762 - val_mse: 0.7762\n",
      "Epoch 12/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1376 - mse: 1.1376 - val_loss: 0.7774 - val_mse: 0.7774\n",
      "Epoch 13/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1373 - mse: 1.1373 - val_loss: 0.7792 - val_mse: 0.7792\n",
      "Epoch 14/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1376 - mse: 1.1376 - val_loss: 0.7777 - val_mse: 0.7777\n",
      "Epoch 15/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1371 - mse: 1.1371 - val_loss: 0.7775 - val_mse: 0.7775\n",
      "Epoch 16/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1375 - mse: 1.1375 - val_loss: 0.7771 - val_mse: 0.7771\n",
      "Epoch 17/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1376 - mse: 1.1376 - val_loss: 0.7764 - val_mse: 0.7764\n",
      "Epoch 18/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1373 - mse: 1.1373 - val_loss: 0.7764 - val_mse: 0.7764\n",
      "Epoch 19/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1376 - mse: 1.1376 - val_loss: 0.7758 - val_mse: 0.7758\n",
      "Epoch 20/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1374 - mse: 1.1374 - val_loss: 0.7768 - val_mse: 0.7768\n",
      "Epoch 21/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1377 - mse: 1.1377 - val_loss: 0.7773 - val_mse: 0.7773\n",
      "Epoch 22/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1371 - mse: 1.1371 - val_loss: 0.7788 - val_mse: 0.7788\n",
      "Epoch 23/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1373 - mse: 1.1373 - val_loss: 0.7765 - val_mse: 0.7765\n",
      "Epoch 24/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1374 - mse: 1.1374 - val_loss: 0.7762 - val_mse: 0.7762\n",
      "Epoch 25/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1372 - mse: 1.1372 - val_loss: 0.7778 - val_mse: 0.7778\n",
      "Epoch 26/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1376 - mse: 1.1376 - val_loss: 0.7764 - val_mse: 0.7764\n",
      "Epoch 27/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1374 - mse: 1.1374 - val_loss: 0.7781 - val_mse: 0.7781\n",
      "Epoch 28/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1371 - mse: 1.1371 - val_loss: 0.7772 - val_mse: 0.7772\n",
      "Epoch 29/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1373 - mse: 1.1373 - val_loss: 0.7779 - val_mse: 0.7779\n",
      "Epoch 30/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1372 - mse: 1.1372 - val_loss: 0.7789 - val_mse: 0.7789\n",
      "Epoch 31/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1373 - mse: 1.1373 - val_loss: 0.7783 - val_mse: 0.7783\n",
      "Epoch 32/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1381 - mse: 1.1381 - val_loss: 0.7767 - val_mse: 0.7767\n",
      "Epoch 33/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1374 - mse: 1.1374 - val_loss: 0.7764 - val_mse: 0.7764\n",
      "Epoch 34/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1371 - mse: 1.1371 - val_loss: 0.7760 - val_mse: 0.7760\n",
      "Epoch 35/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1375 - mse: 1.1375 - val_loss: 0.7767 - val_mse: 0.7767\n",
      "Epoch 36/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1376 - mse: 1.1376 - val_loss: 0.7760 - val_mse: 0.7760\n",
      "Epoch 37/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1374 - mse: 1.1374 - val_loss: 0.7759 - val_mse: 0.7759\n",
      "Epoch 38/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1371 - mse: 1.1371 - val_loss: 0.7747 - val_mse: 0.7747\n",
      "Epoch 39/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1371 - mse: 1.1371 - val_loss: 0.7744 - val_mse: 0.7744\n",
      "Epoch 40/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1374 - mse: 1.1374 - val_loss: 0.7751 - val_mse: 0.7751\n",
      "Epoch 41/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1371 - mse: 1.1371 - val_loss: 0.7755 - val_mse: 0.7755\n",
      "Epoch 42/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1373 - mse: 1.1373 - val_loss: 0.7762 - val_mse: 0.7762\n",
      "Epoch 43/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1374 - mse: 1.1374 - val_loss: 0.7767 - val_mse: 0.7767\n",
      "Epoch 44/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1371 - mse: 1.1371 - val_loss: 0.7753 - val_mse: 0.7753\n",
      "Epoch 45/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1370 - mse: 1.1370 - val_loss: 0.7762 - val_mse: 0.7762\n",
      "Epoch 46/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1371 - mse: 1.1371 - val_loss: 0.7758 - val_mse: 0.7758\n",
      "Epoch 47/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1372 - mse: 1.1372 - val_loss: 0.7753 - val_mse: 0.7753\n",
      "Epoch 48/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1373 - mse: 1.1373 - val_loss: 0.7742 - val_mse: 0.7742\n",
      "Epoch 49/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1377 - mse: 1.1377 - val_loss: 0.7749 - val_mse: 0.7749\n",
      "Epoch 50/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1372 - mse: 1.1372 - val_loss: 0.7754 - val_mse: 0.7754\n",
      "Epoch 51/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1376 - mse: 1.1376 - val_loss: 0.7756 - val_mse: 0.7756\n",
      "Epoch 52/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1372 - mse: 1.1372 - val_loss: 0.7765 - val_mse: 0.7765\n",
      "Epoch 53/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1374 - mse: 1.1374 - val_loss: 0.7770 - val_mse: 0.7770\n",
      "Epoch 54/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1373 - mse: 1.1373 - val_loss: 0.7776 - val_mse: 0.7776\n",
      "Epoch 55/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1375 - mse: 1.1375 - val_loss: 0.7783 - val_mse: 0.7783\n",
      "Epoch 56/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1372 - mse: 1.1372 - val_loss: 0.7777 - val_mse: 0.7777\n",
      "Epoch 57/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1372 - mse: 1.1372 - val_loss: 0.7769 - val_mse: 0.7769\n",
      "Epoch 58/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1371 - mse: 1.1371 - val_loss: 0.7776 - val_mse: 0.7776\n",
      "Epoch 59/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1373 - mse: 1.1373 - val_loss: 0.7783 - val_mse: 0.7783\n",
      "Epoch 60/60\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 1.1373 - mse: 1.1373 - val_loss: 0.7787 - val_mse: 0.7787\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.7787 - mse: 0.7787\n",
      "\n",
      "Weight: [array([[2.03014]], dtype=float32), array([1.0489523], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/60\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0123 - val_mse: 0.0123\n",
      "Epoch 2/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 3/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 4/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 5/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 6/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 7/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 8/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 9/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 10/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 11/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 12/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 13/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 14/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 15/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 16/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 17/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 18/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 19/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 20/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 21/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 22/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 23/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 24/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 25/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 26/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 27/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 28/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 29/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 30/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 31/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 32/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 33/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 34/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 35/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 36/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 37/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 38/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 39/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 40/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 41/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 42/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 43/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 44/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 45/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 46/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 47/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 48/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 49/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 50/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 51/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 52/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 53/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 54/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 55/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 56/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 57/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 58/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 59/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 60/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0108 - mse: 0.0108\n",
      "\n",
      "Weight: [array([[2.005555]], dtype=float32), array([0.994712], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/60\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.0790 - val_mse: 0.0790\n",
      "Epoch 2/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0858 - mse: 0.0858 - val_loss: 0.0794 - val_mse: 0.0794\n",
      "Epoch 3/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0797 - val_mse: 0.0797\n",
      "Epoch 4/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0801 - val_mse: 0.0801\n",
      "Epoch 5/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0802 - val_mse: 0.0802\n",
      "Epoch 6/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0802 - val_mse: 0.0802\n",
      "Epoch 7/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0802 - val_mse: 0.0802\n",
      "Epoch 8/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0803 - val_mse: 0.0803\n",
      "Epoch 9/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 10/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0805 - val_mse: 0.0805\n",
      "Epoch 11/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 12/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 13/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0803 - val_mse: 0.0803\n",
      "Epoch 14/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0802 - val_mse: 0.0802\n",
      "Epoch 15/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0803 - val_mse: 0.0803\n",
      "Epoch 16/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 17/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0803 - val_mse: 0.0803\n",
      "Epoch 18/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 19/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0805 - val_mse: 0.0805\n",
      "Epoch 20/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0805 - val_mse: 0.0805\n",
      "Epoch 21/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0805 - val_mse: 0.0805\n",
      "Epoch 22/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 23/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0805 - val_mse: 0.0805\n",
      "Epoch 24/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0805 - val_mse: 0.0805\n",
      "Epoch 25/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 26/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0802 - val_mse: 0.0802\n",
      "Epoch 27/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0803 - val_mse: 0.0803\n",
      "Epoch 28/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0803 - val_mse: 0.0803\n",
      "Epoch 29/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0803 - val_mse: 0.0803\n",
      "Epoch 30/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 31/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0805 - val_mse: 0.0805\n",
      "Epoch 32/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 33/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 34/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0805 - val_mse: 0.0805\n",
      "Epoch 35/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0806 - val_mse: 0.0806\n",
      "Epoch 36/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0807 - val_mse: 0.0807\n",
      "Epoch 37/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0807 - val_mse: 0.0807\n",
      "Epoch 38/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0806 - val_mse: 0.0806\n",
      "Epoch 39/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0806 - val_mse: 0.0806\n",
      "Epoch 40/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 41/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 42/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 43/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 44/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 45/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0803 - val_mse: 0.0803\n",
      "Epoch 46/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0803 - val_mse: 0.0803\n",
      "Epoch 47/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0802 - val_mse: 0.0802\n",
      "Epoch 48/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 49/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0805 - val_mse: 0.0805\n",
      "Epoch 50/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 51/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0803 - val_mse: 0.0803\n",
      "Epoch 52/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0803 - val_mse: 0.0803\n",
      "Epoch 53/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0802 - val_mse: 0.0802\n",
      "Epoch 54/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0803 - val_mse: 0.0803\n",
      "Epoch 55/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0803 - val_mse: 0.0803\n",
      "Epoch 56/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0805 - val_mse: 0.0805\n",
      "Epoch 57/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 58/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 59/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0803 - val_mse: 0.0803\n",
      "Epoch 60/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.0805 - val_mse: 0.0805\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0805 - mse: 0.0805\n",
      "\n",
      "Weight: [array([[1.994835]], dtype=float32), array([1.0103204], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/60\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2255 - mse: 0.2255 - val_loss: 0.2757 - val_mse: 0.2757\n",
      "Epoch 2/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2253 - mse: 0.2253 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 3/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2252 - mse: 0.2252 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 4/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2766 - val_mse: 0.2766\n",
      "Epoch 5/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2250 - mse: 0.2250 - val_loss: 0.2766 - val_mse: 0.2766\n",
      "Epoch 6/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2765 - val_mse: 0.2765\n",
      "Epoch 7/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2766 - val_mse: 0.2766\n",
      "Epoch 8/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2250 - mse: 0.2250 - val_loss: 0.2765 - val_mse: 0.2765\n",
      "Epoch 9/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2766 - val_mse: 0.2766\n",
      "Epoch 10/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2768 - val_mse: 0.2768\n",
      "Epoch 11/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2767 - val_mse: 0.2767\n",
      "Epoch 12/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2769 - val_mse: 0.2769\n",
      "Epoch 13/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2766 - val_mse: 0.2766\n",
      "Epoch 14/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2766 - val_mse: 0.2766\n",
      "Epoch 15/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 16/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 17/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2765 - val_mse: 0.2765\n",
      "Epoch 18/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2766 - val_mse: 0.2766\n",
      "Epoch 19/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2767 - val_mse: 0.2767\n",
      "Epoch 20/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2766 - val_mse: 0.2766\n",
      "Epoch 21/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2765 - val_mse: 0.2765\n",
      "Epoch 22/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 23/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2761 - val_mse: 0.2761\n",
      "Epoch 24/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 25/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2761 - val_mse: 0.2761\n",
      "Epoch 26/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2761 - val_mse: 0.2761\n",
      "Epoch 27/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 28/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2765 - val_mse: 0.2765\n",
      "Epoch 29/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2766 - val_mse: 0.2766\n",
      "Epoch 30/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 31/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 32/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2766 - val_mse: 0.2766\n",
      "Epoch 33/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2765 - val_mse: 0.2765\n",
      "Epoch 34/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 35/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 36/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 37/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 38/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 39/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 40/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 41/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2250 - mse: 0.2250 - val_loss: 0.2765 - val_mse: 0.2765\n",
      "Epoch 42/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 43/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 44/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 45/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2761 - val_mse: 0.2761\n",
      "Epoch 46/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 47/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 48/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 49/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 50/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 51/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 52/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2760 - val_mse: 0.2760\n",
      "Epoch 53/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2761 - val_mse: 0.2761\n",
      "Epoch 54/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 55/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 56/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 57/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 58/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 59/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2765 - val_mse: 0.2765\n",
      "Epoch 60/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2764 - mse: 0.2764\n",
      "\n",
      "Weight: [array([[1.9581065]], dtype=float32), array([0.98985994], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/60\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3895 - val_mse: 0.3895\n",
      "Epoch 2/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3899 - val_mse: 0.3899\n",
      "Epoch 3/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4636 - mse: 0.4636 - val_loss: 0.3902 - val_mse: 0.3902\n",
      "Epoch 4/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4635 - mse: 0.4635 - val_loss: 0.3906 - val_mse: 0.3906\n",
      "Epoch 5/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3901 - val_mse: 0.3901\n",
      "Epoch 6/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3901 - val_mse: 0.3901\n",
      "Epoch 7/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4633 - mse: 0.4633 - val_loss: 0.3900 - val_mse: 0.3900\n",
      "Epoch 8/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3903 - val_mse: 0.3903\n",
      "Epoch 9/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3897 - val_mse: 0.3897\n",
      "Epoch 10/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3896 - val_mse: 0.3896\n",
      "Epoch 11/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3899 - val_mse: 0.3899\n",
      "Epoch 12/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4635 - mse: 0.4635 - val_loss: 0.3901 - val_mse: 0.3901\n",
      "Epoch 13/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4633 - mse: 0.4633 - val_loss: 0.3907 - val_mse: 0.3907\n",
      "Epoch 14/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4633 - mse: 0.4633 - val_loss: 0.3905 - val_mse: 0.3905\n",
      "Epoch 15/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4635 - mse: 0.4635 - val_loss: 0.3909 - val_mse: 0.3909\n",
      "Epoch 16/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3907 - val_mse: 0.3907\n",
      "Epoch 17/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4635 - mse: 0.4635 - val_loss: 0.3902 - val_mse: 0.3902\n",
      "Epoch 18/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4633 - mse: 0.4633 - val_loss: 0.3903 - val_mse: 0.3903\n",
      "Epoch 19/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4633 - mse: 0.4633 - val_loss: 0.3909 - val_mse: 0.3909\n",
      "Epoch 20/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4633 - mse: 0.4633 - val_loss: 0.3898 - val_mse: 0.3898\n",
      "Epoch 21/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4633 - mse: 0.4633 - val_loss: 0.3898 - val_mse: 0.3898\n",
      "Epoch 22/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3906 - val_mse: 0.3906\n",
      "Epoch 23/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3900 - val_mse: 0.3900\n",
      "Epoch 24/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3894 - val_mse: 0.3894\n",
      "Epoch 25/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4633 - mse: 0.4633 - val_loss: 0.3900 - val_mse: 0.3900\n",
      "Epoch 26/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3901 - val_mse: 0.3901\n",
      "Epoch 27/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4635 - mse: 0.4635 - val_loss: 0.3903 - val_mse: 0.3903\n",
      "Epoch 28/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3901 - val_mse: 0.3901\n",
      "Epoch 29/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3901 - val_mse: 0.3901\n",
      "Epoch 30/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4635 - mse: 0.4635 - val_loss: 0.3901 - val_mse: 0.3901\n",
      "Epoch 31/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4635 - mse: 0.4635 - val_loss: 0.3898 - val_mse: 0.3898\n",
      "Epoch 32/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3902 - val_mse: 0.3902\n",
      "Epoch 33/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3901 - val_mse: 0.3901\n",
      "Epoch 34/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3905 - val_mse: 0.3905\n",
      "Epoch 35/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4632 - mse: 0.4632 - val_loss: 0.3905 - val_mse: 0.3905\n",
      "Epoch 36/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4633 - mse: 0.4633 - val_loss: 0.3907 - val_mse: 0.3907\n",
      "Epoch 37/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3901 - val_mse: 0.3901\n",
      "Epoch 38/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4632 - mse: 0.4632 - val_loss: 0.3903 - val_mse: 0.3903\n",
      "Epoch 39/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3900 - val_mse: 0.3900\n",
      "Epoch 40/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4633 - mse: 0.4633 - val_loss: 0.3906 - val_mse: 0.3906\n",
      "Epoch 41/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4632 - mse: 0.4632 - val_loss: 0.3904 - val_mse: 0.3904\n",
      "Epoch 42/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4635 - mse: 0.4635 - val_loss: 0.3906 - val_mse: 0.3906\n",
      "Epoch 43/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3909 - val_mse: 0.3909\n",
      "Epoch 44/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4633 - mse: 0.4633 - val_loss: 0.3909 - val_mse: 0.3909\n",
      "Epoch 45/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3907 - val_mse: 0.3907\n",
      "Epoch 46/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3906 - val_mse: 0.3906\n",
      "Epoch 47/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4633 - mse: 0.4633 - val_loss: 0.3903 - val_mse: 0.3903\n",
      "Epoch 48/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4633 - mse: 0.4633 - val_loss: 0.3905 - val_mse: 0.3905\n",
      "Epoch 49/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4633 - mse: 0.4633 - val_loss: 0.3906 - val_mse: 0.3906\n",
      "Epoch 50/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3904 - val_mse: 0.3904\n",
      "Epoch 51/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4633 - mse: 0.4633 - val_loss: 0.3898 - val_mse: 0.3898\n",
      "Epoch 52/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4632 - mse: 0.4632 - val_loss: 0.3893 - val_mse: 0.3893\n",
      "Epoch 53/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4635 - mse: 0.4635 - val_loss: 0.3894 - val_mse: 0.3894\n",
      "Epoch 54/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4633 - mse: 0.4633 - val_loss: 0.3896 - val_mse: 0.3896\n",
      "Epoch 55/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4635 - mse: 0.4635 - val_loss: 0.3887 - val_mse: 0.3887\n",
      "Epoch 56/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.3890 - val_mse: 0.3890\n",
      "Epoch 57/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4635 - mse: 0.4635 - val_loss: 0.3896 - val_mse: 0.3896\n",
      "Epoch 58/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4635 - mse: 0.4635 - val_loss: 0.3900 - val_mse: 0.3900\n",
      "Epoch 59/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4633 - mse: 0.4633 - val_loss: 0.3900 - val_mse: 0.3900\n",
      "Epoch 60/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4635 - mse: 0.4635 - val_loss: 0.3903 - val_mse: 0.3903\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.3903 - mse: 0.3903\n",
      "\n",
      "Weight: [array([[1.9619362]], dtype=float32), array([0.99547017], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/60\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.7989 - mse: 0.7989 - val_loss: 0.6723 - val_mse: 0.6723\n",
      "Epoch 2/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7984 - mse: 0.7984 - val_loss: 0.6716 - val_mse: 0.6716\n",
      "Epoch 3/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7985 - mse: 0.7985 - val_loss: 0.6710 - val_mse: 0.6710\n",
      "Epoch 4/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7985 - mse: 0.7985 - val_loss: 0.6706 - val_mse: 0.6706\n",
      "Epoch 5/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7984 - mse: 0.7984 - val_loss: 0.6702 - val_mse: 0.6702\n",
      "Epoch 6/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7981 - mse: 0.7981 - val_loss: 0.6700 - val_mse: 0.6700\n",
      "Epoch 7/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7982 - mse: 0.7982 - val_loss: 0.6698 - val_mse: 0.6698\n",
      "Epoch 8/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7983 - mse: 0.7983 - val_loss: 0.6697 - val_mse: 0.6697\n",
      "Epoch 9/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7985 - mse: 0.7985 - val_loss: 0.6696 - val_mse: 0.6696\n",
      "Epoch 10/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7982 - mse: 0.7982 - val_loss: 0.6693 - val_mse: 0.6693\n",
      "Epoch 11/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7979 - mse: 0.7979 - val_loss: 0.6692 - val_mse: 0.6692\n",
      "Epoch 12/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7981 - mse: 0.7981 - val_loss: 0.6691 - val_mse: 0.6691\n",
      "Epoch 13/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7982 - mse: 0.7982 - val_loss: 0.6689 - val_mse: 0.6689\n",
      "Epoch 14/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7981 - mse: 0.7981 - val_loss: 0.6688 - val_mse: 0.6688\n",
      "Epoch 15/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7982 - mse: 0.7982 - val_loss: 0.6687 - val_mse: 0.6687\n",
      "Epoch 16/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7981 - mse: 0.7981 - val_loss: 0.6687 - val_mse: 0.6687\n",
      "Epoch 17/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7980 - mse: 0.7980 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 18/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7981 - mse: 0.7981 - val_loss: 0.6684 - val_mse: 0.6684\n",
      "Epoch 19/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7984 - mse: 0.7984 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 20/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7981 - mse: 0.7981 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 21/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7982 - mse: 0.7982 - val_loss: 0.6687 - val_mse: 0.6687\n",
      "Epoch 22/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7982 - mse: 0.7982 - val_loss: 0.6687 - val_mse: 0.6687\n",
      "Epoch 23/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7980 - mse: 0.7980 - val_loss: 0.6687 - val_mse: 0.6687\n",
      "Epoch 24/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7983 - mse: 0.7983 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 25/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7982 - mse: 0.7982 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 26/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7983 - mse: 0.7983 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 27/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7981 - mse: 0.7981 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 28/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7982 - mse: 0.7982 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 29/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7980 - mse: 0.7980 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 30/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7981 - mse: 0.7981 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 31/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7982 - mse: 0.7982 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 32/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7983 - mse: 0.7983 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 33/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7980 - mse: 0.7980 - val_loss: 0.6684 - val_mse: 0.6684\n",
      "Epoch 34/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7981 - mse: 0.7981 - val_loss: 0.6683 - val_mse: 0.6683\n",
      "Epoch 35/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7982 - mse: 0.7982 - val_loss: 0.6683 - val_mse: 0.6683\n",
      "Epoch 36/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7981 - mse: 0.7981 - val_loss: 0.6683 - val_mse: 0.6683\n",
      "Epoch 37/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7984 - mse: 0.7984 - val_loss: 0.6684 - val_mse: 0.6684\n",
      "Epoch 38/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7983 - mse: 0.7983 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 39/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7979 - mse: 0.7979 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 40/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7980 - mse: 0.7980 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 41/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7984 - mse: 0.7984 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 42/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7981 - mse: 0.7981 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 43/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7980 - mse: 0.7980 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 44/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7984 - mse: 0.7984 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 45/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7980 - mse: 0.7980 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 46/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7984 - mse: 0.7984 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 47/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7979 - mse: 0.7979 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 48/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7979 - mse: 0.7979 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 49/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7983 - mse: 0.7983 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 50/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7982 - mse: 0.7982 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 51/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7981 - mse: 0.7981 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 52/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7982 - mse: 0.7982 - val_loss: 0.6684 - val_mse: 0.6684\n",
      "Epoch 53/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7981 - mse: 0.7981 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 54/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7981 - mse: 0.7981 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 55/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7981 - mse: 0.7981 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 56/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7980 - mse: 0.7980 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 57/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7982 - mse: 0.7982 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 58/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7981 - mse: 0.7981 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 59/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7980 - mse: 0.7980 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 60/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7980 - mse: 0.7980 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.6686 - mse: 0.6686\n",
      "\n",
      "Weight: [array([[2.0058627]], dtype=float32), array([1.0139314], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  700.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/60\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.2583 - mse: 1.2583 - val_loss: 1.5474 - val_mse: 1.5474\n",
      "Epoch 2/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2570 - mse: 1.2570 - val_loss: 1.5502 - val_mse: 1.5502\n",
      "Epoch 3/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2562 - mse: 1.2562 - val_loss: 1.5518 - val_mse: 1.5518\n",
      "Epoch 4/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2558 - mse: 1.2558 - val_loss: 1.5526 - val_mse: 1.5526\n",
      "Epoch 5/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2554 - mse: 1.2554 - val_loss: 1.5533 - val_mse: 1.5533\n",
      "Epoch 6/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2552 - mse: 1.2552 - val_loss: 1.5530 - val_mse: 1.5530\n",
      "Epoch 7/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2554 - mse: 1.2554 - val_loss: 1.5533 - val_mse: 1.5533\n",
      "Epoch 8/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2550 - mse: 1.2550 - val_loss: 1.5541 - val_mse: 1.5541\n",
      "Epoch 9/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2549 - mse: 1.2549 - val_loss: 1.5531 - val_mse: 1.5531\n",
      "Epoch 10/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2551 - mse: 1.2551 - val_loss: 1.5551 - val_mse: 1.5551\n",
      "Epoch 11/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2546 - mse: 1.2546 - val_loss: 1.5558 - val_mse: 1.5558\n",
      "Epoch 12/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2552 - mse: 1.2552 - val_loss: 1.5561 - val_mse: 1.5561\n",
      "Epoch 13/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2547 - mse: 1.2547 - val_loss: 1.5554 - val_mse: 1.5554\n",
      "Epoch 14/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2546 - mse: 1.2546 - val_loss: 1.5549 - val_mse: 1.5549\n",
      "Epoch 15/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2547 - mse: 1.2547 - val_loss: 1.5548 - val_mse: 1.5548\n",
      "Epoch 16/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2548 - mse: 1.2548 - val_loss: 1.5554 - val_mse: 1.5554\n",
      "Epoch 17/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2547 - mse: 1.2547 - val_loss: 1.5548 - val_mse: 1.5548\n",
      "Epoch 18/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2546 - mse: 1.2546 - val_loss: 1.5557 - val_mse: 1.5557\n",
      "Epoch 19/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2545 - mse: 1.2545 - val_loss: 1.5568 - val_mse: 1.5568\n",
      "Epoch 20/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2545 - mse: 1.2545 - val_loss: 1.5552 - val_mse: 1.5552\n",
      "Epoch 21/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2549 - mse: 1.2549 - val_loss: 1.5552 - val_mse: 1.5552\n",
      "Epoch 22/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2551 - mse: 1.2551 - val_loss: 1.5542 - val_mse: 1.5542\n",
      "Epoch 23/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2547 - mse: 1.2547 - val_loss: 1.5524 - val_mse: 1.5524\n",
      "Epoch 24/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2544 - mse: 1.2544 - val_loss: 1.5547 - val_mse: 1.5547\n",
      "Epoch 25/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2546 - mse: 1.2546 - val_loss: 1.5544 - val_mse: 1.5544\n",
      "Epoch 26/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2545 - mse: 1.2545 - val_loss: 1.5554 - val_mse: 1.5554\n",
      "Epoch 27/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2551 - mse: 1.2551 - val_loss: 1.5554 - val_mse: 1.5554\n",
      "Epoch 28/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2545 - mse: 1.2545 - val_loss: 1.5558 - val_mse: 1.5558\n",
      "Epoch 29/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2547 - mse: 1.2547 - val_loss: 1.5547 - val_mse: 1.5547\n",
      "Epoch 30/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2544 - mse: 1.2544 - val_loss: 1.5546 - val_mse: 1.5546\n",
      "Epoch 31/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2544 - mse: 1.2544 - val_loss: 1.5557 - val_mse: 1.5557\n",
      "Epoch 32/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2547 - mse: 1.2547 - val_loss: 1.5548 - val_mse: 1.5548\n",
      "Epoch 33/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2547 - mse: 1.2547 - val_loss: 1.5534 - val_mse: 1.5534\n",
      "Epoch 34/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2547 - mse: 1.2547 - val_loss: 1.5550 - val_mse: 1.5550\n",
      "Epoch 35/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2546 - mse: 1.2546 - val_loss: 1.5572 - val_mse: 1.5572\n",
      "Epoch 36/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2550 - mse: 1.2550 - val_loss: 1.5554 - val_mse: 1.5554\n",
      "Epoch 37/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2548 - mse: 1.2548 - val_loss: 1.5553 - val_mse: 1.5553\n",
      "Epoch 38/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2547 - mse: 1.2547 - val_loss: 1.5555 - val_mse: 1.5555\n",
      "Epoch 39/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2550 - mse: 1.2550 - val_loss: 1.5542 - val_mse: 1.5542\n",
      "Epoch 40/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2549 - mse: 1.2549 - val_loss: 1.5536 - val_mse: 1.5536\n",
      "Epoch 41/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2548 - mse: 1.2548 - val_loss: 1.5536 - val_mse: 1.5536\n",
      "Epoch 42/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2551 - mse: 1.2551 - val_loss: 1.5546 - val_mse: 1.5546\n",
      "Epoch 43/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2546 - mse: 1.2546 - val_loss: 1.5554 - val_mse: 1.5554\n",
      "Epoch 44/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2550 - mse: 1.2550 - val_loss: 1.5569 - val_mse: 1.5569\n",
      "Epoch 45/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2549 - mse: 1.2549 - val_loss: 1.5556 - val_mse: 1.5556\n",
      "Epoch 46/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2549 - mse: 1.2549 - val_loss: 1.5547 - val_mse: 1.5547\n",
      "Epoch 47/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2546 - mse: 1.2546 - val_loss: 1.5533 - val_mse: 1.5533\n",
      "Epoch 48/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2550 - mse: 1.2550 - val_loss: 1.5531 - val_mse: 1.5531\n",
      "Epoch 49/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2549 - mse: 1.2549 - val_loss: 1.5534 - val_mse: 1.5534\n",
      "Epoch 50/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2543 - mse: 1.2543 - val_loss: 1.5533 - val_mse: 1.5533\n",
      "Epoch 51/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2547 - mse: 1.2547 - val_loss: 1.5525 - val_mse: 1.5525\n",
      "Epoch 52/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2549 - mse: 1.2549 - val_loss: 1.5552 - val_mse: 1.5552\n",
      "Epoch 53/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2548 - mse: 1.2548 - val_loss: 1.5547 - val_mse: 1.5547\n",
      "Epoch 54/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2547 - mse: 1.2547 - val_loss: 1.5541 - val_mse: 1.5541\n",
      "Epoch 55/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2548 - mse: 1.2548 - val_loss: 1.5542 - val_mse: 1.5542\n",
      "Epoch 56/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2544 - mse: 1.2544 - val_loss: 1.5534 - val_mse: 1.5534\n",
      "Epoch 57/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2545 - mse: 1.2545 - val_loss: 1.5535 - val_mse: 1.5535\n",
      "Epoch 58/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2547 - mse: 1.2547 - val_loss: 1.5540 - val_mse: 1.5540\n",
      "Epoch 59/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2549 - mse: 1.2549 - val_loss: 1.5538 - val_mse: 1.5538\n",
      "Epoch 60/60\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2548 - mse: 1.2548 - val_loss: 1.5532 - val_mse: 1.5532\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.5532 - mse: 1.5532\n",
      "\n",
      "Weight: [array([[2.1057973]], dtype=float32), array([0.99068165], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/60\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0125 - val_mse: 0.0125\n",
      "Epoch 2/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 3/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 4/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 5/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 6/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 7/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 8/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 9/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 10/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 11/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 12/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 13/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 14/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 15/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 16/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 17/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 18/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 19/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 20/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 21/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 22/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 23/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 24/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 25/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 26/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 27/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 28/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 29/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 30/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 31/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 32/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 33/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 34/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 35/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 36/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 37/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 38/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 39/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 40/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 41/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 42/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 43/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 44/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 45/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 46/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 47/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 48/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 49/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 50/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 51/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 52/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 53/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 54/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 55/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 56/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 57/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 58/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 59/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 60/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095\n",
      "\n",
      "Weight: [array([[2.0023148]], dtype=float32), array([0.9964519], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/60\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.1089 - val_mse: 0.1089\n",
      "Epoch 2/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0902 - mse: 0.0902 - val_loss: 0.1092 - val_mse: 0.1092\n",
      "Epoch 3/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0900 - mse: 0.0900 - val_loss: 0.1094 - val_mse: 0.1094\n",
      "Epoch 4/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0899 - mse: 0.0899 - val_loss: 0.1096 - val_mse: 0.1096\n",
      "Epoch 5/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0898 - mse: 0.0898 - val_loss: 0.1098 - val_mse: 0.1098\n",
      "Epoch 6/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.1100 - val_mse: 0.1100\n",
      "Epoch 7/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.1101 - val_mse: 0.1101\n",
      "Epoch 8/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.1103 - val_mse: 0.1103\n",
      "Epoch 9/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.1104 - val_mse: 0.1104\n",
      "Epoch 10/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1105 - val_mse: 0.1105\n",
      "Epoch 11/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1106 - val_mse: 0.1106\n",
      "Epoch 12/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1107 - val_mse: 0.1107\n",
      "Epoch 13/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1107 - val_mse: 0.1107\n",
      "Epoch 14/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1108 - val_mse: 0.1108\n",
      "Epoch 15/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 16/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 17/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 18/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 19/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 20/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 21/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 22/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 23/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 24/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 25/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 26/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 27/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 28/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 29/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 30/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 31/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 32/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 33/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 34/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 35/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 36/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 37/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 38/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 39/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 40/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 41/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 42/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 43/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 44/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 45/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 46/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 47/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 48/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 49/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 50/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 51/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 52/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 53/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 54/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 55/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 56/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 57/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 58/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 59/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 60/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.1112 - mse: 0.1112\n",
      "\n",
      "Weight: [array([[2.058361]], dtype=float32), array([1.0089456], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/60\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2652 - val_mse: 0.2652\n",
      "Epoch 2/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2632 - mse: 0.2632 - val_loss: 0.2660 - val_mse: 0.2660\n",
      "Epoch 3/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2628 - mse: 0.2628 - val_loss: 0.2668 - val_mse: 0.2668\n",
      "Epoch 4/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2625 - mse: 0.2625 - val_loss: 0.2676 - val_mse: 0.2676\n",
      "Epoch 5/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2624 - mse: 0.2624 - val_loss: 0.2682 - val_mse: 0.2682\n",
      "Epoch 6/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2623 - mse: 0.2623 - val_loss: 0.2689 - val_mse: 0.2689\n",
      "Epoch 7/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2622 - mse: 0.2622 - val_loss: 0.2693 - val_mse: 0.2693\n",
      "Epoch 8/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2621 - mse: 0.2621 - val_loss: 0.2698 - val_mse: 0.2698\n",
      "Epoch 9/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.2701 - val_mse: 0.2701\n",
      "Epoch 10/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.2704 - val_mse: 0.2704\n",
      "Epoch 11/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2621 - mse: 0.2621 - val_loss: 0.2707 - val_mse: 0.2707\n",
      "Epoch 12/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.2710 - val_mse: 0.2710\n",
      "Epoch 13/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.2711 - val_mse: 0.2711\n",
      "Epoch 14/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2713 - val_mse: 0.2713\n",
      "Epoch 15/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2621 - mse: 0.2621 - val_loss: 0.2714 - val_mse: 0.2714\n",
      "Epoch 16/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2716 - val_mse: 0.2716\n",
      "Epoch 17/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.2717 - val_mse: 0.2717\n",
      "Epoch 18/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2718 - val_mse: 0.2718\n",
      "Epoch 19/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2719 - val_mse: 0.2719\n",
      "Epoch 20/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2720 - val_mse: 0.2720\n",
      "Epoch 21/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2721 - val_mse: 0.2721\n",
      "Epoch 22/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.2721 - val_mse: 0.2721\n",
      "Epoch 23/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2722 - val_mse: 0.2722\n",
      "Epoch 24/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2722 - val_mse: 0.2722\n",
      "Epoch 25/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2722 - val_mse: 0.2722\n",
      "Epoch 26/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2722 - val_mse: 0.2722\n",
      "Epoch 27/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 28/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2722 - val_mse: 0.2722\n",
      "Epoch 29/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2618 - mse: 0.2618 - val_loss: 0.2722 - val_mse: 0.2722\n",
      "Epoch 30/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2722 - val_mse: 0.2722\n",
      "Epoch 31/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 32/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 33/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2722 - val_mse: 0.2722\n",
      "Epoch 34/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 35/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 36/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 37/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 38/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 39/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 40/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 41/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 42/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 43/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 44/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 45/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 46/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 47/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 48/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2621 - mse: 0.2621 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 49/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.2724 - val_mse: 0.2724\n",
      "Epoch 50/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 51/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.2724 - val_mse: 0.2724\n",
      "Epoch 52/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 53/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 54/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 55/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 56/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2724 - val_mse: 0.2724\n",
      "Epoch 57/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2724 - val_mse: 0.2724\n",
      "Epoch 58/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 59/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 60/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2723 - mse: 0.2723\n",
      "\n",
      "Weight: [array([[1.9830078]], dtype=float32), array([0.9844624], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/60\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5005 - mse: 0.5005 - val_loss: 0.5182 - val_mse: 0.5182\n",
      "Epoch 2/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5002 - mse: 0.5002 - val_loss: 0.5175 - val_mse: 0.5175\n",
      "Epoch 3/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5173 - val_mse: 0.5173\n",
      "Epoch 4/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5001 - mse: 0.5001 - val_loss: 0.5168 - val_mse: 0.5168\n",
      "Epoch 5/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5167 - val_mse: 0.5167\n",
      "Epoch 6/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5173 - val_mse: 0.5173\n",
      "Epoch 7/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5001 - mse: 0.5001 - val_loss: 0.5170 - val_mse: 0.5170\n",
      "Epoch 8/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5172 - val_mse: 0.5172\n",
      "Epoch 9/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5001 - mse: 0.5001 - val_loss: 0.5175 - val_mse: 0.5175\n",
      "Epoch 10/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5174 - val_mse: 0.5174\n",
      "Epoch 11/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5178 - val_mse: 0.5178\n",
      "Epoch 12/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4999 - mse: 0.4999 - val_loss: 0.5177 - val_mse: 0.5177\n",
      "Epoch 13/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5003 - mse: 0.5003 - val_loss: 0.5174 - val_mse: 0.5174\n",
      "Epoch 14/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5176 - val_mse: 0.5176\n",
      "Epoch 15/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5178 - val_mse: 0.5178\n",
      "Epoch 16/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5180 - val_mse: 0.5180\n",
      "Epoch 17/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5180 - val_mse: 0.5180\n",
      "Epoch 18/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4999 - mse: 0.4999 - val_loss: 0.5177 - val_mse: 0.5177\n",
      "Epoch 19/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5177 - val_mse: 0.5177\n",
      "Epoch 20/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5002 - mse: 0.5002 - val_loss: 0.5178 - val_mse: 0.5178\n",
      "Epoch 21/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5179 - val_mse: 0.5179\n",
      "Epoch 22/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5001 - mse: 0.5001 - val_loss: 0.5177 - val_mse: 0.5177\n",
      "Epoch 23/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5001 - mse: 0.5001 - val_loss: 0.5177 - val_mse: 0.5177\n",
      "Epoch 24/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4999 - mse: 0.4999 - val_loss: 0.5179 - val_mse: 0.5179\n",
      "Epoch 25/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5001 - mse: 0.5001 - val_loss: 0.5177 - val_mse: 0.5177\n",
      "Epoch 26/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5176 - val_mse: 0.5176\n",
      "Epoch 27/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5178 - val_mse: 0.5178\n",
      "Epoch 28/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4999 - mse: 0.4999 - val_loss: 0.5176 - val_mse: 0.5176\n",
      "Epoch 29/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5175 - val_mse: 0.5175\n",
      "Epoch 30/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5001 - mse: 0.5001 - val_loss: 0.5175 - val_mse: 0.5175\n",
      "Epoch 31/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5002 - mse: 0.5002 - val_loss: 0.5177 - val_mse: 0.5177\n",
      "Epoch 32/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5001 - mse: 0.5001 - val_loss: 0.5175 - val_mse: 0.5175\n",
      "Epoch 33/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5178 - val_mse: 0.5178\n",
      "Epoch 34/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5001 - mse: 0.5001 - val_loss: 0.5178 - val_mse: 0.5178\n",
      "Epoch 35/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5178 - val_mse: 0.5178\n",
      "Epoch 36/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5177 - val_mse: 0.5177\n",
      "Epoch 37/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4999 - mse: 0.4999 - val_loss: 0.5175 - val_mse: 0.5175\n",
      "Epoch 38/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5001 - mse: 0.5001 - val_loss: 0.5176 - val_mse: 0.5176\n",
      "Epoch 39/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5172 - val_mse: 0.5172\n",
      "Epoch 40/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4999 - mse: 0.4999 - val_loss: 0.5175 - val_mse: 0.5175\n",
      "Epoch 41/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5001 - mse: 0.5001 - val_loss: 0.5172 - val_mse: 0.5172\n",
      "Epoch 42/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5002 - mse: 0.5002 - val_loss: 0.5170 - val_mse: 0.5170\n",
      "Epoch 43/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4999 - mse: 0.4999 - val_loss: 0.5172 - val_mse: 0.5172\n",
      "Epoch 44/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4999 - mse: 0.4999 - val_loss: 0.5171 - val_mse: 0.5171\n",
      "Epoch 45/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5001 - mse: 0.5001 - val_loss: 0.5173 - val_mse: 0.5173\n",
      "Epoch 46/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5179 - val_mse: 0.5179\n",
      "Epoch 47/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5002 - mse: 0.5002 - val_loss: 0.5177 - val_mse: 0.5177\n",
      "Epoch 48/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5173 - val_mse: 0.5173\n",
      "Epoch 49/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5173 - val_mse: 0.5173\n",
      "Epoch 50/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5175 - val_mse: 0.5175\n",
      "Epoch 51/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5001 - mse: 0.5001 - val_loss: 0.5173 - val_mse: 0.5173\n",
      "Epoch 52/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5175 - val_mse: 0.5175\n",
      "Epoch 53/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5001 - mse: 0.5001 - val_loss: 0.5171 - val_mse: 0.5171\n",
      "Epoch 54/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5175 - val_mse: 0.5175\n",
      "Epoch 55/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4999 - mse: 0.4999 - val_loss: 0.5178 - val_mse: 0.5178\n",
      "Epoch 56/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5002 - mse: 0.5002 - val_loss: 0.5176 - val_mse: 0.5176\n",
      "Epoch 57/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5176 - val_mse: 0.5176\n",
      "Epoch 58/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5001 - mse: 0.5001 - val_loss: 0.5175 - val_mse: 0.5175\n",
      "Epoch 59/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5000 - mse: 0.5000 - val_loss: 0.5174 - val_mse: 0.5174\n",
      "Epoch 60/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4999 - mse: 0.4999 - val_loss: 0.5174 - val_mse: 0.5174\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5174 - mse: 0.5174\n",
      "\n",
      "Weight: [array([[1.9721664]], dtype=float32), array([1.0112771], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/60\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8277 - mse: 0.8277 - val_loss: 0.6461 - val_mse: 0.6461\n",
      "Epoch 2/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8273 - mse: 0.8273 - val_loss: 0.6453 - val_mse: 0.6453\n",
      "Epoch 3/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8274 - mse: 0.8274 - val_loss: 0.6452 - val_mse: 0.6452\n",
      "Epoch 4/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8269 - mse: 0.8269 - val_loss: 0.6449 - val_mse: 0.6449\n",
      "Epoch 5/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8268 - mse: 0.8268 - val_loss: 0.6448 - val_mse: 0.6448\n",
      "Epoch 6/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8271 - mse: 0.8271 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 7/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.6448 - val_mse: 0.6448\n",
      "Epoch 8/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6449 - val_mse: 0.6449\n",
      "Epoch 9/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6448 - val_mse: 0.6448\n",
      "Epoch 10/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8268 - mse: 0.8268 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 11/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 12/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 13/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8270 - mse: 0.8270 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 14/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 15/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 16/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8272 - mse: 0.8272 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 17/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 18/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8267 - mse: 0.8267 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 19/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8264 - mse: 0.8264 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 20/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6448 - val_mse: 0.6448\n",
      "Epoch 21/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8263 - mse: 0.8263 - val_loss: 0.6448 - val_mse: 0.6448\n",
      "Epoch 22/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8263 - mse: 0.8263 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 23/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8267 - mse: 0.8267 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 24/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8268 - mse: 0.8268 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 25/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8262 - mse: 0.8262 - val_loss: 0.6445 - val_mse: 0.6445\n",
      "Epoch 26/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 27/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8267 - mse: 0.8267 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 28/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 29/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 30/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8268 - mse: 0.8268 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 31/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 32/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 33/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8264 - mse: 0.8264 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 34/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 35/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 36/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 37/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8267 - mse: 0.8267 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 38/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 39/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8264 - mse: 0.8264 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 40/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8264 - mse: 0.8264 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 41/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 42/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8263 - mse: 0.8263 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 43/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 44/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 45/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8267 - mse: 0.8267 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 46/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 47/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8264 - mse: 0.8264 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 48/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8268 - mse: 0.8268 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 49/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 50/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 51/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8264 - mse: 0.8264 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 52/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 53/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 54/60\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 55/60\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 56/60\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 57/60\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 58/60\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8266 - mse: 0.8266 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 59/60\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6448 - val_mse: 0.6448\n",
      "Epoch 60/60\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8265 - mse: 0.8265 - val_loss: 0.6449 - val_mse: 0.6449\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.6449 - mse: 0.6449\n",
      "\n",
      "Weight: [array([[2.0320737]], dtype=float32), array([1.0238577], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  800.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/60\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.1802 - mse: 1.1802 - val_loss: 1.1737 - val_mse: 1.1737\n",
      "Epoch 2/60\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1799 - mse: 1.1799 - val_loss: 1.1746 - val_mse: 1.1746\n",
      "Epoch 3/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1795 - mse: 1.1795 - val_loss: 1.1729 - val_mse: 1.1729\n",
      "Epoch 4/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1792 - mse: 1.1792 - val_loss: 1.1733 - val_mse: 1.1733\n",
      "Epoch 5/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1795 - mse: 1.1795 - val_loss: 1.1724 - val_mse: 1.1724\n",
      "Epoch 6/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1795 - mse: 1.1795 - val_loss: 1.1727 - val_mse: 1.1727\n",
      "Epoch 7/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1795 - mse: 1.1795 - val_loss: 1.1733 - val_mse: 1.1733\n",
      "Epoch 8/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1797 - mse: 1.1797 - val_loss: 1.1734 - val_mse: 1.1734\n",
      "Epoch 9/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1794 - mse: 1.1794 - val_loss: 1.1737 - val_mse: 1.1737\n",
      "Epoch 10/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1794 - mse: 1.1794 - val_loss: 1.1736 - val_mse: 1.1736\n",
      "Epoch 11/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1796 - mse: 1.1796 - val_loss: 1.1728 - val_mse: 1.1728\n",
      "Epoch 12/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1794 - mse: 1.1794 - val_loss: 1.1725 - val_mse: 1.1725\n",
      "Epoch 13/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1795 - mse: 1.1795 - val_loss: 1.1716 - val_mse: 1.1716\n",
      "Epoch 14/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1794 - mse: 1.1794 - val_loss: 1.1726 - val_mse: 1.1726\n",
      "Epoch 15/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1796 - mse: 1.1796 - val_loss: 1.1743 - val_mse: 1.1743\n",
      "Epoch 16/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1795 - mse: 1.1795 - val_loss: 1.1729 - val_mse: 1.1729\n",
      "Epoch 17/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1793 - mse: 1.1793 - val_loss: 1.1725 - val_mse: 1.1725\n",
      "Epoch 18/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1792 - mse: 1.1792 - val_loss: 1.1733 - val_mse: 1.1733\n",
      "Epoch 19/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1789 - mse: 1.1789 - val_loss: 1.1730 - val_mse: 1.1730\n",
      "Epoch 20/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1791 - mse: 1.1791 - val_loss: 1.1724 - val_mse: 1.1724\n",
      "Epoch 21/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1793 - mse: 1.1793 - val_loss: 1.1724 - val_mse: 1.1724\n",
      "Epoch 22/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1792 - mse: 1.1792 - val_loss: 1.1723 - val_mse: 1.1723\n",
      "Epoch 23/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1792 - mse: 1.1792 - val_loss: 1.1732 - val_mse: 1.1732\n",
      "Epoch 24/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1795 - mse: 1.1795 - val_loss: 1.1726 - val_mse: 1.1726\n",
      "Epoch 25/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1795 - mse: 1.1795 - val_loss: 1.1728 - val_mse: 1.1728\n",
      "Epoch 26/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1793 - mse: 1.1793 - val_loss: 1.1728 - val_mse: 1.1728\n",
      "Epoch 27/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1797 - mse: 1.1797 - val_loss: 1.1725 - val_mse: 1.1725\n",
      "Epoch 28/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1792 - mse: 1.1792 - val_loss: 1.1725 - val_mse: 1.1725\n",
      "Epoch 29/60\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1791 - mse: 1.1791 - val_loss: 1.1732 - val_mse: 1.1732\n",
      "Epoch 30/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1797 - mse: 1.1797 - val_loss: 1.1728 - val_mse: 1.1728\n",
      "Epoch 31/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1792 - mse: 1.1792 - val_loss: 1.1719 - val_mse: 1.1719\n",
      "Epoch 32/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1794 - mse: 1.1794 - val_loss: 1.1721 - val_mse: 1.1721\n",
      "Epoch 33/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1791 - mse: 1.1791 - val_loss: 1.1713 - val_mse: 1.1713\n",
      "Epoch 34/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1792 - mse: 1.1792 - val_loss: 1.1712 - val_mse: 1.1712\n",
      "Epoch 35/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1797 - mse: 1.1797 - val_loss: 1.1709 - val_mse: 1.1709\n",
      "Epoch 36/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1795 - mse: 1.1795 - val_loss: 1.1715 - val_mse: 1.1715\n",
      "Epoch 37/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1791 - mse: 1.1791 - val_loss: 1.1705 - val_mse: 1.1705\n",
      "Epoch 38/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1792 - mse: 1.1792 - val_loss: 1.1732 - val_mse: 1.1732\n",
      "Epoch 39/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1791 - mse: 1.1791 - val_loss: 1.1730 - val_mse: 1.1730\n",
      "Epoch 40/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1791 - mse: 1.1791 - val_loss: 1.1719 - val_mse: 1.1719\n",
      "Epoch 41/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1793 - mse: 1.1793 - val_loss: 1.1728 - val_mse: 1.1728\n",
      "Epoch 42/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1792 - mse: 1.1792 - val_loss: 1.1726 - val_mse: 1.1726\n",
      "Epoch 43/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1790 - mse: 1.1790 - val_loss: 1.1723 - val_mse: 1.1723\n",
      "Epoch 44/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1794 - mse: 1.1794 - val_loss: 1.1723 - val_mse: 1.1723\n",
      "Epoch 45/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1793 - mse: 1.1793 - val_loss: 1.1726 - val_mse: 1.1726\n",
      "Epoch 46/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1796 - mse: 1.1796 - val_loss: 1.1729 - val_mse: 1.1729\n",
      "Epoch 47/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1793 - mse: 1.1793 - val_loss: 1.1730 - val_mse: 1.1730\n",
      "Epoch 48/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1797 - mse: 1.1797 - val_loss: 1.1730 - val_mse: 1.1730\n",
      "Epoch 49/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1792 - mse: 1.1792 - val_loss: 1.1727 - val_mse: 1.1727\n",
      "Epoch 50/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1795 - mse: 1.1795 - val_loss: 1.1728 - val_mse: 1.1728\n",
      "Epoch 51/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1796 - mse: 1.1796 - val_loss: 1.1724 - val_mse: 1.1724\n",
      "Epoch 52/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1793 - mse: 1.1793 - val_loss: 1.1720 - val_mse: 1.1720\n",
      "Epoch 53/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1791 - mse: 1.1791 - val_loss: 1.1728 - val_mse: 1.1728\n",
      "Epoch 54/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1795 - mse: 1.1795 - val_loss: 1.1725 - val_mse: 1.1725\n",
      "Epoch 55/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1794 - mse: 1.1794 - val_loss: 1.1725 - val_mse: 1.1725\n",
      "Epoch 56/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1795 - mse: 1.1795 - val_loss: 1.1730 - val_mse: 1.1730\n",
      "Epoch 57/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1792 - mse: 1.1792 - val_loss: 1.1719 - val_mse: 1.1719\n",
      "Epoch 58/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1797 - mse: 1.1797 - val_loss: 1.1729 - val_mse: 1.1729\n",
      "Epoch 59/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1791 - mse: 1.1791 - val_loss: 1.1736 - val_mse: 1.1736\n",
      "Epoch 60/60\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1791 - mse: 1.1791 - val_loss: 1.1715 - val_mse: 1.1715\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1715 - mse: 1.1715\n",
      "\n",
      "Weight: [array([[1.9951934]], dtype=float32), array([1.0327168], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 2/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 3/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 4/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 5/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 6/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 7/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 8/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 9/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 10/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 11/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 12/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 13/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 14/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 15/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 16/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 17/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 18/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 19/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 20/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 21/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 22/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 23/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 24/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 25/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 26/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 27/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 28/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 29/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 30/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 31/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 32/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 33/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 34/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 35/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 36/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 37/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 38/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 39/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 40/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 41/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 42/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 43/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 44/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 45/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 46/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 47/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 48/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 49/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 50/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 51/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 52/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 53/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 54/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 55/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 56/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 57/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 58/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 59/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 60/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0063 - mse: 0.0063\n",
      "\n",
      "Weight: [array([[1.992053]], dtype=float32), array([1.0008606], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0926 - val_mse: 0.0926\n",
      "Epoch 2/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0920 - val_mse: 0.0920\n",
      "Epoch 3/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0917 - val_mse: 0.0917\n",
      "Epoch 4/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0917 - val_mse: 0.0917\n",
      "Epoch 5/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0926 - val_mse: 0.0926\n",
      "Epoch 6/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0933 - val_mse: 0.0933\n",
      "Epoch 7/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0931 - val_mse: 0.0931\n",
      "Epoch 8/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0921 - val_mse: 0.0921\n",
      "Epoch 9/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0920 - val_mse: 0.0920\n",
      "Epoch 10/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0918 - val_mse: 0.0918\n",
      "Epoch 11/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0918 - val_mse: 0.0918\n",
      "Epoch 12/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0920 - val_mse: 0.0920\n",
      "Epoch 13/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0924 - val_mse: 0.0924\n",
      "Epoch 14/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0921 - val_mse: 0.0921\n",
      "Epoch 15/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0922 - val_mse: 0.0922\n",
      "Epoch 16/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0919 - val_mse: 0.0919\n",
      "Epoch 17/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0922 - val_mse: 0.0922\n",
      "Epoch 18/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0929 - val_mse: 0.0929\n",
      "Epoch 19/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0927 - val_mse: 0.0927\n",
      "Epoch 20/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0919 - val_mse: 0.0919\n",
      "Epoch 21/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0916 - val_mse: 0.0916\n",
      "Epoch 22/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0917 - val_mse: 0.0917\n",
      "Epoch 23/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0922 - val_mse: 0.0922\n",
      "Epoch 24/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0922 - val_mse: 0.0922\n",
      "Epoch 25/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0923 - val_mse: 0.0923\n",
      "Epoch 26/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0926 - val_mse: 0.0926\n",
      "Epoch 27/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0928 - val_mse: 0.0928\n",
      "Epoch 28/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0926 - val_mse: 0.0926\n",
      "Epoch 29/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0923 - val_mse: 0.0923\n",
      "Epoch 30/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0925 - val_mse: 0.0925\n",
      "Epoch 31/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0921 - val_mse: 0.0921\n",
      "Epoch 32/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0925 - val_mse: 0.0925\n",
      "Epoch 33/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0924 - val_mse: 0.0924\n",
      "Epoch 34/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0920 - val_mse: 0.0920\n",
      "Epoch 35/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0924 - val_mse: 0.0924\n",
      "Epoch 36/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0925 - val_mse: 0.0925\n",
      "Epoch 37/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0925 - val_mse: 0.0925\n",
      "Epoch 38/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0930 - val_mse: 0.0930\n",
      "Epoch 39/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0931 - val_mse: 0.0931\n",
      "Epoch 40/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0922 - val_mse: 0.0922\n",
      "Epoch 41/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0923 - val_mse: 0.0923\n",
      "Epoch 42/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0920 - val_mse: 0.0920\n",
      "Epoch 43/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0920 - val_mse: 0.0920\n",
      "Epoch 44/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0922 - val_mse: 0.0922\n",
      "Epoch 45/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0922 - val_mse: 0.0922\n",
      "Epoch 46/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0922 - val_mse: 0.0922\n",
      "Epoch 47/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0923 - val_mse: 0.0923\n",
      "Epoch 48/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0929 - val_mse: 0.0929\n",
      "Epoch 49/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0927 - val_mse: 0.0927\n",
      "Epoch 50/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0928 - val_mse: 0.0928\n",
      "Epoch 51/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0926 - val_mse: 0.0926\n",
      "Epoch 52/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0924 - val_mse: 0.0924\n",
      "Epoch 53/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0922 - val_mse: 0.0922\n",
      "Epoch 54/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0926 - val_mse: 0.0926\n",
      "Epoch 55/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0924 - val_mse: 0.0924\n",
      "Epoch 56/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0922 - val_mse: 0.0922\n",
      "Epoch 57/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0923 - val_mse: 0.0923\n",
      "Epoch 58/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0927 - val_mse: 0.0927\n",
      "Epoch 59/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0928 - val_mse: 0.0928\n",
      "Epoch 60/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.0922 - val_mse: 0.0922\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0922 - mse: 0.0922\n",
      "\n",
      "Weight: [array([[1.9921317]], dtype=float32), array([0.99730384], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/60\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.3048 - val_mse: 0.3048\n",
      "Epoch 2/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2690 - mse: 0.2690 - val_loss: 0.3038 - val_mse: 0.3038\n",
      "Epoch 3/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2687 - mse: 0.2687 - val_loss: 0.3028 - val_mse: 0.3028\n",
      "Epoch 4/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2686 - mse: 0.2686 - val_loss: 0.3031 - val_mse: 0.3031\n",
      "Epoch 5/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2686 - mse: 0.2686 - val_loss: 0.3029 - val_mse: 0.3029\n",
      "Epoch 6/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.3027 - val_mse: 0.3027\n",
      "Epoch 7/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3027 - val_mse: 0.3027\n",
      "Epoch 8/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3029 - val_mse: 0.3029\n",
      "Epoch 9/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.3037 - val_mse: 0.3037\n",
      "Epoch 10/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2686 - mse: 0.2686 - val_loss: 0.3029 - val_mse: 0.3029\n",
      "Epoch 11/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3026 - val_mse: 0.3026\n",
      "Epoch 12/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.3027 - val_mse: 0.3027\n",
      "Epoch 13/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.3030 - val_mse: 0.3030\n",
      "Epoch 14/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3023 - val_mse: 0.3023\n",
      "Epoch 15/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3026 - val_mse: 0.3026\n",
      "Epoch 16/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3028 - val_mse: 0.3028\n",
      "Epoch 17/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3018 - val_mse: 0.3018\n",
      "Epoch 18/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.3021 - val_mse: 0.3021\n",
      "Epoch 19/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3022 - val_mse: 0.3022\n",
      "Epoch 20/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3028 - val_mse: 0.3028\n",
      "Epoch 21/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3027 - val_mse: 0.3027\n",
      "Epoch 22/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3024 - val_mse: 0.3024\n",
      "Epoch 23/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3028 - val_mse: 0.3028\n",
      "Epoch 24/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3027 - val_mse: 0.3027\n",
      "Epoch 25/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3023 - val_mse: 0.3023\n",
      "Epoch 26/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3019 - val_mse: 0.3019\n",
      "Epoch 27/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3019 - val_mse: 0.3019\n",
      "Epoch 28/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3019 - val_mse: 0.3019\n",
      "Epoch 29/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3019 - val_mse: 0.3019\n",
      "Epoch 30/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3022 - val_mse: 0.3022\n",
      "Epoch 31/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2683 - mse: 0.2683 - val_loss: 0.3021 - val_mse: 0.3021\n",
      "Epoch 32/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3022 - val_mse: 0.3022\n",
      "Epoch 33/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3022 - val_mse: 0.3022\n",
      "Epoch 34/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3015 - val_mse: 0.3015\n",
      "Epoch 35/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3014 - val_mse: 0.3014\n",
      "Epoch 36/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3017 - val_mse: 0.3017\n",
      "Epoch 37/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.3015 - val_mse: 0.3015\n",
      "Epoch 38/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3022 - val_mse: 0.3022\n",
      "Epoch 39/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2683 - mse: 0.2683 - val_loss: 0.3025 - val_mse: 0.3025\n",
      "Epoch 40/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.3023 - val_mse: 0.3023\n",
      "Epoch 41/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.3021 - val_mse: 0.3021\n",
      "Epoch 42/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.3014 - val_mse: 0.3014\n",
      "Epoch 43/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3009 - val_mse: 0.3009\n",
      "Epoch 44/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2687 - mse: 0.2687 - val_loss: 0.3010 - val_mse: 0.3010\n",
      "Epoch 45/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2686 - mse: 0.2686 - val_loss: 0.3011 - val_mse: 0.3011\n",
      "Epoch 46/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.3012 - val_mse: 0.3012\n",
      "Epoch 47/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.3008 - val_mse: 0.3008\n",
      "Epoch 48/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.3011 - val_mse: 0.3011\n",
      "Epoch 49/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.3012 - val_mse: 0.3012\n",
      "Epoch 50/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.3015 - val_mse: 0.3015\n",
      "Epoch 51/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3018 - val_mse: 0.3018\n",
      "Epoch 52/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2683 - mse: 0.2683 - val_loss: 0.3024 - val_mse: 0.3024\n",
      "Epoch 53/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3028 - val_mse: 0.3028\n",
      "Epoch 54/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3027 - val_mse: 0.3027\n",
      "Epoch 55/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.3017 - val_mse: 0.3017\n",
      "Epoch 56/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3021 - val_mse: 0.3021\n",
      "Epoch 57/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3022 - val_mse: 0.3022\n",
      "Epoch 58/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3026 - val_mse: 0.3026\n",
      "Epoch 59/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.3018 - val_mse: 0.3018\n",
      "Epoch 60/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2684 - val_loss: 0.3021 - val_mse: 0.3021\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.3021 - mse: 0.3021\n",
      "\n",
      "Weight: [array([[2.0480506]], dtype=float32), array([0.9869478], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/60\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.5091 - mse: 0.5091 - val_loss: 0.4562 - val_mse: 0.4562\n",
      "Epoch 2/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5088 - mse: 0.5088 - val_loss: 0.4539 - val_mse: 0.4539\n",
      "Epoch 3/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5084 - mse: 0.5084 - val_loss: 0.4522 - val_mse: 0.4522\n",
      "Epoch 4/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5082 - mse: 0.5082 - val_loss: 0.4510 - val_mse: 0.4510\n",
      "Epoch 5/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5081 - mse: 0.5081 - val_loss: 0.4502 - val_mse: 0.4502\n",
      "Epoch 6/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5081 - mse: 0.5081 - val_loss: 0.4499 - val_mse: 0.4499\n",
      "Epoch 7/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4501 - val_mse: 0.4501\n",
      "Epoch 8/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4491 - val_mse: 0.4491\n",
      "Epoch 9/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4492 - val_mse: 0.4492\n",
      "Epoch 10/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5078 - mse: 0.5078 - val_loss: 0.4503 - val_mse: 0.4503\n",
      "Epoch 11/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5081 - mse: 0.5081 - val_loss: 0.4504 - val_mse: 0.4504\n",
      "Epoch 12/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5082 - mse: 0.5082 - val_loss: 0.4494 - val_mse: 0.4494\n",
      "Epoch 13/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4504 - val_mse: 0.4504\n",
      "Epoch 14/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4495 - val_mse: 0.4495\n",
      "Epoch 15/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5082 - mse: 0.5082 - val_loss: 0.4489 - val_mse: 0.4489\n",
      "Epoch 16/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4488 - val_mse: 0.4488\n",
      "Epoch 17/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5078 - mse: 0.5078 - val_loss: 0.4481 - val_mse: 0.4481\n",
      "Epoch 18/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4475 - val_mse: 0.4475\n",
      "Epoch 19/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4483 - val_mse: 0.4483\n",
      "Epoch 20/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4481 - val_mse: 0.4481\n",
      "Epoch 21/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4475 - val_mse: 0.4475\n",
      "Epoch 22/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4484 - val_mse: 0.4484\n",
      "Epoch 23/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4473 - val_mse: 0.4473\n",
      "Epoch 24/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5083 - mse: 0.5083 - val_loss: 0.4471 - val_mse: 0.4471\n",
      "Epoch 25/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4469 - val_mse: 0.4469\n",
      "Epoch 26/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5081 - mse: 0.5081 - val_loss: 0.4471 - val_mse: 0.4471\n",
      "Epoch 27/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5083 - mse: 0.5083 - val_loss: 0.4474 - val_mse: 0.4474\n",
      "Epoch 28/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4475 - val_mse: 0.4475\n",
      "Epoch 29/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5082 - mse: 0.5082 - val_loss: 0.4472 - val_mse: 0.4472\n",
      "Epoch 30/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4480 - val_mse: 0.4480\n",
      "Epoch 31/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4487 - val_mse: 0.4487\n",
      "Epoch 32/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4490 - val_mse: 0.4490\n",
      "Epoch 33/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5078 - mse: 0.5078 - val_loss: 0.4491 - val_mse: 0.4491\n",
      "Epoch 34/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5081 - mse: 0.5081 - val_loss: 0.4488 - val_mse: 0.4488\n",
      "Epoch 35/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4482 - val_mse: 0.4482\n",
      "Epoch 36/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4485 - val_mse: 0.4485\n",
      "Epoch 37/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4493 - val_mse: 0.4493\n",
      "Epoch 38/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4495 - val_mse: 0.4495\n",
      "Epoch 39/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5078 - mse: 0.5078 - val_loss: 0.4489 - val_mse: 0.4489\n",
      "Epoch 40/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4490 - val_mse: 0.4490\n",
      "Epoch 41/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5081 - mse: 0.5081 - val_loss: 0.4492 - val_mse: 0.4492\n",
      "Epoch 42/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5082 - mse: 0.5082 - val_loss: 0.4476 - val_mse: 0.4476\n",
      "Epoch 43/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5084 - mse: 0.5084 - val_loss: 0.4477 - val_mse: 0.4477\n",
      "Epoch 44/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4482 - val_mse: 0.4482\n",
      "Epoch 45/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4481 - val_mse: 0.4481\n",
      "Epoch 46/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4488 - val_mse: 0.4488\n",
      "Epoch 47/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4492 - val_mse: 0.4492\n",
      "Epoch 48/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4491 - val_mse: 0.4491\n",
      "Epoch 49/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5081 - mse: 0.5081 - val_loss: 0.4493 - val_mse: 0.4493\n",
      "Epoch 50/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4489 - val_mse: 0.4489\n",
      "Epoch 51/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4495 - val_mse: 0.4495\n",
      "Epoch 52/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5078 - mse: 0.5078 - val_loss: 0.4495 - val_mse: 0.4495\n",
      "Epoch 53/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4502 - val_mse: 0.4502\n",
      "Epoch 54/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4501 - val_mse: 0.4501\n",
      "Epoch 55/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4502 - val_mse: 0.4502\n",
      "Epoch 56/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5081 - mse: 0.5081 - val_loss: 0.4499 - val_mse: 0.4499\n",
      "Epoch 57/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5079 - mse: 0.5079 - val_loss: 0.4495 - val_mse: 0.4495\n",
      "Epoch 58/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5081 - mse: 0.5081 - val_loss: 0.4497 - val_mse: 0.4497\n",
      "Epoch 59/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5081 - mse: 0.5081 - val_loss: 0.4503 - val_mse: 0.4503\n",
      "Epoch 60/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5080 - mse: 0.5080 - val_loss: 0.4511 - val_mse: 0.4511\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4511 - mse: 0.4511\n",
      "\n",
      "Weight: [array([[2.0001633]], dtype=float32), array([0.9737332], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/60\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.8472 - mse: 0.8472 - val_loss: 0.8548 - val_mse: 0.8548\n",
      "Epoch 2/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8456 - mse: 0.8456 - val_loss: 0.8553 - val_mse: 0.8553\n",
      "Epoch 3/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8442 - mse: 0.8442 - val_loss: 0.8572 - val_mse: 0.8572\n",
      "Epoch 4/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8436 - mse: 0.8436 - val_loss: 0.8589 - val_mse: 0.8589\n",
      "Epoch 5/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8438 - mse: 0.8438 - val_loss: 0.8591 - val_mse: 0.8591\n",
      "Epoch 6/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8438 - mse: 0.8438 - val_loss: 0.8592 - val_mse: 0.8592\n",
      "Epoch 7/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8434 - mse: 0.8434 - val_loss: 0.8594 - val_mse: 0.8594\n",
      "Epoch 8/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8434 - mse: 0.8434 - val_loss: 0.8609 - val_mse: 0.8609\n",
      "Epoch 9/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8436 - mse: 0.8436 - val_loss: 0.8619 - val_mse: 0.8619\n",
      "Epoch 10/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8439 - mse: 0.8439 - val_loss: 0.8618 - val_mse: 0.8618\n",
      "Epoch 11/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8433 - mse: 0.8433 - val_loss: 0.8607 - val_mse: 0.8607\n",
      "Epoch 12/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8433 - mse: 0.8433 - val_loss: 0.8612 - val_mse: 0.8612\n",
      "Epoch 13/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8437 - mse: 0.8437 - val_loss: 0.8635 - val_mse: 0.8635\n",
      "Epoch 14/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8433 - mse: 0.8433 - val_loss: 0.8600 - val_mse: 0.8600\n",
      "Epoch 15/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8433 - mse: 0.8433 - val_loss: 0.8591 - val_mse: 0.8591\n",
      "Epoch 16/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8435 - mse: 0.8435 - val_loss: 0.8609 - val_mse: 0.8609\n",
      "Epoch 17/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8434 - mse: 0.8434 - val_loss: 0.8591 - val_mse: 0.8591\n",
      "Epoch 18/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8435 - mse: 0.8435 - val_loss: 0.8616 - val_mse: 0.8616\n",
      "Epoch 19/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8434 - mse: 0.8434 - val_loss: 0.8629 - val_mse: 0.8629\n",
      "Epoch 20/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8434 - mse: 0.8434 - val_loss: 0.8621 - val_mse: 0.8621\n",
      "Epoch 21/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8435 - mse: 0.8435 - val_loss: 0.8621 - val_mse: 0.8621\n",
      "Epoch 22/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8434 - mse: 0.8434 - val_loss: 0.8638 - val_mse: 0.8638\n",
      "Epoch 23/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8436 - mse: 0.8436 - val_loss: 0.8649 - val_mse: 0.8649\n",
      "Epoch 24/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8433 - mse: 0.8433 - val_loss: 0.8632 - val_mse: 0.8632\n",
      "Epoch 25/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8435 - mse: 0.8435 - val_loss: 0.8627 - val_mse: 0.8627\n",
      "Epoch 26/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8434 - mse: 0.8434 - val_loss: 0.8658 - val_mse: 0.8658\n",
      "Epoch 27/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8433 - mse: 0.8433 - val_loss: 0.8658 - val_mse: 0.8658\n",
      "Epoch 28/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8433 - mse: 0.8433 - val_loss: 0.8650 - val_mse: 0.8650\n",
      "Epoch 29/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8435 - mse: 0.8435 - val_loss: 0.8663 - val_mse: 0.8663\n",
      "Epoch 30/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8435 - mse: 0.8435 - val_loss: 0.8651 - val_mse: 0.8651\n",
      "Epoch 31/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8434 - mse: 0.8434 - val_loss: 0.8677 - val_mse: 0.8677\n",
      "Epoch 32/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8433 - mse: 0.8433 - val_loss: 0.8663 - val_mse: 0.8663\n",
      "Epoch 33/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8434 - mse: 0.8434 - val_loss: 0.8665 - val_mse: 0.8665\n",
      "Epoch 34/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8432 - mse: 0.8432 - val_loss: 0.8638 - val_mse: 0.8638\n",
      "Epoch 35/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8438 - mse: 0.8438 - val_loss: 0.8628 - val_mse: 0.8628\n",
      "Epoch 36/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8435 - mse: 0.8435 - val_loss: 0.8604 - val_mse: 0.8604\n",
      "Epoch 37/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8436 - mse: 0.8436 - val_loss: 0.8599 - val_mse: 0.8599\n",
      "Epoch 38/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8436 - mse: 0.8436 - val_loss: 0.8601 - val_mse: 0.8601\n",
      "Epoch 39/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8432 - mse: 0.8432 - val_loss: 0.8619 - val_mse: 0.8619\n",
      "Epoch 40/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8434 - mse: 0.8434 - val_loss: 0.8618 - val_mse: 0.8618\n",
      "Epoch 41/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8435 - mse: 0.8435 - val_loss: 0.8597 - val_mse: 0.8597\n",
      "Epoch 42/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8434 - mse: 0.8434 - val_loss: 0.8620 - val_mse: 0.8620\n",
      "Epoch 43/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8436 - mse: 0.8436 - val_loss: 0.8622 - val_mse: 0.8622\n",
      "Epoch 44/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8435 - mse: 0.8435 - val_loss: 0.8641 - val_mse: 0.8641\n",
      "Epoch 45/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8434 - mse: 0.8434 - val_loss: 0.8655 - val_mse: 0.8655\n",
      "Epoch 46/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8435 - mse: 0.8435 - val_loss: 0.8632 - val_mse: 0.8632\n",
      "Epoch 47/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8434 - mse: 0.8434 - val_loss: 0.8629 - val_mse: 0.8629\n",
      "Epoch 48/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8434 - mse: 0.8434 - val_loss: 0.8629 - val_mse: 0.8629\n",
      "Epoch 49/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8435 - mse: 0.8435 - val_loss: 0.8640 - val_mse: 0.8640\n",
      "Epoch 50/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8436 - mse: 0.8436 - val_loss: 0.8643 - val_mse: 0.8643\n",
      "Epoch 51/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8435 - mse: 0.8435 - val_loss: 0.8633 - val_mse: 0.8633\n",
      "Epoch 52/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8433 - mse: 0.8433 - val_loss: 0.8621 - val_mse: 0.8621\n",
      "Epoch 53/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8432 - mse: 0.8432 - val_loss: 0.8628 - val_mse: 0.8628\n",
      "Epoch 54/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8434 - mse: 0.8434 - val_loss: 0.8612 - val_mse: 0.8612\n",
      "Epoch 55/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8435 - mse: 0.8435 - val_loss: 0.8618 - val_mse: 0.8618\n",
      "Epoch 56/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8433 - mse: 0.8433 - val_loss: 0.8624 - val_mse: 0.8624\n",
      "Epoch 57/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8433 - mse: 0.8433 - val_loss: 0.8631 - val_mse: 0.8631\n",
      "Epoch 58/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.8434 - mse: 0.8434 - val_loss: 0.8629 - val_mse: 0.8629\n",
      "Epoch 59/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8435 - mse: 0.8435 - val_loss: 0.8637 - val_mse: 0.8637\n",
      "Epoch 60/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8435 - mse: 0.8435 - val_loss: 0.8625 - val_mse: 0.8625\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.8625 - mse: 0.8625\n",
      "\n",
      "Weight: [array([[1.9680122]], dtype=float32), array([1.0479301], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  900.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3668 - mse: 1.3668 - val_loss: 0.8674 - val_mse: 0.8674\n",
      "Epoch 2/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.3657 - mse: 1.3657 - val_loss: 0.8699 - val_mse: 0.8699\n",
      "Epoch 3/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.3626 - mse: 1.3626 - val_loss: 0.8709 - val_mse: 0.8709\n",
      "Epoch 4/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.3614 - mse: 1.3614 - val_loss: 0.8680 - val_mse: 0.8680\n",
      "Epoch 5/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3611 - mse: 1.3611 - val_loss: 0.8712 - val_mse: 0.8712\n",
      "Epoch 6/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3608 - mse: 1.3608 - val_loss: 0.8750 - val_mse: 0.8750\n",
      "Epoch 7/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3610 - mse: 1.3610 - val_loss: 0.8701 - val_mse: 0.8701\n",
      "Epoch 8/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3608 - mse: 1.3608 - val_loss: 0.8706 - val_mse: 0.8706\n",
      "Epoch 9/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3613 - mse: 1.3613 - val_loss: 0.8710 - val_mse: 0.8710\n",
      "Epoch 10/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3610 - mse: 1.3610 - val_loss: 0.8678 - val_mse: 0.8678\n",
      "Epoch 11/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3610 - mse: 1.3610 - val_loss: 0.8710 - val_mse: 0.8710\n",
      "Epoch 12/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.3615 - mse: 1.3615 - val_loss: 0.8743 - val_mse: 0.8743\n",
      "Epoch 13/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3611 - mse: 1.3611 - val_loss: 0.8726 - val_mse: 0.8726\n",
      "Epoch 14/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.3610 - mse: 1.3610 - val_loss: 0.8705 - val_mse: 0.8705\n",
      "Epoch 15/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3612 - mse: 1.3612 - val_loss: 0.8733 - val_mse: 0.8733\n",
      "Epoch 16/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3616 - mse: 1.3616 - val_loss: 0.8696 - val_mse: 0.8696\n",
      "Epoch 17/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3610 - mse: 1.3610 - val_loss: 0.8702 - val_mse: 0.8702\n",
      "Epoch 18/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.3615 - mse: 1.3615 - val_loss: 0.8711 - val_mse: 0.8711\n",
      "Epoch 19/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3611 - mse: 1.3611 - val_loss: 0.8743 - val_mse: 0.8743\n",
      "Epoch 20/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3613 - mse: 1.3613 - val_loss: 0.8702 - val_mse: 0.8702\n",
      "Epoch 21/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3611 - mse: 1.3611 - val_loss: 0.8736 - val_mse: 0.8736\n",
      "Epoch 22/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3612 - mse: 1.3612 - val_loss: 0.8729 - val_mse: 0.8729\n",
      "Epoch 23/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.3609 - mse: 1.3609 - val_loss: 0.8727 - val_mse: 0.8727\n",
      "Epoch 24/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3610 - mse: 1.3610 - val_loss: 0.8760 - val_mse: 0.8760\n",
      "Epoch 25/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3614 - mse: 1.3614 - val_loss: 0.8728 - val_mse: 0.8728\n",
      "Epoch 26/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3612 - mse: 1.3612 - val_loss: 0.8695 - val_mse: 0.8695\n",
      "Epoch 27/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3611 - mse: 1.3611 - val_loss: 0.8686 - val_mse: 0.8686\n",
      "Epoch 28/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3611 - mse: 1.3611 - val_loss: 0.8690 - val_mse: 0.8690\n",
      "Epoch 29/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3609 - mse: 1.3609 - val_loss: 0.8719 - val_mse: 0.8719\n",
      "Epoch 30/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3613 - mse: 1.3613 - val_loss: 0.8719 - val_mse: 0.8719\n",
      "Epoch 31/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3607 - mse: 1.3607 - val_loss: 0.8732 - val_mse: 0.8732\n",
      "Epoch 32/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3612 - mse: 1.3612 - val_loss: 0.8705 - val_mse: 0.8705\n",
      "Epoch 33/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3610 - mse: 1.3610 - val_loss: 0.8723 - val_mse: 0.8723\n",
      "Epoch 34/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3611 - mse: 1.3611 - val_loss: 0.8707 - val_mse: 0.8707\n",
      "Epoch 35/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3610 - mse: 1.3610 - val_loss: 0.8717 - val_mse: 0.8717\n",
      "Epoch 36/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3613 - mse: 1.3613 - val_loss: 0.8753 - val_mse: 0.8753\n",
      "Epoch 37/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3608 - mse: 1.3608 - val_loss: 0.8740 - val_mse: 0.8740\n",
      "Epoch 38/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.3610 - mse: 1.3610 - val_loss: 0.8751 - val_mse: 0.8751\n",
      "Epoch 39/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3611 - mse: 1.3611 - val_loss: 0.8737 - val_mse: 0.8737\n",
      "Epoch 40/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3612 - mse: 1.3612 - val_loss: 0.8719 - val_mse: 0.8719\n",
      "Epoch 41/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3612 - mse: 1.3612 - val_loss: 0.8753 - val_mse: 0.8753\n",
      "Epoch 42/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.3611 - mse: 1.3611 - val_loss: 0.8752 - val_mse: 0.8752\n",
      "Epoch 43/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3609 - mse: 1.3609 - val_loss: 0.8770 - val_mse: 0.8770\n",
      "Epoch 44/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3609 - mse: 1.3609 - val_loss: 0.8716 - val_mse: 0.8716\n",
      "Epoch 45/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3618 - mse: 1.3618 - val_loss: 0.8727 - val_mse: 0.8727\n",
      "Epoch 46/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.3610 - mse: 1.3610 - val_loss: 0.8760 - val_mse: 0.8760\n",
      "Epoch 47/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3612 - mse: 1.3612 - val_loss: 0.8753 - val_mse: 0.8753\n",
      "Epoch 48/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3610 - mse: 1.3610 - val_loss: 0.8745 - val_mse: 0.8745\n",
      "Epoch 49/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3610 - mse: 1.3610 - val_loss: 0.8735 - val_mse: 0.8735\n",
      "Epoch 50/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3614 - mse: 1.3614 - val_loss: 0.8738 - val_mse: 0.8738\n",
      "Epoch 51/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3613 - mse: 1.3613 - val_loss: 0.8713 - val_mse: 0.8713\n",
      "Epoch 52/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3614 - mse: 1.3614 - val_loss: 0.8694 - val_mse: 0.8694\n",
      "Epoch 53/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3621 - mse: 1.3621 - val_loss: 0.8715 - val_mse: 0.8715\n",
      "Epoch 54/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.3612 - mse: 1.3612 - val_loss: 0.8707 - val_mse: 0.8707\n",
      "Epoch 55/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3611 - mse: 1.3611 - val_loss: 0.8735 - val_mse: 0.8735\n",
      "Epoch 56/60\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.3612 - mse: 1.3612 - val_loss: 0.8755 - val_mse: 0.8755\n",
      "Epoch 57/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3611 - mse: 1.3611 - val_loss: 0.8768 - val_mse: 0.8768\n",
      "Epoch 58/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3610 - mse: 1.3610 - val_loss: 0.8776 - val_mse: 0.8776\n",
      "Epoch 59/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3617 - mse: 1.3617 - val_loss: 0.8770 - val_mse: 0.8770\n",
      "Epoch 60/60\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.3612 - mse: 1.3612 - val_loss: 0.8801 - val_mse: 0.8801\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.8801 - mse: 0.8801\n",
      "\n",
      "Weight: [array([[1.9971709]], dtype=float32), array([0.9309615], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.1\n",
      "Epoch 1/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 2/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 3/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 4/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 5/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 6/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 7/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 8/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 9/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 10/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 11/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 12/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 13/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 14/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 15/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 16/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 17/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 18/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 19/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 20/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 21/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 22/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 23/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 24/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 25/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 26/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 27/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 28/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 29/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 30/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 31/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 32/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 33/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 34/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 35/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 36/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 37/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 38/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 39/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 40/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 41/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 42/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 43/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 44/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 45/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 46/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 47/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 48/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 49/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 50/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 51/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 52/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 53/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 54/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 55/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 56/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 57/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 58/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 59/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 60/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0074 - mse: 0.0074\n",
      "\n",
      "Weight: [array([[2.0035822]], dtype=float32), array([1.0043781], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.30000000000000004\n",
      "Epoch 1/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0978 - val_mse: 0.0978\n",
      "Epoch 2/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 3/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 4/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 5/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 6/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 7/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 8/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0978 - val_mse: 0.0978\n",
      "Epoch 9/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 10/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 11/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 12/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 13/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 14/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 15/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 16/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 17/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0975 - val_mse: 0.0975\n",
      "Epoch 18/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0975 - val_mse: 0.0975\n",
      "Epoch 19/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0975 - val_mse: 0.0975\n",
      "Epoch 20/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 21/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 22/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 23/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 24/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 25/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 26/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 27/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 28/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 29/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 30/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0978 - val_mse: 0.0978\n",
      "Epoch 31/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0979 - val_mse: 0.0979\n",
      "Epoch 32/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 33/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 34/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 35/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 36/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0978 - val_mse: 0.0978\n",
      "Epoch 37/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 38/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 39/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 40/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 41/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 42/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 43/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0975 - val_mse: 0.0975\n",
      "Epoch 44/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 45/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 46/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 47/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 48/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 49/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 50/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 51/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0978 - val_mse: 0.0978\n",
      "Epoch 52/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 53/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 54/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 55/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 56/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 57/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 58/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0975 - val_mse: 0.0975\n",
      "Epoch 59/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 60/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0976 - mse: 0.0976\n",
      "\n",
      "Weight: [array([[2.0095167]], dtype=float32), array([1.0045277], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.5\n",
      "Epoch 1/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2402 - mse: 0.2402 - val_loss: 0.2306 - val_mse: 0.2306\n",
      "Epoch 2/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2399 - mse: 0.2399 - val_loss: 0.2285 - val_mse: 0.2285\n",
      "Epoch 3/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2243 - val_mse: 0.2243\n",
      "Epoch 4/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2243 - val_mse: 0.2243\n",
      "Epoch 5/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2236 - val_mse: 0.2236\n",
      "Epoch 6/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2232 - val_mse: 0.2232\n",
      "Epoch 7/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2229 - val_mse: 0.2229\n",
      "Epoch 8/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2216 - val_mse: 0.2216\n",
      "Epoch 9/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2223 - val_mse: 0.2223\n",
      "Epoch 10/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2246 - val_mse: 0.2246\n",
      "Epoch 11/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2243 - val_mse: 0.2243\n",
      "Epoch 12/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2251 - val_mse: 0.2251\n",
      "Epoch 13/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2236 - val_mse: 0.2236\n",
      "Epoch 14/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2245 - val_mse: 0.2245\n",
      "Epoch 15/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2246 - val_mse: 0.2246\n",
      "Epoch 16/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2233 - val_mse: 0.2233\n",
      "Epoch 17/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2243 - val_mse: 0.2243\n",
      "Epoch 18/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2236 - val_mse: 0.2236\n",
      "Epoch 19/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2242 - val_mse: 0.2242\n",
      "Epoch 20/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2233 - val_mse: 0.2233\n",
      "Epoch 21/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2225 - val_mse: 0.2225\n",
      "Epoch 22/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2226 - val_mse: 0.2226\n",
      "Epoch 23/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2231 - val_mse: 0.2231\n",
      "Epoch 24/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2223 - val_mse: 0.2223\n",
      "Epoch 25/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2227 - val_mse: 0.2227\n",
      "Epoch 26/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2233 - val_mse: 0.2233\n",
      "Epoch 27/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2227 - val_mse: 0.2227\n",
      "Epoch 28/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2224 - val_mse: 0.2224\n",
      "Epoch 29/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2225 - val_mse: 0.2225\n",
      "Epoch 30/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2224 - val_mse: 0.2224\n",
      "Epoch 31/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2234 - val_mse: 0.2234\n",
      "Epoch 32/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2227 - val_mse: 0.2227\n",
      "Epoch 33/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2233 - val_mse: 0.2233\n",
      "Epoch 34/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2230 - val_mse: 0.2230\n",
      "Epoch 35/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2243 - val_mse: 0.2243\n",
      "Epoch 36/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2240 - val_mse: 0.2240\n",
      "Epoch 37/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2226 - val_mse: 0.2226\n",
      "Epoch 38/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2237 - val_mse: 0.2237\n",
      "Epoch 39/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2226 - val_mse: 0.2226\n",
      "Epoch 40/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2231 - val_mse: 0.2231\n",
      "Epoch 41/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2226 - val_mse: 0.2226\n",
      "Epoch 42/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2242 - val_mse: 0.2242\n",
      "Epoch 43/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2232 - val_mse: 0.2232\n",
      "Epoch 44/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2231 - val_mse: 0.2231\n",
      "Epoch 45/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2242 - val_mse: 0.2242\n",
      "Epoch 46/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2241 - val_mse: 0.2241\n",
      "Epoch 47/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2243 - val_mse: 0.2243\n",
      "Epoch 48/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2263 - val_mse: 0.2263\n",
      "Epoch 49/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2236 - val_mse: 0.2236\n",
      "Epoch 50/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2226 - val_mse: 0.2226\n",
      "Epoch 51/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2223 - val_mse: 0.2223\n",
      "Epoch 52/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2229 - val_mse: 0.2229\n",
      "Epoch 53/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2237 - val_mse: 0.2237\n",
      "Epoch 54/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2232 - val_mse: 0.2232\n",
      "Epoch 55/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2224 - val_mse: 0.2224\n",
      "Epoch 56/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2241 - val_mse: 0.2241\n",
      "Epoch 57/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2224 - val_mse: 0.2224\n",
      "Epoch 58/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2223 - val_mse: 0.2223\n",
      "Epoch 59/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2226 - val_mse: 0.2226\n",
      "Epoch 60/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2220 - val_mse: 0.2220\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.2220 - mse: 0.2220\n",
      "\n",
      "Weight: [array([[1.9983943]], dtype=float32), array([0.9652687], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.7000000000000001\n",
      "Epoch 1/60\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4991 - mse: 0.4991 - val_loss: 0.4929 - val_mse: 0.4929\n",
      "Epoch 2/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4988 - mse: 0.4988 - val_loss: 0.4927 - val_mse: 0.4927\n",
      "Epoch 3/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4988 - mse: 0.4988 - val_loss: 0.4927 - val_mse: 0.4927\n",
      "Epoch 4/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4923 - val_mse: 0.4923\n",
      "Epoch 5/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4991 - mse: 0.4991 - val_loss: 0.4920 - val_mse: 0.4920\n",
      "Epoch 6/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4923 - val_mse: 0.4923\n",
      "Epoch 7/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4922 - val_mse: 0.4922\n",
      "Epoch 8/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4988 - mse: 0.4988 - val_loss: 0.4921 - val_mse: 0.4921\n",
      "Epoch 9/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4988 - mse: 0.4988 - val_loss: 0.4921 - val_mse: 0.4921\n",
      "Epoch 10/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4920 - val_mse: 0.4920\n",
      "Epoch 11/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4925 - val_mse: 0.4925\n",
      "Epoch 12/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4988 - mse: 0.4988 - val_loss: 0.4921 - val_mse: 0.4921\n",
      "Epoch 13/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4922 - val_mse: 0.4922\n",
      "Epoch 14/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4988 - mse: 0.4988 - val_loss: 0.4927 - val_mse: 0.4927\n",
      "Epoch 15/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4922 - val_mse: 0.4922\n",
      "Epoch 16/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4919 - val_mse: 0.4919\n",
      "Epoch 17/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4991 - mse: 0.4991 - val_loss: 0.4926 - val_mse: 0.4926\n",
      "Epoch 18/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4925 - val_mse: 0.4925\n",
      "Epoch 19/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4924 - val_mse: 0.4924\n",
      "Epoch 20/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4988 - mse: 0.4988 - val_loss: 0.4924 - val_mse: 0.4924\n",
      "Epoch 21/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4924 - val_mse: 0.4924\n",
      "Epoch 22/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4921 - val_mse: 0.4921\n",
      "Epoch 23/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4922 - val_mse: 0.4922\n",
      "Epoch 24/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4925 - val_mse: 0.4925\n",
      "Epoch 25/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4931 - val_mse: 0.4931\n",
      "Epoch 26/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4926 - val_mse: 0.4926\n",
      "Epoch 27/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4925 - val_mse: 0.4925\n",
      "Epoch 28/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4922 - val_mse: 0.4922\n",
      "Epoch 29/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4918 - val_mse: 0.4918\n",
      "Epoch 30/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4920 - val_mse: 0.4920\n",
      "Epoch 31/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4924 - val_mse: 0.4924\n",
      "Epoch 32/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4926 - val_mse: 0.4926\n",
      "Epoch 33/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4988 - mse: 0.4988 - val_loss: 0.4931 - val_mse: 0.4931\n",
      "Epoch 34/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4925 - val_mse: 0.4925\n",
      "Epoch 35/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4925 - val_mse: 0.4925\n",
      "Epoch 36/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4924 - val_mse: 0.4924\n",
      "Epoch 37/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4930 - val_mse: 0.4930\n",
      "Epoch 38/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4929 - val_mse: 0.4929\n",
      "Epoch 39/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4924 - val_mse: 0.4924\n",
      "Epoch 40/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4988 - mse: 0.4988 - val_loss: 0.4924 - val_mse: 0.4924\n",
      "Epoch 41/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4923 - val_mse: 0.4923\n",
      "Epoch 42/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4920 - val_mse: 0.4920\n",
      "Epoch 43/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4988 - mse: 0.4988 - val_loss: 0.4921 - val_mse: 0.4921\n",
      "Epoch 44/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4921 - val_mse: 0.4921\n",
      "Epoch 45/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4922 - val_mse: 0.4922\n",
      "Epoch 46/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4925 - val_mse: 0.4925\n",
      "Epoch 47/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4988 - mse: 0.4988 - val_loss: 0.4920 - val_mse: 0.4920\n",
      "Epoch 48/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4919 - val_mse: 0.4919\n",
      "Epoch 49/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4917 - val_mse: 0.4917\n",
      "Epoch 50/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4918 - val_mse: 0.4918\n",
      "Epoch 51/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4991 - mse: 0.4991 - val_loss: 0.4917 - val_mse: 0.4917\n",
      "Epoch 52/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4917 - val_mse: 0.4917\n",
      "Epoch 53/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4921 - val_mse: 0.4921\n",
      "Epoch 54/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4988 - mse: 0.4988 - val_loss: 0.4925 - val_mse: 0.4925\n",
      "Epoch 55/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4988 - mse: 0.4988 - val_loss: 0.4927 - val_mse: 0.4927\n",
      "Epoch 56/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4925 - val_mse: 0.4925\n",
      "Epoch 57/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4925 - val_mse: 0.4925\n",
      "Epoch 58/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4988 - mse: 0.4988 - val_loss: 0.4926 - val_mse: 0.4926\n",
      "Epoch 59/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 0.4932 - val_mse: 0.4932\n",
      "Epoch 60/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4923 - val_mse: 0.4923\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4923 - mse: 0.4923\n",
      "\n",
      "Weight: [array([[2.0125332]], dtype=float32), array([0.9857023], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  0.9\n",
      "Epoch 1/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7745 - mse: 0.7745 - val_loss: 0.7927 - val_mse: 0.7927\n",
      "Epoch 2/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7970 - val_mse: 0.7970\n",
      "Epoch 3/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7744 - mse: 0.7744 - val_loss: 0.7933 - val_mse: 0.7933\n",
      "Epoch 4/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7742 - mse: 0.7742 - val_loss: 0.7937 - val_mse: 0.7937\n",
      "Epoch 5/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7741 - mse: 0.7741 - val_loss: 0.7914 - val_mse: 0.7914\n",
      "Epoch 6/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7743 - mse: 0.7743 - val_loss: 0.7901 - val_mse: 0.7901\n",
      "Epoch 7/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7739 - mse: 0.7739 - val_loss: 0.7901 - val_mse: 0.7901\n",
      "Epoch 8/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7741 - mse: 0.7741 - val_loss: 0.7902 - val_mse: 0.7902\n",
      "Epoch 9/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7741 - mse: 0.7741 - val_loss: 0.7903 - val_mse: 0.7903\n",
      "Epoch 10/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7738 - mse: 0.7738 - val_loss: 0.7901 - val_mse: 0.7901\n",
      "Epoch 11/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7742 - mse: 0.7742 - val_loss: 0.7898 - val_mse: 0.7898\n",
      "Epoch 12/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7739 - mse: 0.7739 - val_loss: 0.7882 - val_mse: 0.7882\n",
      "Epoch 13/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7886 - val_mse: 0.7886\n",
      "Epoch 14/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7739 - mse: 0.7739 - val_loss: 0.7882 - val_mse: 0.7882\n",
      "Epoch 15/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7743 - mse: 0.7743 - val_loss: 0.7912 - val_mse: 0.7912\n",
      "Epoch 16/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7741 - mse: 0.7741 - val_loss: 0.7914 - val_mse: 0.7914\n",
      "Epoch 17/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7738 - mse: 0.7738 - val_loss: 0.7917 - val_mse: 0.7917\n",
      "Epoch 18/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7739 - mse: 0.7739 - val_loss: 0.7909 - val_mse: 0.7909\n",
      "Epoch 19/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7737 - mse: 0.7737 - val_loss: 0.7892 - val_mse: 0.7892\n",
      "Epoch 20/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7738 - mse: 0.7738 - val_loss: 0.7888 - val_mse: 0.7888\n",
      "Epoch 21/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7911 - val_mse: 0.7911\n",
      "Epoch 22/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7739 - mse: 0.7739 - val_loss: 0.7925 - val_mse: 0.7925\n",
      "Epoch 23/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7739 - mse: 0.7739 - val_loss: 0.7900 - val_mse: 0.7900\n",
      "Epoch 24/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7739 - mse: 0.7739 - val_loss: 0.7905 - val_mse: 0.7905\n",
      "Epoch 25/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7901 - val_mse: 0.7901\n",
      "Epoch 26/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7897 - val_mse: 0.7897\n",
      "Epoch 27/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7901 - val_mse: 0.7901\n",
      "Epoch 28/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7739 - mse: 0.7739 - val_loss: 0.7892 - val_mse: 0.7892\n",
      "Epoch 29/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7744 - mse: 0.7744 - val_loss: 0.7890 - val_mse: 0.7890\n",
      "Epoch 30/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7739 - mse: 0.7739 - val_loss: 0.7886 - val_mse: 0.7886\n",
      "Epoch 31/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7865 - val_mse: 0.7865\n",
      "Epoch 32/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7882 - val_mse: 0.7882\n",
      "Epoch 33/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7739 - mse: 0.7739 - val_loss: 0.7890 - val_mse: 0.7890\n",
      "Epoch 34/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7926 - val_mse: 0.7926\n",
      "Epoch 35/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7741 - mse: 0.7741 - val_loss: 0.7912 - val_mse: 0.7912\n",
      "Epoch 36/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7739 - mse: 0.7739 - val_loss: 0.7905 - val_mse: 0.7905\n",
      "Epoch 37/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7741 - mse: 0.7741 - val_loss: 0.7895 - val_mse: 0.7895\n",
      "Epoch 38/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7878 - val_mse: 0.7878\n",
      "Epoch 39/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7739 - mse: 0.7739 - val_loss: 0.7890 - val_mse: 0.7890\n",
      "Epoch 40/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7739 - mse: 0.7739 - val_loss: 0.7917 - val_mse: 0.7917\n",
      "Epoch 41/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7741 - mse: 0.7741 - val_loss: 0.7936 - val_mse: 0.7936\n",
      "Epoch 42/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7917 - val_mse: 0.7917\n",
      "Epoch 43/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7739 - mse: 0.7739 - val_loss: 0.7895 - val_mse: 0.7895\n",
      "Epoch 44/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7878 - val_mse: 0.7878\n",
      "Epoch 45/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7869 - val_mse: 0.7869\n",
      "Epoch 46/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7741 - mse: 0.7741 - val_loss: 0.7873 - val_mse: 0.7873\n",
      "Epoch 47/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7894 - val_mse: 0.7894\n",
      "Epoch 48/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7741 - mse: 0.7741 - val_loss: 0.7885 - val_mse: 0.7885\n",
      "Epoch 49/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7880 - val_mse: 0.7880\n",
      "Epoch 50/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7739 - mse: 0.7739 - val_loss: 0.7875 - val_mse: 0.7875\n",
      "Epoch 51/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7741 - mse: 0.7741 - val_loss: 0.7871 - val_mse: 0.7871\n",
      "Epoch 52/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7899 - val_mse: 0.7899\n",
      "Epoch 53/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7922 - val_mse: 0.7922\n",
      "Epoch 54/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7742 - mse: 0.7742 - val_loss: 0.7903 - val_mse: 0.7903\n",
      "Epoch 55/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7741 - mse: 0.7741 - val_loss: 0.7914 - val_mse: 0.7914\n",
      "Epoch 56/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7934 - val_mse: 0.7934\n",
      "Epoch 57/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7744 - mse: 0.7744 - val_loss: 0.7922 - val_mse: 0.7922\n",
      "Epoch 58/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7904 - val_mse: 0.7904\n",
      "Epoch 59/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7737 - mse: 0.7737 - val_loss: 0.7886 - val_mse: 0.7886\n",
      "Epoch 60/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7740 - mse: 0.7740 - val_loss: 0.7891 - val_mse: 0.7891\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.7891 - mse: 0.7891\n",
      "\n",
      "Weight: [array([[1.9782888]], dtype=float32), array([1.0003182], dtype=float32)]\n",
      "\n",
      "\n",
      "N_train =  1000.0\n",
      "Sigma =  1.1\n",
      "Epoch 1/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1941 - mse: 1.1941 - val_loss: 0.9933 - val_mse: 0.9933\n",
      "Epoch 2/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1928 - mse: 1.1928 - val_loss: 0.9965 - val_mse: 0.9965\n",
      "Epoch 3/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1915 - mse: 1.1915 - val_loss: 0.9939 - val_mse: 0.9939\n",
      "Epoch 4/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1914 - mse: 1.1914 - val_loss: 0.9953 - val_mse: 0.9953\n",
      "Epoch 5/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1913 - mse: 1.1913 - val_loss: 0.9920 - val_mse: 0.9920\n",
      "Epoch 6/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1911 - mse: 1.1911 - val_loss: 0.9943 - val_mse: 0.9943\n",
      "Epoch 7/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1909 - mse: 1.1909 - val_loss: 0.9923 - val_mse: 0.9923\n",
      "Epoch 8/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1909 - mse: 1.1909 - val_loss: 0.9894 - val_mse: 0.9894\n",
      "Epoch 9/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1905 - mse: 1.1905 - val_loss: 0.9894 - val_mse: 0.9894\n",
      "Epoch 10/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1907 - mse: 1.1907 - val_loss: 0.9875 - val_mse: 0.9875\n",
      "Epoch 11/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1908 - mse: 1.1908 - val_loss: 0.9891 - val_mse: 0.9891\n",
      "Epoch 12/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1907 - mse: 1.1907 - val_loss: 0.9892 - val_mse: 0.9892\n",
      "Epoch 13/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1909 - mse: 1.1909 - val_loss: 0.9876 - val_mse: 0.9876\n",
      "Epoch 14/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1910 - mse: 1.1910 - val_loss: 0.9891 - val_mse: 0.9891\n",
      "Epoch 15/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1908 - mse: 1.1908 - val_loss: 0.9898 - val_mse: 0.9898\n",
      "Epoch 16/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1907 - mse: 1.1907 - val_loss: 0.9883 - val_mse: 0.9883\n",
      "Epoch 17/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1911 - mse: 1.1911 - val_loss: 0.9888 - val_mse: 0.9888\n",
      "Epoch 18/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1907 - mse: 1.1907 - val_loss: 0.9893 - val_mse: 0.9893\n",
      "Epoch 19/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1905 - mse: 1.1905 - val_loss: 0.9892 - val_mse: 0.9892\n",
      "Epoch 20/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1906 - mse: 1.1906 - val_loss: 0.9875 - val_mse: 0.9875\n",
      "Epoch 21/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1908 - mse: 1.1908 - val_loss: 0.9897 - val_mse: 0.9897\n",
      "Epoch 22/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1909 - mse: 1.1909 - val_loss: 0.9877 - val_mse: 0.9877\n",
      "Epoch 23/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1908 - mse: 1.1908 - val_loss: 0.9843 - val_mse: 0.9843\n",
      "Epoch 24/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1908 - mse: 1.1908 - val_loss: 0.9896 - val_mse: 0.9896\n",
      "Epoch 25/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1907 - mse: 1.1907 - val_loss: 0.9894 - val_mse: 0.9894\n",
      "Epoch 26/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1910 - mse: 1.1910 - val_loss: 0.9902 - val_mse: 0.9902\n",
      "Epoch 27/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1910 - mse: 1.1910 - val_loss: 0.9881 - val_mse: 0.9881\n",
      "Epoch 28/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1910 - mse: 1.1910 - val_loss: 0.9930 - val_mse: 0.9930\n",
      "Epoch 29/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1907 - mse: 1.1907 - val_loss: 0.9905 - val_mse: 0.9905\n",
      "Epoch 30/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1906 - mse: 1.1906 - val_loss: 0.9903 - val_mse: 0.9903\n",
      "Epoch 31/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1907 - mse: 1.1907 - val_loss: 0.9916 - val_mse: 0.9916\n",
      "Epoch 32/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1906 - mse: 1.1906 - val_loss: 0.9897 - val_mse: 0.9897\n",
      "Epoch 33/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1910 - mse: 1.1910 - val_loss: 0.9923 - val_mse: 0.9923\n",
      "Epoch 34/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1907 - mse: 1.1907 - val_loss: 0.9893 - val_mse: 0.9893\n",
      "Epoch 35/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1906 - mse: 1.1906 - val_loss: 0.9915 - val_mse: 0.9915\n",
      "Epoch 36/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1908 - mse: 1.1908 - val_loss: 0.9911 - val_mse: 0.9911\n",
      "Epoch 37/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1910 - mse: 1.1910 - val_loss: 0.9905 - val_mse: 0.9905\n",
      "Epoch 38/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1908 - mse: 1.1908 - val_loss: 0.9911 - val_mse: 0.9911\n",
      "Epoch 39/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1908 - mse: 1.1908 - val_loss: 0.9943 - val_mse: 0.9943\n",
      "Epoch 40/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1909 - mse: 1.1909 - val_loss: 0.9927 - val_mse: 0.9927\n",
      "Epoch 41/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1905 - mse: 1.1905 - val_loss: 0.9891 - val_mse: 0.9891\n",
      "Epoch 42/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1911 - mse: 1.1911 - val_loss: 0.9906 - val_mse: 0.9906\n",
      "Epoch 43/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1905 - mse: 1.1905 - val_loss: 0.9916 - val_mse: 0.9916\n",
      "Epoch 44/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1909 - mse: 1.1909 - val_loss: 0.9897 - val_mse: 0.9897\n",
      "Epoch 45/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1909 - mse: 1.1909 - val_loss: 0.9886 - val_mse: 0.9886\n",
      "Epoch 46/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1910 - mse: 1.1910 - val_loss: 0.9904 - val_mse: 0.9904\n",
      "Epoch 47/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1911 - mse: 1.1911 - val_loss: 0.9883 - val_mse: 0.9883\n",
      "Epoch 48/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1906 - mse: 1.1906 - val_loss: 0.9871 - val_mse: 0.9871\n",
      "Epoch 49/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1908 - mse: 1.1908 - val_loss: 0.9860 - val_mse: 0.9860\n",
      "Epoch 50/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1906 - mse: 1.1906 - val_loss: 0.9888 - val_mse: 0.9888\n",
      "Epoch 51/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1907 - mse: 1.1907 - val_loss: 0.9888 - val_mse: 0.9888\n",
      "Epoch 52/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1906 - mse: 1.1906 - val_loss: 0.9896 - val_mse: 0.9896\n",
      "Epoch 53/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1908 - mse: 1.1908 - val_loss: 0.9908 - val_mse: 0.9908\n",
      "Epoch 54/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1907 - mse: 1.1907 - val_loss: 0.9898 - val_mse: 0.9898\n",
      "Epoch 55/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1909 - mse: 1.1909 - val_loss: 0.9891 - val_mse: 0.9891\n",
      "Epoch 56/60\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1904 - mse: 1.1904 - val_loss: 0.9910 - val_mse: 0.9910\n",
      "Epoch 57/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1908 - mse: 1.1908 - val_loss: 0.9917 - val_mse: 0.9917\n",
      "Epoch 58/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1907 - mse: 1.1907 - val_loss: 0.9915 - val_mse: 0.9915\n",
      "Epoch 59/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1907 - mse: 1.1907 - val_loss: 0.9906 - val_mse: 0.9906\n",
      "Epoch 60/60\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1909 - mse: 1.1909 - val_loss: 0.9882 - val_mse: 0.9882\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.9882 - mse: 0.9882\n",
      "\n",
      "Weight: [array([[2.072607]], dtype=float32), array([0.9661132], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "N_train = np.zeros(6)\n",
    "sigma = np.zeros(6)\n",
    "score60 = np.zeros((6,6,2))\n",
    "weight60 = np.zeros((6,6,2))\n",
    "\n",
    "history60 = []\n",
    "\n",
    "\n",
    "#GRID SEARCH\n",
    "for i in range(6):\n",
    "    N_train[i] = 500 + i*100\n",
    "    row = []\n",
    "    for j in range(6):\n",
    "        sigma[j] = 0.1 + j*0.2 # noise standard deviation\n",
    "        x_train = np.random.uniform(-1, 1, int(N_train[i]))\n",
    "    \n",
    "        y_train = np.random.normal(m * x_train + b, sigma[j]) # actual measures from which we want to guess regression parameters\n",
    "        y_valid = np.random.normal(m * x_valid + b, sigma[j])\n",
    "        # fit the model using training dataset\n",
    "        # over 60 epochs of 32 batch size each\n",
    "        # report training progress against validation data\n",
    "        print(\"\\n\\nN_train = \",N_train[i])\n",
    "        print(\"Sigma = \",sigma[j])\n",
    "        row.append(model.fit(x=x_train, y=y_train, \n",
    "              batch_size=32, epochs=60,\n",
    "              shuffle=True, # a good idea is to shuffle input before at each epoch\n",
    "              validation_data=(x_valid, y_valid)))\n",
    "        score60[i][j] = model.evaluate(x_valid, y_valid, batch_size=32, verbose=1)\n",
    "        pesi = model.get_weights()\n",
    "        weight60[i][j] = [pesi[0][0][0],pesi[1][0]]\n",
    "        print(\"\\nWeight:\",pesi) \n",
    "    history60.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostro i risultati della grid search appena svolta per il numero di epoche utilizzate: varieranno il numero di dati di training ed il valore di $\\sigma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 epochs:\n",
      "\n",
      "Indexes of minimum test loss  (array([3]), array([0]))\n",
      "N_train of minimum test loss 800.0\n",
      "sigma of minimum test loss 0.1\n",
      "Minimum Test Loss =  0.007924294099211693\n",
      "\n",
      "Minimum Test Loss per sigma:\n",
      "sigma =  0.1, N_train =  800.0:    0.007924294099211693\n",
      "sigma =  0.3, N_train =  900.0:    0.06013498455286026\n",
      "sigma =  0.5, N_train =  900.0:    0.17158323526382446\n",
      "sigma =  0.7, N_train =  500.0:    0.4086116850376129\n",
      "sigma =  0.9, N_train =  800.0:    0.6876957416534424\n",
      "sigma =  1.1, N_train =  900.0:    0.9424852132797241\n",
      "\n",
      "20 epochs:\n",
      "\n",
      "Indexes of minimum test loss  (array([2]), array([0]))\n",
      "N_train of minimum test loss 700.0\n",
      "sigma of minimum test loss 0.1\n",
      "Minimum Test Loss =  0.007940101437270641\n",
      "\n",
      "Minimum Test Loss per sigma:\n",
      "sigma =  0.1, N_train =  700.0:    0.007940101437270641\n",
      "sigma =  0.3, N_train =  700.0:    0.05484117567539215\n",
      "sigma =  0.5, N_train =  500.0:    0.16879959404468536\n",
      "sigma =  0.7, N_train =  800.0:    0.40462353825569153\n",
      "sigma =  0.9, N_train =  600.0:    0.7635987997055054\n",
      "sigma =  1.1, N_train =  900.0:    0.938497006893158\n",
      "\n",
      "30 epochs:\n",
      "\n",
      "Indexes of minimum test loss  (array([0]), array([0]))\n",
      "N_train of minimum test loss 500.0\n",
      "sigma of minimum test loss 0.1\n",
      "Minimum Test Loss =  0.007292888127267361\n",
      "\n",
      "Minimum Test Loss per sigma:\n",
      "sigma =  0.1, N_train =  500.0:    0.007292888127267361\n",
      "sigma =  0.3, N_train =  700.0:    0.07347969710826874\n",
      "sigma =  0.5, N_train =  700.0:    0.2062835842370987\n",
      "sigma =  0.7, N_train =  700.0:    0.3572252690792084\n",
      "sigma =  0.9, N_train =  900.0:    0.7185412645339966\n",
      "sigma =  1.1, N_train =  800.0:    1.1001830101013184\n",
      "40 epochs:\n",
      "\n",
      "Indexes of minimum test loss  (array([1]), array([0]))\n",
      "N_train of minimum test loss 600.0\n",
      "sigma of minimum test loss 0.1\n",
      "Minimum Test Loss =  0.006139276083558798\n",
      "\n",
      "Minimum Test Loss per sigma:\n",
      "sigma =  0.1, N_train =  600.0:    0.006139276083558798\n",
      "sigma =  0.3, N_train =  800.0:    0.057628944516181946\n",
      "sigma =  0.5, N_train =  500.0:    0.20449909567832947\n",
      "sigma =  0.7, N_train =  600.0:    0.3528153896331787\n",
      "sigma =  0.9, N_train =  900.0:    0.7905957698822021\n",
      "sigma =  1.1, N_train =  600.0:    1.0850791931152344\n",
      "\n",
      "50 epochs:\n",
      "\n",
      "Indexes of minimum test loss  (array([1]), array([0]))\n",
      "N_train of minimum test loss 600.0\n",
      "sigma of minimum test loss 0.1\n",
      "Minimum Test Loss =  0.006675671320408583\n",
      "\n",
      "Minimum Test Loss per sigma:\n",
      "sigma =  0.1, N_train =  600.0:    0.006675671320408583\n",
      "sigma =  0.3, N_train =  900.0:    0.0766650140285492\n",
      "sigma =  0.5, N_train =  800.0:    0.19493809342384338\n",
      "sigma =  0.7, N_train =  600.0:    0.4135359525680542\n",
      "sigma =  0.9, N_train =  500.0:    0.5945382714271545\n",
      "sigma =  1.1, N_train =  800.0:    1.0018442869186401\n",
      "\n",
      "60 epochs:\n",
      "\n",
      "Indexes of minimum test loss  (array([4]), array([0]))\n",
      "N_train of minimum test loss 900.0\n",
      "sigma of minimum test loss 0.1\n",
      "Minimum Test Loss =  0.006265712436288595\n",
      "\n",
      "Minimum Test Loss per sigma:\n",
      "sigma =  0.1, N_train =  900.0:    0.006265712436288595\n",
      "sigma =  0.3, N_train =  500.0:    0.07436589896678925\n",
      "sigma =  0.5, N_train =  1000.0:    0.22199398279190063\n",
      "sigma =  0.7, N_train =  700.0:    0.390293687582016\n",
      "sigma =  0.9, N_train =  800.0:    0.6449233889579773\n",
      "sigma =  1.1, N_train =  500.0:    0.7181247472763062\n"
     ]
    }
   ],
   "source": [
    "test_loss10 = [[score10[i][j][0] for j in range(6)] for i in range(6)]\n",
    "test_loss20 = [[score20[i][j][0] for j in range(6)] for i in range(6)]\n",
    "test_loss30 = [[score30[i][j][0] for j in range(6)] for i in range(6)]\n",
    "test_loss40 = [[score40[i][j][0] for j in range(6)] for i in range(6)]\n",
    "test_loss50 = [[score50[i][j][0] for j in range(6)] for i in range(6)]\n",
    "test_loss60 = [[score60[i][j][0] for j in range(6)] for i in range(6)]\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(20, 10))\n",
    "fig.suptitle(\"Grid Search Test Loss\", fontsize=16, fontweight = 'bold')\n",
    "\n",
    "\n",
    "#x = sigma, y = N_train\n",
    "img10 = ax[0][0].imshow(test_loss10, extent = [0.1 , 1.3 , 1100 , 500], aspect = 'auto')\n",
    "img20 = ax[0][1].imshow(test_loss20, extent = [0.1 , 1.3 , 1100 , 500], aspect = 'auto')\n",
    "img30 = ax[0][2].imshow(test_loss30, extent = [0.1 , 1.3 , 1100 , 500], aspect = 'auto')\n",
    "img40 = ax[1][0].imshow(test_loss40, extent = [0.1 , 1.3 , 1100 , 500], aspect = 'auto')\n",
    "img50 = ax[1][1].imshow(test_loss50, extent = [0.1 , 1.3 , 1100 , 500], aspect = 'auto')\n",
    "img60 = ax[1][2].imshow(test_loss60, extent = [0.1 , 1.3 , 1100 , 500], aspect = 'auto')\n",
    "\n",
    "plt.colorbar(img10, ax=ax[0][0])\n",
    "plt.colorbar(img20, ax=ax[0][1])\n",
    "plt.colorbar(img30, ax=ax[0][2])\n",
    "plt.colorbar(img40, ax=ax[1][0])\n",
    "plt.colorbar(img50, ax=ax[1][1])\n",
    "plt.colorbar(img60, ax=ax[1][2])\n",
    "\n",
    "ax[0][0].set_xlabel('$\\mathbf{\\sigma}$', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][0].set_ylabel('$\\mathbf{N_{train}}$', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][0].set_title('Test Loss (10 epochs)', fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "ax[0][1].set_xlabel('$\\mathbf{\\sigma}$', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][1].set_ylabel('$\\mathbf{N_{train}}$', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][1].set_title('Test Loss (20 epochs)', fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "ax[0][2].set_xlabel('$\\mathbf{\\sigma}$', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][2].set_ylabel('$\\mathbf{N_{train}}$', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][2].set_title('Test Loss (30 epochs)', fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "ax[1][0].set_xlabel('$\\mathbf{\\sigma}$', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][0].set_ylabel('$\\mathbf{N_{train}}$', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][0].set_title('Test Loss (40 epochs)', fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "ax[1][1].set_xlabel('$\\mathbf{\\sigma}$', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][1].set_ylabel('$\\mathbf{N_{train}}$', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][1].set_title('Test Loss (50 epochs)', fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "ax[1][2].set_xlabel('$\\mathbf{\\sigma}$', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][2].set_ylabel('$\\mathbf{N_{train}}$', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][2].set_title('Test Loss (60 epochs)', fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "\n",
    "sigma[0] = '{:.1f}'.format(sigma[0])\n",
    "sigma[1] = '{:.1f}'.format(sigma[1])\n",
    "sigma[2] = '{:.1f}'.format(sigma[2])\n",
    "sigma[3] = '{:.1f}'.format(sigma[3])\n",
    "sigma[4] = '{:.1f}'.format(sigma[4])\n",
    "sigma[5] = '{:.1f}'.format(sigma[5])\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "ij_min10 = np.where(test_loss10 == np.amin(test_loss10))\n",
    "print(\"10 epochs:\\n\")\n",
    "print(\"Indexes of minimum test loss \",ij_min10)\n",
    "print(\"N_train of minimum test loss\",N_train[ij_min10[0][0]])\n",
    "print(\"sigma of minimum test loss\",sigma[ij_min10[1][0]])\n",
    "print(\"Minimum Test Loss = \",np.amin(test_loss10))\n",
    "print(\"\\nMinimum Test Loss per sigma:\")\n",
    "print(\"sigma = \",sigma[0], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss10[i][0] for i in range(6)] == np.amin([test_loss10[i][0] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss10[i][0] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[1], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss10[i][1] for i in range(6)] == np.amin([test_loss10[i][1] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss10[i][1] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[2], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss10[i][2] for i in range(6)] == np.amin([test_loss10[i][2] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss10[i][2] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[3], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss10[i][3] for i in range(6)] == np.amin([test_loss10[i][3] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss10[i][3] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[4], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss10[i][4] for i in range(6)] == np.amin([test_loss10[i][4] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss10[i][4] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[5], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss10[i][5] for i in range(6)] == np.amin([test_loss10[i][5] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss10[i][5] for i in range(6)]))\n",
    "\n",
    "\n",
    "\n",
    "ij_min20 = np.where(test_loss20 == np.amin(test_loss20))\n",
    "print(\"\\n20 epochs:\\n\")\n",
    "print(\"Indexes of minimum test loss \",ij_min20)\n",
    "print(\"N_train of minimum test loss\",N_train[ij_min20[0][0]])\n",
    "print(\"sigma of minimum test loss\",sigma[ij_min20[1][0]])\n",
    "print(\"Minimum Test Loss = \",np.amin(test_loss20))\n",
    "print(\"\\nMinimum Test Loss per sigma:\")\n",
    "print(\"sigma = \",sigma[0], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss20[i][0] for i in range(6)] == np.amin([test_loss20[i][0] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss20[i][0] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[1], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss20[i][1] for i in range(6)] == np.amin([test_loss20[i][1] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss20[i][1] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[2], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss20[i][2] for i in range(6)] == np.amin([test_loss20[i][2] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss20[i][2] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[3], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss20[i][3] for i in range(6)] == np.amin([test_loss20[i][3] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss20[i][3] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[4], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss20[i][4] for i in range(6)] == np.amin([test_loss20[i][4] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss20[i][4] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[5], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss20[i][5] for i in range(6)] == np.amin([test_loss20[i][5] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss20[i][5] for i in range(6)]))\n",
    "\n",
    "\n",
    "\n",
    "ij_min30 = np.where(test_loss30 == np.amin(test_loss30))\n",
    "print(\"\\n30 epochs:\\n\")\n",
    "print(\"Indexes of minimum test loss \",ij_min30)\n",
    "print(\"N_train of minimum test loss\",N_train[ij_min30[0][0]])\n",
    "print(\"sigma of minimum test loss\",sigma[ij_min30[1][0]])\n",
    "print(\"Minimum Test Loss = \",np.amin(test_loss30))\n",
    "print(\"\\nMinimum Test Loss per sigma:\")\n",
    "print(\"sigma = \",sigma[0], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss30[i][0] for i in range(6)] == np.amin([test_loss30[i][0] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss30[i][0] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[1], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss30[i][1] for i in range(6)] == np.amin([test_loss30[i][1] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss30[i][1] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[2], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss30[i][2] for i in range(6)] == np.amin([test_loss30[i][2] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss30[i][2] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[3], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss30[i][3] for i in range(6)] == np.amin([test_loss30[i][3] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss30[i][3] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[4], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss30[i][4] for i in range(6)] == np.amin([test_loss30[i][4] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss30[i][4] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[5], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss30[i][5] for i in range(6)] == np.amin([test_loss30[i][5] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss30[i][5] for i in range(6)]))\n",
    "\n",
    "\n",
    "ij_min40 = np.where(test_loss40 == np.amin(test_loss40))\n",
    "print(\"40 epochs:\\n\")\n",
    "print(\"Indexes of minimum test loss \",ij_min40)\n",
    "print(\"N_train of minimum test loss\",N_train[ij_min40[0][0]])\n",
    "print(\"sigma of minimum test loss\",sigma[ij_min40[1][0]])\n",
    "print(\"Minimum Test Loss = \",np.amin(test_loss40))\n",
    "print(\"\\nMinimum Test Loss per sigma:\")\n",
    "print(\"sigma = \",sigma[0], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss40[i][0] for i in range(6)] == np.amin([test_loss40[i][0] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss40[i][0] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[1], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss40[i][1] for i in range(6)] == np.amin([test_loss40[i][1] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss40[i][1] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[2], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss40[i][2] for i in range(6)] == np.amin([test_loss40[i][2] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss40[i][2] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[3], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss40[i][3] for i in range(6)] == np.amin([test_loss40[i][3] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss40[i][3] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[4], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss40[i][4] for i in range(6)] == np.amin([test_loss40[i][4] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss40[i][4] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[5], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss40[i][5] for i in range(6)] == np.amin([test_loss40[i][5] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss40[i][5] for i in range(6)]))\n",
    "\n",
    "\n",
    "\n",
    "ij_min50 = np.where(test_loss50 == np.amin(test_loss50))\n",
    "print(\"\\n50 epochs:\\n\")\n",
    "print(\"Indexes of minimum test loss \",ij_min50)\n",
    "print(\"N_train of minimum test loss\",N_train[ij_min50[0][0]])\n",
    "print(\"sigma of minimum test loss\",sigma[ij_min50[1][0]])\n",
    "print(\"Minimum Test Loss = \",np.amin(test_loss50))\n",
    "print(\"\\nMinimum Test Loss per sigma:\")\n",
    "print(\"sigma = \",sigma[0], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss50[i][0] for i in range(6)] == np.amin([test_loss50[i][0] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss50[i][0] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[1], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss50[i][1] for i in range(6)] == np.amin([test_loss50[i][1] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss50[i][1] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[2], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss50[i][2] for i in range(6)] == np.amin([test_loss50[i][2] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss50[i][2] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[3], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss50[i][3] for i in range(6)] == np.amin([test_loss50[i][3] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss50[i][3] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[4], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss50[i][4] for i in range(6)] == np.amin([test_loss50[i][4] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss50[i][4] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[5], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss50[i][5] for i in range(6)] == np.amin([test_loss50[i][5] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss50[i][5] for i in range(6)]))\n",
    "\n",
    "\n",
    "\n",
    "ij_min60 = np.where(test_loss60 == np.amin(test_loss60))\n",
    "print(\"\\n60 epochs:\\n\")\n",
    "print(\"Indexes of minimum test loss \",ij_min60)\n",
    "print(\"N_train of minimum test loss\",N_train[ij_min60[0][0]])\n",
    "print(\"sigma of minimum test loss\",sigma[ij_min60[1][0]])\n",
    "print(\"Minimum Test Loss = \",np.amin(test_loss60))\n",
    "print(\"\\nMinimum Test Loss per sigma:\")\n",
    "print(\"sigma = \",sigma[0], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss60[i][0] for i in range(6)] == np.amin([test_loss60[i][0] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss60[i][0] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[1], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss60[i][1] for i in range(6)] == np.amin([test_loss60[i][1] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss60[i][1] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[2], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss60[i][2] for i in range(6)] == np.amin([test_loss60[i][2] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss60[i][2] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[3], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss60[i][3] for i in range(6)] == np.amin([test_loss60[i][3] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss60[i][3] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[4], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss60[i][4] for i in range(6)] == np.amin([test_loss60[i][4] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss60[i][4] for i in range(6)]))\n",
    "print(\"sigma = \",sigma[5], end = \", \")\n",
    "print(\"N_train = \",N_train[np.where([test_loss60[i][5] for i in range(6)] == np.amin([test_loss60[i][5] for i in range(6)]))][0], end = \":    \")\n",
    "print(np.amin([test_loss60[i][5] for i in range(6)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chiaramente i risultati migliori sono ottenuti quando l'errore $\\sigma$ è minimo ($\\sigma=0.1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best01 = []\n",
    "best03 = []\n",
    "best05 = []\n",
    "best07 = []\n",
    "best09 = []\n",
    "best11 = []\n",
    "\n",
    "best01.append(np.amin([test_loss10[i][0] for i in range(6)])),best01.append(np.amin([test_loss20[i][0] for i in range(6)])),best01.append(np.amin([test_loss30[i][0] for i in range(6)])),best01.append(np.amin([test_loss40[i][0] for i in range(6)])),best01.append(np.amin([test_loss50[i][0] for i in range(6)])),best01.append(np.amin([test_loss60[i][0] for i in range(6)]))\n",
    "best03.append(np.amin([test_loss10[i][1] for i in range(6)])),best03.append(np.amin([test_loss20[i][1] for i in range(6)])),best03.append(np.amin([test_loss30[i][1] for i in range(6)])),best03.append(np.amin([test_loss40[i][1] for i in range(6)])),best03.append(np.amin([test_loss50[i][1] for i in range(6)])),best03.append(np.amin([test_loss60[i][1] for i in range(6)]))\n",
    "best05.append(np.amin([test_loss10[i][2] for i in range(6)])),best05.append(np.amin([test_loss20[i][2] for i in range(6)])),best05.append(np.amin([test_loss30[i][2] for i in range(6)])),best05.append(np.amin([test_loss40[i][2] for i in range(6)])),best05.append(np.amin([test_loss50[i][2] for i in range(6)])),best05.append(np.amin([test_loss60[i][2] for i in range(6)]))\n",
    "best07.append(np.amin([test_loss10[i][3] for i in range(6)])),best07.append(np.amin([test_loss20[i][3] for i in range(6)])),best07.append(np.amin([test_loss30[i][3] for i in range(6)])),best07.append(np.amin([test_loss40[i][3] for i in range(6)])),best07.append(np.amin([test_loss50[i][3] for i in range(6)])),best07.append(np.amin([test_loss60[i][3] for i in range(6)]))\n",
    "best09.append(np.amin([test_loss10[i][4] for i in range(6)])),best09.append(np.amin([test_loss20[i][4] for i in range(6)])),best09.append(np.amin([test_loss30[i][4] for i in range(6)])),best09.append(np.amin([test_loss40[i][4] for i in range(6)])),best09.append(np.amin([test_loss50[i][4] for i in range(6)])),best09.append(np.amin([test_loss60[i][4] for i in range(6)]))\n",
    "best11.append(np.amin([test_loss10[i][5] for i in range(6)])),best11.append(np.amin([test_loss20[i][5] for i in range(6)])),best11.append(np.amin([test_loss30[i][5] for i in range(6)])),best11.append(np.amin([test_loss40[i][5] for i in range(6)])),best11.append(np.amin([test_loss50[i][5] for i in range(6)])),best11.append(np.amin([test_loss60[i][5] for i in range(6)]))\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(20, 10))\n",
    "fig.suptitle(\"Best Test Loss per Number of Epochs\", fontsize=16, fontweight = 'bold')\n",
    "n_epochs = [10,20,30,40,50,60]\n",
    "\n",
    "ax[0][0].plot(n_epochs,best01)\n",
    "ax[0][0].set_ylabel('Test Loss', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][0].set_xlabel('# of epochs', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][0].set_title('Test Loss ($\\mathbf{\\sigma = 0.1}$)', fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "ax[0][1].plot(n_epochs,best03)\n",
    "ax[0][1].set_ylabel('Test Loss', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][1].set_xlabel('# of epochs', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][1].set_title('Test Loss ($\\mathbf{\\sigma = 0.3}$)', fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "ax[0][2].plot(n_epochs,best05)\n",
    "ax[0][2].set_ylabel('Test Loss', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][2].set_xlabel('# of epochs', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][2].set_title('Test Loss ($\\mathbf{\\sigma = 0.5}$)', fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "ax[1][0].plot(n_epochs,best07)\n",
    "ax[1][0].set_ylabel('Test Loss', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][0].set_xlabel('# of epochs', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][0].set_title('Test Loss ($\\mathbf{\\sigma = 0.7}$)', fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "ax[1][1].plot(n_epochs,best09)\n",
    "ax[1][1].set_ylabel('Test Loss', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][1].set_xlabel('# of epoch', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][1].set_title('Test Loss ($\\mathbf{\\sigma = 0.9}$)', fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "ax[1][2].plot(n_epochs,best11)\n",
    "ax[1][2].set_ylabel('Test Loss', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][2].set_xlabel('# of epochs', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][2].set_title('Test Loss ($\\mathbf{\\sigma = 1.1}$)', fontsize = 14, fontweight = 'bold')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vede come all'aumento del numero di epoche (prendendo la coppia $(N_{train},n_{epochs})$ con i risultati migliori) non corrisponde necessariamente una diminuzione della funzione di costo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_sigma = []\n",
    "best_sigma.append(np.amin(best01)),best_sigma.append(np.amin(best03)),best_sigma.append(np.amin(best05)),best_sigma.append(np.amin(best07)),best_sigma.append(np.amin(best09)),best_sigma.append(np.amin(best11))\n",
    "\n",
    "fig = plt.figure(figsize = (14,6))\n",
    "#fig.suptitle(\"Thermodynamic Quantities, Ising 1D\", fontsize = 16, fontweight = \"bold\")\n",
    "ax = fig.add_subplot(121)\n",
    "\n",
    "plt.plot(n_epochs,best01)\n",
    "plt.plot(n_epochs,best03)\n",
    "plt.plot(n_epochs,best05)\n",
    "plt.plot(n_epochs,best07)\n",
    "plt.plot(n_epochs,best09)\n",
    "plt.plot(n_epochs,best11)\n",
    "\n",
    "ax.set_title('Best Test Loss per Number of Epoch', fontsize = 14, fontweight = 'bold')\n",
    "ax.set_ylabel('Test Loss', fontsize = 12, fontweight = 'bold')\n",
    "ax.set_xlabel('# of epochs', fontsize = 12, fontweight = 'bold')\n",
    "ax.legend(['$\\sigma = 0.1$', '$\\sigma = 0.3$', '$\\sigma = 0.5$', '$\\sigma = 0.7$', '$\\sigma = 0.9$', '$\\sigma = 1.1$'], loc='best')\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "\n",
    "plt.plot(sigma,best_sigma)\n",
    "\n",
    "ax.set_title('Best Test Loss per $\\mathbf{\\sigma}$', fontsize = 14, fontweight = 'bold')\n",
    "ax.set_ylabel('Test Loss', fontsize = 12, fontweight = 'bold')\n",
    "ax.set_xlabel('$\\mathbf{\\sigma}$', fontsize = 12, fontweight = 'bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora considero per ogni $\\sigma$ la coppia $(N_{train},n_{epochs})$ che ha dato il miglior risultato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimum Test Loss per sigma:\n",
      "sigma = 0.1\n",
      "N_train = 600, n_epochs = 40\n",
      "Test Loss =  0.006139276083558798\n",
      "m =  2.0094635486602783, b =  0.9969154000282288\n",
      "\n",
      "sigma = 0.3\n",
      "N_train = 700, n_epochs = 20\n",
      "Test Loss =  0.05484117567539215\n",
      "m =  2.00840163230896, b =  1.0069239139556885\n",
      "\n",
      "sigma = 0.5\n",
      "N_train = 500, n_epochs = 20\n",
      "Test Loss =  0.16879959404468536\n",
      "m =  2.035062313079834, b =  0.9986358880996704\n",
      "\n",
      "sigma = 0.7\n",
      "N_train = 600, n_epochs = 40\n",
      "Test Loss =  0.3528153896331787\n",
      "m =  2.0361452102661133, b =  0.9965873956680298\n",
      "\n",
      "sigma = 0.9\n",
      "N_train = 500, n_epochs = 50\n",
      "Test Loss =  0.5945382714271545\n",
      "m =  1.983452320098877, b =  0.9565958976745605\n",
      "\n",
      "sigma = 1.1\n",
      "N_train = 500, n_epochs = 60\n",
      "Test Loss =  0.7181247472763062\n",
      "m =  2.050143241882324, b =  1.0496081113815308\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(20, 10))\n",
    "fig.suptitle(\"Best Test Loss per Sigma\", fontsize=16, fontweight = 'bold')\n",
    "x_predicted = np.random.uniform(-1, 1, 100)\n",
    "\n",
    "print(\"\\nMinimum Test Loss per sigma:\")\n",
    "print(\"sigma = 0.1\")\n",
    "print(\"N_train = 600, n_epochs = 40\")\n",
    "print(\"Test Loss = \",test_loss40[1][0])\n",
    "print(\"m = \",weight40[1][0][0], end = \", \")\n",
    "print(\"b = \",weight40[1][0][1], end = \"\\n\\n\")\n",
    "y_predicted = x_predicted*weight40[1][0][0] + weight40[1][0][1]\n",
    "ax[0][0].scatter(x_predicted, y_predicted,color='r', label = 'Prediction')\n",
    "ax[0][0].plot(x_valid, y_target, label = 'Target')\n",
    "ax[0][0].set_ylabel('y', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][0].set_xlabel('x', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][0].grid()\n",
    "ax[0][0].set_title('$\\mathbf{\\sigma = 0.1}$', fontsize = 14, fontweight = 'bold')\n",
    "ax[0][0].legend()\n",
    "\n",
    "print(\"sigma = 0.3\")\n",
    "print(\"N_train = 700, n_epochs = 20\")\n",
    "print(\"Test Loss = \",test_loss20[2][1])\n",
    "print(\"m = \",weight20[2][1][0], end = \", \")\n",
    "print(\"b = \",weight20[2][1][1], end = \"\\n\\n\")\n",
    "y_predicted = x_predicted*weight20[2][1][0] + weight20[2][1][1]\n",
    "ax[0][1].scatter(x_predicted, y_predicted,color='r', label = 'Prediction')\n",
    "ax[0][1].plot(x_valid, y_target, label = 'Target')\n",
    "ax[0][1].set_ylabel('y', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][1].set_xlabel('x', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][1].grid()\n",
    "ax[0][1].set_title('$\\mathbf{\\sigma = 0.3}$', fontsize = 14, fontweight = 'bold')\n",
    "ax[0][1].legend()\n",
    "\n",
    "print(\"sigma = 0.5\")\n",
    "print(\"N_train = 500, n_epochs = 20\")\n",
    "print(\"Test Loss = \",test_loss20[0][2])\n",
    "print(\"m = \",weight20[0][2][0], end = \", \")\n",
    "print(\"b = \",weight20[0][2][1], end = \"\\n\\n\")\n",
    "y_predicted = x_predicted*weight20[0][2][0] + weight20[0][2][1]\n",
    "ax[0][2].scatter(x_predicted, y_predicted,color='r', label = 'Prediction')\n",
    "ax[0][2].plot(x_valid, y_target, label = 'Target')\n",
    "ax[0][2].set_ylabel('y', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][2].set_xlabel('x', fontsize = 12, fontweight = 'bold')\n",
    "ax[0][2].grid()\n",
    "ax[0][2].set_title('$\\mathbf{\\sigma = 0.5}$', fontsize = 14, fontweight = 'bold')\n",
    "ax[0][2].legend()\n",
    "\n",
    "print(\"sigma = 0.7\")\n",
    "print(\"N_train = 600, n_epochs = 40\")\n",
    "print(\"Test Loss = \",test_loss40[1][3])\n",
    "print(\"m = \",weight40[1][3][0], end = \", \")\n",
    "print(\"b = \",weight40[1][3][1], end = \"\\n\\n\")\n",
    "y_predicted = x_predicted*weight40[1][3][0] + weight40[1][3][1]\n",
    "ax[1][0].scatter(x_predicted, y_predicted,color='r', label = 'Prediction')\n",
    "ax[1][0].plot(x_valid, y_target, label = 'Target')\n",
    "ax[1][0].set_ylabel('y', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][0].set_xlabel('x', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][0].grid()\n",
    "ax[1][0].set_title('$\\mathbf{\\sigma = 0.7}$', fontsize = 14, fontweight = 'bold')\n",
    "ax[1][0].legend()\n",
    "\n",
    "print(\"sigma = 0.9\")\n",
    "print(\"N_train = 500, n_epochs = 50\")\n",
    "print(\"Test Loss = \",test_loss50[0][4])\n",
    "print(\"m = \",weight50[0][4][0], end = \", \")\n",
    "print(\"b = \",weight50[0][4][1], end = \"\\n\\n\")\n",
    "y_predicted = x_predicted*weight50[0][4][0] + weight50[0][4][1]\n",
    "ax[1][1].scatter(x_predicted, y_predicted,color='r', label = 'Prediction')\n",
    "ax[1][1].plot(x_valid, y_target, label = 'Target')\n",
    "ax[1][1].set_ylabel('y', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][1].set_xlabel('x', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][1].grid()\n",
    "ax[1][1].set_title('$\\mathbf{\\sigma = 0.9}$', fontsize = 14, fontweight = 'bold')\n",
    "ax[1][1].legend()\n",
    "\n",
    "print(\"sigma = 1.1\")\n",
    "print(\"N_train = 500, n_epochs = 60\")\n",
    "print(\"Test Loss = \",test_loss60[0][5])\n",
    "print(\"m = \",weight60[0][5][0], end = \", \")\n",
    "print(\"b = \",weight60[0][5][1], end = \"\\n\\n\")\n",
    "y_predicted = x_predicted*weight60[0][5][0] + weight60[0][5][1]\n",
    "ax[1][2].scatter(x_predicted, y_predicted,color='r', label = 'Prediction')\n",
    "ax[1][2].plot(x_valid, y_target, label = 'Target')\n",
    "ax[1][2].set_ylabel('y', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][2].set_xlabel('x', fontsize = 12, fontweight = 'bold')\n",
    "ax[1][2].grid()\n",
    "ax[1][2].set_title('$\\mathbf{\\sigma = 1.1}$', fontsize = 14, fontweight = 'bold')\n",
    "ax[1][2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le reti neurali riescono, per ogni $\\sigma$ considerato, a fare previsioni corrette. Vista la semplicità del problema, non mi aspettavo un risultato molto diverso."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
